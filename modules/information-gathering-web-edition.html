
<body>
  <div class="container">
<link rel="stylesheet" href="style.css">


<h1 id="4-information-gathering-web-edition">4. Information Gathering â€” Web Edition</h1>
<h2 id="-cheat-sheet-"><strong>Cheat Sheet</strong></h2>
<p>The cheat sheet is a useful command reference for this module.</p>
<hr>
<p>Web reconnaissance is the first step in any security assessment or penetration testing engagement. It&#39;s akin to a detective&#39;s initial investigation, meticulously gathering clues and evidence about a target before formulating a plan of action. In the digital realm, this translates to accumulating information about a website or web application to identify potential vulnerabilities, security misconfigurations, and valuable assets.</p>
<p>The primary goals of web reconnaissance revolve around gaining a comprehensive understanding of the target&#39;s digital footprint. This includes:</p>
<ul>
<li><code>Identifying Assets</code>: Discovering all associated domains, subdomains, and IP addresses provides a map of the target&#39;s online presence.</li>
<li><code>Uncovering Hidden Information</code>: Web reconnaissance aims to uncover directories, files, and technologies that are not readily apparent and could serve as entry points for an attacker.</li>
<li><code>Analyzing the Attack Surface</code>: By identifying open ports, running services, and software versions, you can assess the potential vulnerabilities and weaknesses of the target.</li>
<li><code>Gathering Intelligence</code>: Collecting information about employees, email addresses, and technologies used can aid in social engineering attacks or identifying specific vulnerabilities associated with certain software.</li>
</ul>
<p>Web reconnaissance can be conducted using either active or passive techniques, each with its own advantages and drawbacks:</p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
<th>Risk of Detection</th>
<th>Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td>Active Reconnaissance</td>
<td>Involves directly interacting with the target system, such as sending probes or requests.</td>
<td>Higher</td>
<td>Port scanning, vulnerability scanning, network mapping</td>
</tr>
<tr>
<td>Passive Reconnaissance</td>
<td>Gathers information without directly interacting with the target, relying on publicly available data.</td>
<td>Lower</td>
<td>Search engine queries, WHOIS lookups, DNS enumeration, web archive analysis, social media</td>
</tr>
</tbody>
</table>
<h3 id="whois">WHOIS</h3>
<p>WHOIS is a query and response protocol used to retrieve information about domain names, IP addresses, and other internet resources. It&#39;s essentially a directory service that details who owns a domain, when it was registered, contact information, and more. In the context of web reconnaissance, WHOIS lookups can be a valuable source of information, potentially revealing the identity of the website owner, their contact information, and other details that could be used for further investigation or social engineering attacks.</p>
<p>For example, if you wanted to find out who owns the domain <code>example.com</code>, you could run the following command in your terminal:</p>
<p>Code: bash</p>
<pre><code class="lang-bash"><span class="hljs-selector-tag">whois</span> <span class="hljs-selector-tag">example</span><span class="hljs-selector-class">.com</span>
</code></pre>
<p>This would return a wealth of information, including the registrar, registration, and expiration dates, nameservers, and contact information for the domain owner.</p>
<p>However, it&#39;s important to note that WHOIS data can be inaccurate or intentionally obscured, so it&#39;s always wise to verify the information from multiple sources. Privacy services can also mask the true owner of a domain, making it more difficult to obtain accurate information through WHOIS.</p>
<h3 id="dns">DNS</h3>
<p>The Domain Name System (DNS) functions as the internet&#39;s GPS, translating user-friendly domain names into the numerical IP addresses computers use to communicate. Like GPS converting a destination&#39;s name into coordinates, DNS ensures your browser reaches the correct website by matching its name with its IP address. This eliminates memorizing complex numerical addresses, making web navigation seamless and efficient.</p>
<p>The <code>dig</code> command allows you to query DNS servers directly, retrieving specific information about domain names. For instance, if you want to find the IP address associated with <code>example.com</code>, you can execute the following command:</p>
<p>Code: bash</p>
<pre><code class="lang-bash">dig example<span class="hljs-selector-class">.com</span> A
</code></pre>
<p>This command instructs <code>dig</code> to query the DNS for the <code>A</code> record (which maps a hostname to an IPv4 address) of <code>example.com</code>. The output will typically include the requested IP address, along with additional details about the query and response. By mastering the <code>dig</code> command and understanding the various DNS record types, you gain the ability to extract valuable information about a target&#39;s infrastructure and online presence.</p>
<p>DNS servers store various types of records, each serving a specific purpose:</p>
<table>
<thead>
<tr>
<th>Record Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>Maps a hostname to an IPv4 address.</td>
</tr>
<tr>
<td>AAAA</td>
<td>Maps a hostname to an IPv6 address.</td>
</tr>
<tr>
<td>CNAME</td>
<td>Creates an alias for a hostname, pointing it to another hostname.</td>
</tr>
<tr>
<td>MX</td>
<td>Specifies mail servers responsible for handling email for the domain.</td>
</tr>
<tr>
<td>NS</td>
<td>Delegates a DNS zone to a specific authoritative name server.</td>
</tr>
<tr>
<td>TXT</td>
<td>Stores arbitrary text information.</td>
</tr>
<tr>
<td>SOA</td>
<td>Contains administrative information about a DNS zone.</td>
</tr>
</tbody>
</table>
<h3 id="subdomains">Subdomains</h3>
<p>Subdomains are essentially extensions of a primary domain name, often used to organize different sections or services within a website. For example, a company might use <code>mail.example.com</code> for their email server or <code>blog.example.com</code> for their blog.</p>
<p>From a reconnaissance perspective, subdomains are incredibly valuable. They can expose additional attack surfaces, reveal hidden services, and provide clues about the internal structure of a target&#39;s network. Subdomains might host development servers, staging environments, or even forgotten applications that haven&#39;t been properly secured.</p>
<p>The process of discovering subdomains is known as subdomain enumeration. There are two main approaches to subdomain enumeration:</p>
<table>
<thead>
<tr>
<th>Approach</th>
<th>Description</th>
<th>Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Active Enumeration</code></td>
<td>Directly interacts with the target&#39;s DNS servers or utilizes tools to probe for subdomains.</td>
<td>Brute-forcing, DNS zone transfers</td>
</tr>
<tr>
<td><code>Passive Enumeration</code></td>
<td>Collects information about subdomains without directly interacting with the target, relying on public sources.</td>
<td>Certificate Transparency (CT) logs, search engine queries</td>
</tr>
</tbody>
</table>
<p><code>Active enumeration</code> can be more thorough but carries a higher risk of detection. Conversely, <code>passive enumeration</code> is stealthier but may not uncover all subdomains. Combining both techniques can significantly increase the likelihood of discovering a comprehensive list of subdomains associated with your target, expanding your understanding of their online presence and potential vulnerabilities.</p>
<h4 id="subdomain-brute-forcing">Subdomain Brute-Forcing</h4>
<p>Subdomain brute-forcing is a proactive technique used in web reconnaissance to uncover subdomains that may not be readily apparent through passive methods. It involves systematically generating many potential subdomain names and testing them against the target&#39;s DNS server to see if they exist. This approach can unveil hidden subdomains that may host valuable information, development servers, or vulnerable applications.</p>
<p>One of the most versatile tools for subdomain brute-forcing is <code>dnsenum</code>. This powerful command-line tool combines various DNS enumeration techniques, including dictionary-based brute-forcing, to uncover subdomains associated with your target.</p>
<p>To use <code>dnsenum</code> for subdomain brute-forcing, you&#39;ll typically provide it with the target domain and a wordlist containing potential subdomain names. The tool will then systematically query the DNS server for each potential subdomain and report any that exist.</p>
<p>For example, the following command would attempt to brute-force subdomains of <code>example.com</code> using a wordlist named <code>subdomains.txt</code>:</p>
<p>Code: bash</p>
<pre><code class="lang-bash"><span class="hljs-selector-tag">dnsenum</span> <span class="hljs-selector-tag">example</span><span class="hljs-selector-class">.com</span> <span class="hljs-selector-tag">-f</span> <span class="hljs-selector-tag">subdomains</span><span class="hljs-selector-class">.txt</span>
</code></pre>
<h4 id="zone-transfers">Zone Transfers</h4>
<p>DNS zone transfers, also known as AXFR (Asynchronous Full Transfer) requests, offer a potential goldmine of information for web reconnaissance. A zone transfer is a mechanism for replicating DNS data across servers. When a zone transfer is successful, it provides a complete copy of the DNS zone file, which contains a wealth of details about the target domain.</p>
<p>This zone file lists all the domain&#39;s subdomains, their associated IP addresses, mail server configurations, and other DNS records. This is akin to obtaining a blueprint of the target&#39;s DNS infrastructure for a reconnaissance expert.</p>
<p>To attempt a zone transfer, you can use the <code>dig</code> command with the <code>axfr</code> (full zone transfer) option. For example, to request a zone transfer from the DNS server <code>ns1.example.com</code> for the domain <code>example.com</code>, you would execute:</p>
<p>Code: bash</p>
<pre><code class="lang-bash">dig @ns1<span class="hljs-selector-class">.example</span><span class="hljs-selector-class">.com</span> example<span class="hljs-selector-class">.com</span> axfr
</code></pre>
<p>However, zone transfers are not always permitted. Many DNS servers are configured to restrict zone transfers to authorized secondary servers only. Misconfigured servers, though, may allow zone transfers from any source, inadvertently exposing sensitive information.</p>
<h4 id="virtual-hosts">Virtual Hosts</h4>
<p>Virtual hosting is a technique that allows multiple websites to share a single IP address. Each website is associated with a unique hostname, which is used to direct incoming requests to the correct site. This can be a cost-effective way for organizations to host multiple websites on a single server, but it can also create a challenge for web reconnaissance.</p>
<p>Since multiple websites share the same IP address, simply scanning the IP won&#39;t reveal all the hosted sites. You need a tool that can test different hostnames against the IP address to see which ones respond.</p>
<p>Gobuster is a versatile tool that can be used for various types of brute-forcing, including virtual host discovery. Its <code>vhost</code> mode is designed to enumerate virtual hosts by sending requests to the target IP address with different hostnames. If a virtual host is configured for a specific hostname, Gobuster will receive a response from the web server.</p>
<p>To use Gobuster to brute-force virtual hosts, you&#39;ll need a wordlist containing potential hostnames. Here&#39;s an example command:</p>
<p>Code: bash</p>
<pre><code class="lang-bash">gobuster vhost -u <span class="hljs-string">http:</span><span class="hljs-comment">//192.0.2.1 -w hostnames.txt</span>
</code></pre>
<p>In this example, <code>-u</code> specifies the target IP address, and <code>-w</code> specifies the wordlist file. Gobuster will then systematically try each hostname in the wordlist and report any that results in a valid response from the web server.</p>
<h4 id="certificate-transparency-ct-logs">Certificate Transparency (CT) Logs</h4>
<p>Certificate Transparency (CT) logs offer a treasure trove of subdomain information for passive reconnaissance. These publicly accessible logs record SSL/TLS certificates issued for domains and their subdomains, serving as a security measure to prevent fraudulent certificates. For reconnaissance, they offer a window into potentially overlooked subdomains.</p>
<p>The <code>crt.sh</code> website provides a searchable interface for CT logs. To efficiently extract subdomains using <code>crt.sh</code> within your terminal, you can use a command like this:</p>
<p>Code: bash</p>
<pre><code class="lang-bash">curl -<span class="hljs-keyword">s</span> <span class="hljs-string">"https://crt.sh/?q=%25.example.com&amp;output=json"</span> | jq -r <span class="hljs-string">'.[].name_value'</span> | sed <span class="hljs-string">'s/\*\.//g'</span> | <span class="hljs-keyword">sort</span> -u
</code></pre>
<p>This command fetches JSON-formatted data from <code>crt.sh</code> for <code>example.com</code> (the <code>%</code> is a wildcard), extracts domain names using <code>jq</code>, removes any wildcard prefixes (<code>*.</code>) with <code>sed</code>, and finally sorts and deduplicates the results.</p>
<h3 id="web-crawling">Web Crawling</h3>
<p>Web crawling is the automated exploration of a website&#39;s structure. A web crawler, or spider, systematically navigates through web pages by following links, mimicking a user&#39;s browsing behavior. This process maps out the site&#39;s architecture and gathers valuable information embedded within the pages.</p>
<p>A crucial file that guides web crawlers is <code>robots.txt</code>. This file resides in a website&#39;s root directory and dictates which areas are off-limits for crawlers. Analyzing <code>robots.txt</code> can reveal hidden directories or sensitive areas that the website owner doesn&#39;t want to be indexed by search engines.</p>
<p><code>Scrapy</code> is a powerful and efficient Python framework for large-scale web crawling and scraping projects. It provides a structured approach to defining crawling rules, extracting data, and handling various output formats.</p>
<p>Here&#39;s a basic Scrapy spider example to extract links from <code>example.com</code>:</p>
<p>Code: python</p>
<pre><code class="lang-python">import scrapy

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ExampleSpider</span>(<span class="hljs-title">scrapy</span>.<span class="hljs-title">Spider</span>):</span>
    name = <span class="hljs-string">"example"</span>
    start_urls = [<span class="hljs-string">'http://example.com/'</span>]

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">parse</span><span class="hljs-params">(<span class="hljs-keyword">self</span>, response)</span></span>:
        <span class="hljs-keyword">for</span> link <span class="hljs-keyword">in</span> response.css(<span class="hljs-string">'a::attr(href)'</span>).getall():
            <span class="hljs-keyword">if</span> any(link.endswith(ext) <span class="hljs-keyword">for</span> ext <span class="hljs-keyword">in</span> <span class="hljs-keyword">self</span>.interesting_extensions):
                <span class="hljs-keyword">yield</span> {<span class="hljs-string">"file"</span>: link}
            elif <span class="hljs-keyword">not</span> link.startswith(<span class="hljs-string">"#"</span>) <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> link.startswith(<span class="hljs-string">"mailto:"</span>):
                <span class="hljs-keyword">yield</span> response.follow(link, callback=<span class="hljs-keyword">self</span>.parse)
</code></pre>
<p>After running the Scrapy spider, you&#39;ll have a file containing scraped data (e.g., <code>example_data.json</code>). You can analyze these results using standard command-line tools. For instance, to extract all links:</p>
<p>Code: bash</p>
<pre><code class="lang-bash">jq -r '.[] | <span class="hljs-keyword">select</span>(.<span class="hljs-keyword">file</span> != <span class="hljs-keyword">null</span>) | .<span class="hljs-keyword">file</span>' example_data.json | sort -u
</code></pre>
<p>This command uses <code>jq</code> to extract links, <code>awk</code> to isolate file extensions, <code>sort</code> to order them, and <code>uniq -c</code> to count their occurrences. By scrutinizing the extracted data, you can identify patterns, anomalies, or sensitive files that might be of interest for further investigation.</p>
<h3 id="search-engine-discovery">Search Engine Discovery</h3>
<p>Leveraging search engines for reconnaissance involves utilizing their vast indexes of web content to uncover information about your target. This passive technique, often referred to as Open Source Intelligence (OSINT) gathering, can yield valuable insights without directly interacting with the target&#39;s systems.</p>
<p>By employing advanced search operators and specialized queries known as &quot;Google Dorks,&quot; you can pinpoint specific information buried within search results. Here&#39;s a table of some useful search operators for web reconnaissance:</p>
<table>
<thead>
<tr>
<th>Operator</th>
<th>Description</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>site:</code></td>
<td>Restricts search results to a specific website.</td>
<td><code>site:example.com &quot;password reset&quot;</code></td>
</tr>
<tr>
<td><code>inurl:</code></td>
<td>Searches for a specific term in the URL of a page.</td>
<td><code>inurl:admin login</code></td>
</tr>
<tr>
<td><code>filetype:</code></td>
<td>Limits results to files of a specific type.</td>
<td><code>filetype:pdf &quot;confidential report&quot;</code></td>
</tr>
<tr>
<td><code>intitle:</code></td>
<td>Searches for a term within the title of a page.</td>
<td><code>intitle:&quot;index of&quot; /backup</code></td>
</tr>
<tr>
<td><code>cache:</code></td>
<td>Shows the cached version of a webpage.</td>
<td><code>cache:example.com</code></td>
</tr>
<tr>
<td><code>&quot;search term&quot;</code></td>
<td>Searches for the exact phrase within quotation marks.</td>
<td><code>&quot;internal error&quot; site:example.com</code></td>
</tr>
<tr>
<td><code>OR</code></td>
<td>Combines multiple search terms.</td>
<td><code>inurl:admin OR inurl:login</code></td>
</tr>
<tr>
<td><code>-</code></td>
<td>Excludes specific terms from search results.</td>
<td><code>inurl:admin -intext:wordpress</code></td>
</tr>
</tbody>
</table>
<p>By creatively combining these operators and crafting targeted queries, you can uncover sensitive documents, exposed directories, login pages, and other valuable information that may aid in your reconnaissance efforts.</p>
<h3 id="web-archives">Web Archives</h3>
<p>Web archives are digital repositories that store snapshots of websites across time, providing a historical record of their evolution. Among these archives, the Wayback Machine is the most comprehensive and accessible resource for web reconnaissance.</p>
<p>The Wayback Machine, a project by the Internet Archive, has been archiving the web for over two decades, capturing billions of web pages from across the globe. This massive historical data collection can be an invaluable resource for security researchers and investigators.</p>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Description</th>
<th>Use Case in Reconnaissance</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Historical Snapshots</code></td>
<td>View past versions of websites, including pages, content, and design changes.</td>
<td>Identify past website content or functionality that is no longer available.</td>
</tr>
<tr>
<td><code>Hidden Directories</code></td>
<td>Explore directories and files that may have been removed or hidden from the current version of the website.</td>
<td>Discover sensitive information or backups that were inadvertently left accessible in previous versions.</td>
</tr>
<tr>
<td><code>Content Changes</code></td>
<td>Track changes in website content, including text, images, and links.</td>
<td>Identify patterns in content updates and assess the evolution of a website&#39;s security posture.</td>
</tr>
</tbody>
</table>
<p>By leveraging the Wayback Machine, you can gain a historical perspective on your target&#39;s online presence, potentially revealing vulnerabilities that may have been overlooked in the current version of the website.</p>
<h3 id="whois">WHOIS</h3>
<table>
<thead>
<tr>
<th><strong>Command</strong></th>
<th><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>export TARGET=&quot;domain.tld&quot;</code></td>
<td>Assign target to an environment variable.</td>
</tr>
<tr>
<td><code>whois $TARGET</code></td>
<td>WHOIS lookup for the target.</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="dns-enumeration">DNS Enumeration</h3>
<table>
<thead>
<tr>
<th><strong>Command</strong></th>
<th><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>nslookup $TARGET</code></td>
<td>Identify the <code>A</code> record for the target domain.</td>
</tr>
<tr>
<td><code>nslookup -query=A $TARGET</code></td>
<td>Identify the <code>A</code> record for the target domain.</td>
</tr>
<tr>
<td><code>dig $TARGET @&lt;nameserver/IP&gt;</code></td>
<td>Identify the <code>A</code> record for the target domain.</td>
</tr>
<tr>
<td><code>dig a $TARGET @&lt;nameserver/IP&gt;</code></td>
<td>Identify the <code>A</code> record for the target domain.</td>
</tr>
<tr>
<td><code>nslookup -query=PTR &lt;IP&gt;</code></td>
<td>Identify the <code>PTR</code> record for the target IP address.</td>
</tr>
<tr>
<td><code>dig -x &lt;IP&gt; @&lt;nameserver/IP&gt;</code></td>
<td>Identify the <code>PTR</code> record for the target IP address.</td>
</tr>
<tr>
<td><code>nslookup -query=ANY $TARGET</code></td>
<td>Identify <code>ANY</code> records for the target domain.</td>
</tr>
<tr>
<td><code>dig any $TARGET @&lt;nameserver/IP&gt;</code></td>
<td>Identify <code>ANY</code> records for the target domain.</td>
</tr>
<tr>
<td><code>nslookup -query=TXT $TARGET</code></td>
<td>Identify the <code>TXT</code> records for the target domain.</td>
</tr>
<tr>
<td><code>dig txt $TARGET @&lt;nameserver/IP&gt;</code></td>
<td>Identify the <code>TXT</code> records for the target domain.</td>
</tr>
<tr>
<td><code>nslookup -query=MX $TARGET</code></td>
<td>Identify the <code>MX</code> records for the target domain.</td>
</tr>
<tr>
<td><code>dig mx $TARGET @&lt;nameserver/IP&gt;</code></td>
<td>Identify the <code>MX</code> records for the target domain.</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="passive-subdomain-enumeration">Passive Subdomain Enumeration</h3>
<table>
<thead>
<tr>
<th><strong>Resource/Command</strong></th>
<th><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>VirusTotal</code></td>
<td><a href="https://www.virustotal.com/gui/home/url">https://www.virustotal.com/gui/home/url</a></td>
</tr>
<tr>
<td><code>Censys</code></td>
<td><a href="https://censys.io/">https://censys.io/</a></td>
</tr>
<tr>
<td><code>Crt.sh</code></td>
<td><a href="https://crt.sh/">https://crt.sh/</a></td>
</tr>
<tr>
<td>`curl -s <a href="https://sonar.omnisint.io/subdomains/{domain}">https://sonar.omnisint.io/subdomains/{domain}</a> \</td>
<td>jq -r &#39;.[]&#39; \</td>
<td>sort -u`</td>
<td>All subdomains for a given domain.</td>
</tr>
<tr>
<td>`curl -s <a href="https://sonar.omnisint.io/tlds/{domain}">https://sonar.omnisint.io/tlds/{domain}</a> \</td>
<td>jq -r &#39;.[]&#39; \</td>
<td>sort -u`</td>
<td>All TLDs found for a given domain.</td>
</tr>
<tr>
<td>`curl -s <a href="https://sonar.omnisint.io/all/{domain}">https://sonar.omnisint.io/all/{domain}</a> \</td>
<td>jq -r &#39;.[]&#39; \</td>
<td>sort -u`</td>
<td>All results across all TLDs for a given domain.</td>
</tr>
<tr>
<td>`curl -s <a href="https://sonar.omnisint.io/reverse/{ip}">https://sonar.omnisint.io/reverse/{ip}</a> \</td>
<td>jq -r &#39;.[]&#39; \</td>
<td>sort -u`</td>
<td>Reverse DNS lookup on IP address.</td>
</tr>
<tr>
<td>`curl -s <a href="https://sonar.omnisint.io/reverse/{ip}/{mask}">https://sonar.omnisint.io/reverse/{ip}/{mask}</a> \</td>
<td>jq -r &#39;.[]&#39; \</td>
<td>sort -u`</td>
<td>Reverse DNS lookup of a CIDR range.</td>
</tr>
<tr>
<td>`curl -s &quot;<a href="https://crt.sh/?q=${TARGET}&amp;output=json">https://crt.sh/?q=${TARGET}&amp;output=json</a>&quot; \</td>
<td>jq -r &#39;.[] \</td>
<td>&quot;(.name_value)\n(.common_name)&quot;&#39; \</td>
<td>sort -u`</td>
<td>Certificate Transparency.</td>
</tr>
<tr>
<td>`cat sources.txt \</td>
<td>while read source; do theHarvester -d &quot;${TARGET}&quot; -b $source -f &quot;${source}-${TARGET}&quot;;done`</td>
<td>Searching for subdomains and other information on the sources provided in the source.txt list.</td>
</tr>
</tbody>
</table>
<p><strong>Sources.txt</strong></p>
<pre><code class="lang-txt"><span class="hljs-keyword">baidu
</span><span class="hljs-keyword">bufferoverun
</span><span class="hljs-symbol">crtsh</span>
<span class="hljs-symbol">hackertarget</span>
<span class="hljs-symbol">otx</span>
<span class="hljs-symbol">projecdiscovery</span>
<span class="hljs-symbol">rapiddns</span>
<span class="hljs-keyword">sublist3r
</span><span class="hljs-symbol">threatcrowd</span>
<span class="hljs-symbol">trello</span>
<span class="hljs-symbol">urlscan</span>
<span class="hljs-symbol">vhost</span>
<span class="hljs-symbol">virustotal</span>
<span class="hljs-symbol">zoomeye</span>
</code></pre>
<hr>
<h3 id="passive-infrastructure-identification">Passive Infrastructure Identification</h3>
<table>
<thead>
<tr>
<th><strong>Resource/Command</strong></th>
<th><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Netcraft</code></td>
<td><a href="https://www.netcraft.com/">https://www.netcraft.com/</a></td>
</tr>
<tr>
<td><code>WayBackMachine</code></td>
<td><a href="http://web.archive.org/">http://web.archive.org/</a></td>
</tr>
<tr>
<td><code>WayBackURLs</code></td>
<td><a href="https://github.com/tomnomnom/waybackurls">https://github.com/tomnomnom/waybackurls</a></td>
</tr>
<tr>
<td><code>waybackurls -dates https://$TARGET &gt; waybackurls.txt</code></td>
<td>Crawling URLs from a domain with the date it was obtained.</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="active-infrastructure-identification">Active Infrastructure Identification</h3>
<table>
<thead>
<tr>
<th><strong>Resource/Command</strong></th>
<th><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>curl -I &quot;http://${TARGET}&quot;</code></td>
<td>Display HTTP headers of the target webserver.</td>
</tr>
<tr>
<td><code>whatweb -a https://www.facebook.com -v</code></td>
<td>Technology identification.</td>
</tr>
<tr>
<td><code>Wappalyzer</code></td>
<td><a href="https://www.wappalyzer.com/">https://www.wappalyzer.com/</a></td>
</tr>
<tr>
<td><code>wafw00f -v https://$TARGET</code></td>
<td>WAF Fingerprinting.</td>
</tr>
<tr>
<td><code>Aquatone</code></td>
<td><a href="https://github.com/michenriksen/aquatone">https://github.com/michenriksen/aquatone</a></td>
</tr>
<tr>
<td>`cat subdomain.list \</td>
<td>aquatone -out ./aquatone -screenshot-timeout 1000`</td>
<td>Makes screenshots of all subdomains in the subdomain.list.</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="active-subdomain-enumeration">Active Subdomain Enumeration</h3>
<table>
<thead>
<tr>
<th><strong>Resource/Command</strong></th>
<th><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>HackerTarget</code></td>
<td><a href="https://hackertarget.com/zone-transfer/">https://hackertarget.com/zone-transfer/</a></td>
</tr>
<tr>
<td><code>SecLists</code></td>
<td><a href="https://github.com/danielmiessler/SecLists">https://github.com/danielmiessler/SecLists</a></td>
</tr>
<tr>
<td><code>nslookup -type=any -query=AXFR $TARGET nameserver.target.domain</code></td>
<td>Zone Transfer using Nslookup against the target domain and its nameserver.</td>
</tr>
<tr>
<td><code>gobuster dns -q -r &quot;${NS}&quot; -d &quot;${TARGET}&quot; -w &quot;${WORDLIST}&quot; -p ./patterns.txt -o &quot;gobuster_${TARGET}.txt&quot;</code></td>
<td>Bruteforcing subdomains.</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="virtual-hosts">Virtual Hosts</h3>
<table>
<thead>
<tr>
<th><strong>Resource/Command</strong></th>
<th><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>curl -s http://192.168.10.10 -H &quot;Host: randomtarget.com&quot;</code></td>
<td>Changing the HOST HTTP header to request a specific domain.</td>
</tr>
<tr>
<td>`cat ./vhosts.list \</td>
<td>while read vhost;do echo &quot;\n<strong><em>**</em></strong>\nFUZZING: ${vhost}\n<strong><em>**</em></strong>&quot;;curl -s -I http://<IP address> -H &quot;HOST: ${vhost}.target.domain&quot; \</td>
<td>grep &quot;Content-Length: &quot;;done`</td>
<td>Bruteforcing for possible virtual hosts on the target domain.</td>
</tr>
<tr>
<td><code>ffuf -w ./vhosts -u http://&lt;IP address&gt; -H &quot;HOST: FUZZ.target.domain&quot; -fs 612</code></td>
<td>Bruteforcing for possible virtual hosts on the target domain using <code>ffuf</code>.</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="crawling">Crawling</h3>
<table>
<thead>
<tr>
<th><strong>Resource/Command</strong></th>
<th><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>ZAP</code></td>
<td><a href="https://www.zaproxy.org/">https://www.zaproxy.org/</a></td>
</tr>
<tr>
<td><code>ffuf -recursion -recursion-depth 1 -u http://192.168.10.10/FUZZ -w /opt/useful/SecLists/Discovery/Web-Content/raft-small-directories-lowercase.txt</code></td>
<td>Discovering files and folders that cannot be spotted by browsing the website.</td>
</tr>
<tr>
<td><code>ffuf -w ./folders.txt:FOLDERS,./wordlist.txt:WORDLIST,./extensions.txt:EXTENSIONS -u http://www.target.domain/FOLDERS/WORDLISTEXTENSIONS</code></td>
<td>Mutated bruteforcing against the target web server.</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="introduction">Introduction</h2>
<hr>
<p><code>Web Reconnaissance</code> is the foundation of a thorough security assessment. This process involves systematically and meticulously collecting information about a target website or web application. Think of it as the preparatory phase before delving into deeper analysis and potential exploitation. It forms a critical part of the &quot;<code>Information Gathering</code>&quot; phase of the Penetration Testing Process.</p>
<p><img src="https://academy.hackthebox.com/storage/modules/144/PT-process.png" alt="Flowchart of the Penetration Testing Process: Pre-Engagement, Information Gathering, Vulnerability Assessment, Exploitation, Post-Exploitation, Lateral Movement, Proof-of-Concept, and Post-Engagement."></p>
<p>The primary goals of web reconnaissance include:</p>
<ul>
<li><code>Identifying Assets</code>: Uncovering all publicly accessible components of the target, such as web pages, subdomains, IP addresses, and technologies used. This step provides a comprehensive overview of the target&#39;s online presence.</li>
<li><code>Discovering Hidden Information</code>: Locating sensitive information that might be inadvertently exposed, including backup files, configuration files, or internal documentation. These findings can reveal valuable insights and potential entry points for attacks.</li>
<li><code>Analysing the Attack Surface</code>: Examining the target&#39;s attack surface to identify potential vulnerabilities and weaknesses. This involves assessing the technologies used, configurations, and possible entry points for exploitation.</li>
<li><code>Gathering Intelligence</code>: Collecting information that can be leveraged for further exploitation or social engineering attacks. This includes identifying key personnel, email addresses, or patterns of behaviour that could be exploited.</li>
</ul>
<p>Attackers leverage this information to tailor their attacks, allowing them to target specific weaknesses and bypass security measures. Conversely, defenders use recon to proactively identify and patch vulnerabilities before malicious actors can leverage them.</p>
<h3 id="types-of-reconnaissance">Types of Reconnaissance</h3>
<p>Web reconnaissance encompasses two fundamental methodologies: <code>active</code> and <code>passive</code> reconnaissance. Each approach offers distinct advantages and challenges, and understanding their differences is crucial for adequate information gathering.</p>
<h4 id="active-reconnaissance">Active Reconnaissance</h4>
<p>In active reconnaissance, the attacker <code>directly interacts with the target system</code> to gather information. This interaction can take various forms:</p>
<table>
<thead>
<tr>
<th>Technique</th>
<th>Description</th>
<th>Example</th>
<th>Tools</th>
<th>Risk of Detection</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Port Scanning</code></td>
<td>Identifying open ports and services running on the target.</td>
<td>Using Nmap to scan a web server for open ports like 80 (HTTP) and 443 (HTTPS).</td>
<td>Nmap, Masscan, Unicornscan</td>
<td>High: Direct interaction with the target can trigger intrusion detection systems (IDS) and firewalls.</td>
</tr>
<tr>
<td><code>Vulnerability Scanning</code></td>
<td>Probing the target for known vulnerabilities, such as outdated software or misconfigurations.</td>
<td>Running Nessus against a web application to check for SQL injection flaws or cross-site scripting (XSS) vulnerabilities.</td>
<td>Nessus, OpenVAS, Nikto</td>
<td>High: Vulnerability scanners send exploit payloads that security solutions can detect.</td>
</tr>
<tr>
<td><code>Network Mapping</code></td>
<td>Mapping the target&#39;s network topology, including connected devices and their relationships.</td>
<td>Using traceroute to determine the path packets take to reach the target server, revealing potential network hops and infrastructure.</td>
<td>Traceroute, Nmap</td>
<td>Medium to High: Excessive or unusual network traffic can raise suspicion.</td>
</tr>
<tr>
<td><code>Banner Grabbing</code></td>
<td>Retrieving information from banners displayed by services running on the target.</td>
<td>Connecting to a web server on port 80 and examining the HTTP banner to identify the web server software and version.</td>
<td>Netcat, curl</td>
<td>Low: Banner grabbing typically involves minimal interaction but can still be logged.</td>
</tr>
<tr>
<td><code>OS Fingerprinting</code></td>
<td>Identifying the operating system running on the target.</td>
<td>Using Nmap&#39;s OS detection capabilities (<code>-O</code>) to determine if the target is running Windows, Linux, or another OS.</td>
<td>Nmap, Xprobe2</td>
<td>Low: OS fingerprinting is usually passive, but some advanced techniques can be detected.</td>
</tr>
<tr>
<td><code>Service Enumeration</code></td>
<td>Determining the specific versions of services running on open ports.</td>
<td>Using Nmap&#39;s service version detection (<code>-sV</code>) to determine if a web server is running Apache 2.4.50 or Nginx 1.18.0.</td>
<td>Nmap</td>
<td>Low: Similar to banner grabbing, service enumeration can be logged but is less likely to trigger alerts.</td>
</tr>
<tr>
<td><code>Web Spidering</code></td>
<td>Crawling the target website to identify web pages, directories, and files.</td>
<td>Running a web crawler like Burp Suite Spider or OWASP ZAP Spider to map out the structure of a website and discover hidden resources.</td>
<td>Burp Suite Spider, OWASP ZAP Spider, Scrapy (customisable)</td>
<td>Low to Medium: Can be detected if the crawler&#39;s behaviour is not carefully configured to mimic legitimate traffic.</td>
</tr>
</tbody>
</table>
<p>Active reconnaissance provides a direct and often more comprehensive view of the target&#39;s infrastructure and security posture. However, it also carries a higher risk of detection, as the interactions with the target can trigger alerts or raise suspicion.</p>
<h4 id="passive-reconnaissance">Passive Reconnaissance</h4>
<p>In contrast, passive reconnaissance involves gathering information about the target <code>without directly interacting</code> with it. This relies on analysing publicly available information and resources, such as:</p>
<table>
<thead>
<tr>
<th>Technique</th>
<th>Description</th>
<th>Example</th>
<th>Tools</th>
<th>Risk of Detection</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Search Engine Queries</code></td>
<td>Utilising search engines to uncover information about the target, including websites, social media profiles, and news articles.</td>
<td>Searching Google for &quot;<code>[Target Name] employees</code>&quot; to find employee information or social media profiles.</td>
<td>Google, DuckDuckGo, Bing, and specialised search engines (e.g., Shodan)</td>
<td>Very Low: Search engine queries are normal internet activity and unlikely to trigger alerts.</td>
</tr>
<tr>
<td><code>WHOIS Lookups</code></td>
<td>Querying WHOIS databases to retrieve domain registration details.</td>
<td>Performing a WHOIS lookup on a target domain to find the registrant&#39;s name, contact information, and name servers.</td>
<td>whois command-line tool, online WHOIS lookup services</td>
<td>Very Low: WHOIS queries are legitimate and do not raise suspicion.</td>
</tr>
<tr>
<td><code>DNS</code></td>
<td>Analysing DNS records to identify subdomains, mail servers, and other infrastructure.</td>
<td>Using <code>dig</code> to enumerate subdomains of a target domain.</td>
<td>dig, nslookup, host, dnsenum, fierce, dnsrecon</td>
<td>Very Low: DNS queries are essential for internet browsing and are not typically flagged as suspicious.</td>
</tr>
<tr>
<td><code>Web Archive Analysis</code></td>
<td>Examining historical snapshots of the target&#39;s website to identify changes, vulnerabilities, or hidden information.</td>
<td>Using the Wayback Machine to view past versions of a target website to see how it has changed over time.</td>
<td>Wayback Machine</td>
<td>Very Low: Accessing archived versions of websites is a normal activity.</td>
</tr>
<tr>
<td><code>Social Media Analysis</code></td>
<td>Gathering information from social media platforms like LinkedIn, Twitter, or Facebook.</td>
<td>Searching LinkedIn for employees of a target organisation to learn about their roles, responsibilities, and potential social engineering targets.</td>
<td>LinkedIn, Twitter, Facebook, specialised OSINT tools</td>
<td>Very Low: Accessing public social media profiles is not considered intrusive.</td>
</tr>
<tr>
<td><code>Code Repositories</code></td>
<td>Analysing publicly accessible code repositories like GitHub for exposed credentials or vulnerabilities.</td>
<td>Searching GitHub for code snippets or repositories related to the target that might contain sensitive information or code vulnerabilities.</td>
<td>GitHub, GitLab</td>
<td>Very Low: Code repositories are meant for public access, and searching them is not suspicious.</td>
</tr>
</tbody>
</table>
<p>Passive reconnaissance is generally considered stealthier and less likely to trigger alarms than active reconnaissance. However, it may yield less comprehensive information, as it relies on what&#39;s already publicly accessible.</p>
<p>In this module, we will delve into the essential tools and techniques used in web reconnaissance, starting with WHOIS. Understanding the WHOIS protocol provides a gateway to accessing vital information about domain registrations, ownership details, and the digital infrastructure of targets. This foundational knowledge sets the stage for more advanced recon methods we&#39;ll explore later.</p>
<hr>
<h2 id="whois">WHOIS</h2>
<hr>
<p>WHOIS is a widely used query and response protocol designed to access databases that store information about registered internet resources. Primarily associated with domain names, WHOIS can also provide details about IP address blocks and autonomous systems. Think of it as a giant phonebook for the internet, letting you look up who owns or is responsible for various online assets.</p>
<p>&#x20; WHOIS</p>
<pre><code class="lang-shell-session">root<span class="hljs-meta">@htb</span>[/htb]$ whois inlanefreight.com

[...]
Domain <span class="hljs-string">Name:</span> inlanefreight.com
Registry Domain <span class="hljs-string">ID:</span> <span class="hljs-number">2420436757</span>_DOMAIN_COM-VRSN
Registrar WHOIS <span class="hljs-string">Server:</span> whois.registrar.amazon
Registrar <span class="hljs-string">URL:</span> <span class="hljs-string">https:</span><span class="hljs-comment">//registrar.amazon.com</span>
Updated <span class="hljs-string">Date:</span> <span class="hljs-number">2023</span><span class="hljs-number">-07</span><span class="hljs-number">-03</span><span class="hljs-string">T01:</span><span class="hljs-number">11</span>:<span class="hljs-number">15</span>Z
Creation <span class="hljs-string">Date:</span> <span class="hljs-number">2019</span><span class="hljs-number">-08</span><span class="hljs-number">-05</span><span class="hljs-string">T22:</span><span class="hljs-number">43</span>:<span class="hljs-number">09</span>Z
[...]
</code></pre>
<p>Each WHOIS record typically contains the following information:</p>
<ul>
<li><code>Domain Name</code>: The domain name itself (e.g., example.com)</li>
<li><code>Registrar</code>: The company where the domain was registered (e.g., GoDaddy, Namecheap)</li>
<li><code>Registrant Contact</code>: The person or organization that registered the domain.</li>
<li><code>Administrative Contact</code>: The person responsible for managing the domain.</li>
<li><code>Technical Contact</code>: The person handling technical issues related to the domain.</li>
<li><code>Creation and Expiration Dates</code>: When the domain was registered and when it&#39;s set to expire.</li>
<li><code>Name Servers</code>: Servers that translate the domain name into an IP address.</li>
</ul>
<h3 id="history-of-whois">History of WHOIS</h3>
<p>The history of WHOIS is intrinsically linked to the vision and dedication of <a href="https://en.wikipedia.org/wiki/Elizabeth_J._Feinler">Elizabeth Feinler</a>, a computer scientist who played a pivotal role in shaping the early internet.</p>
<p>In the 1970s, Feinler and her team at the Stanford Research Institute&#39;s Network Information Center (NIC) recognised the need for a system to track and manage the growing number of network resources on the ARPANET, the precursor to the modern internet. Their solution was the creation of the WHOIS directory, a rudimentary yet groundbreaking database that stored information about network users, hostnames, and domain names.</p>
<details>

<summary>Click to expand on an interesting bit of internet history if you are interested</summary>

####

####

####

####

</details>

<p>\</p>
<h3 id="why-whois-matters-for-web-recon">Why WHOIS Matters for Web Recon</h3>
<p>WHOIS data serves as a treasure trove of information for penetration testers during the reconnaissance phase of an assessment. It offers valuable insights into the target organisation&#39;s digital footprint and potential vulnerabilities:</p>
<ul>
<li><code>Identifying Key Personnel</code>: WHOIS records often reveal the names, email addresses, and phone numbers of individuals responsible for managing the domain. This information can be leveraged for social engineering attacks or to identify potential targets for phishing campaigns.</li>
<li><code>Discovering Network Infrastructure</code>: Technical details like name servers and IP addresses provide clues about the target&#39;s network infrastructure. This can help penetration testers identify potential entry points or misconfigurations.</li>
<li><code>Historical Data Analysis</code>: Accessing historical WHOIS records through services like <a href="https://whoisfreaks.com/">WhoisFreaks</a> can reveal changes in ownership, contact information, or technical details over time. This can be useful for tracking the evolution of the target&#39;s digital presence.</li>
</ul>
<hr>
<h2 id="utilising-whois">Utilising WHOIS</h2>
<hr>
<p>Let&#39;s consider three scenarios to help illustrate the value of WHOIS data.</p>
<h3 id="scenario-1-phishing-investigation">Scenario 1: Phishing Investigation</h3>
<p>An email security gateway flags a suspicious email sent to multiple employees within a company. The email claims to be from the company&#39;s bank and urges recipients to click on a link to update their account information. A security analyst investigates the email and begins by performing a WHOIS lookup on the domain linked in the email.</p>
<p>The WHOIS record reveals the following:</p>
<ul>
<li><code>Registration Date</code>: The domain was registered just a few days ago.</li>
<li><code>Registrant</code>: The registrant&#39;s information is hidden behind a privacy service.</li>
<li><code>Name Servers</code>: The name servers are associated with a known bulletproof hosting provider often used for malicious activities.</li>
</ul>
<p>This combination of factors raises significant red flags for the analyst. The recent registration date, hidden registrant information, and suspicious hosting strongly suggest a phishing campaign. The analyst promptly alerts the company&#39;s IT department to block the domain and warns employees about the scam.</p>
<p>Further investigation into the hosting provider and associated IP addresses may uncover additional phishing domains or infrastructure the threat actor uses.</p>
<h3 id="scenario-2-malware-analysis">Scenario 2: Malware Analysis</h3>
<p>A security researcher is analysing a new strain of malware that has infected several systems within a network. The malware communicates with a remote server to receive commands and exfiltrate stolen data. To gain insights into the threat actor&#39;s infrastructure, the researcher performs a WHOIS lookup on the domain associated with the command-and-control (C2) server.</p>
<p>The WHOIS record reveals:</p>
<ul>
<li><code>Registrant</code>: The domain is registered to an individual using a free email service known for anonymity.</li>
<li><code>Location</code>: The registrant&#39;s address is in a country with a high prevalence of cybercrime.</li>
<li><code>Registrar</code>: The domain was registered through a registrar with a history of lax abuse policies.</li>
</ul>
<p>Based on this information, the researcher concludes that the C2 server is likely hosted on a compromised or &quot;bulletproof&quot; server. The researcher then uses the WHOIS data to identify the hosting provider and notify them of the malicious activity.</p>
<h3 id="scenario-3-threat-intelligence-report">Scenario 3: Threat Intelligence Report</h3>
<p>A cybersecurity firm tracks the activities of a sophisticated threat actor group known for targeting financial institutions. Analysts gather WHOIS data on multiple domains associated with the group&#39;s past campaigns to compile a comprehensive threat intelligence report.</p>
<p>By analysing the WHOIS records, analysts uncover the following patterns:</p>
<ul>
<li><code>Registration Dates</code>: The domains were registered in clusters, often shortly before major attacks.</li>
<li><code>Registrants</code>: The registrants use various aliases and fake identities.</li>
<li><code>Name Servers</code>: The domains often share the same name servers, suggesting a common infrastructure.</li>
<li><code>Takedown History</code>: Many domains have been taken down after attacks, indicating previous law enforcement or security interventions.</li>
</ul>
<p>These insights allow analysts to create a detailed profile of the threat actor&#39;s tactics, techniques, and procedures (TTPs). The report includes indicators of compromise (IOCs) based on the WHOIS data, which other organisations can use to detect and block future attacks.</p>
<h3 id="using-whois">Using WHOIS</h3>
<p>Before using the <code>whois</code> command, you&#39;ll need to ensure it&#39;s installed on your Linux system. It&#39;s a utility available through linux package managers, and if it&#39;s not installed, it can be installed simply with</p>
<p>&#x20; Utilising WHOIS</p>
<pre><code class="lang-shell-session">root@htb<span class="hljs-string">[/htb]</span>$ sudo apt update
root@htb<span class="hljs-string">[/htb]</span>$ sudo apt install whois -y
</code></pre>
<p>The simplest way to access WHOIS data is through the <code>whois</code> command-line tool. Let&#39;s perform a WHOIS lookup on <code>facebook.com</code>:</p>
<p>&#x20; Utilising WHOIS</p>
<pre><code class="lang-shell-session">root<span class="hljs-meta">@htb</span>[/htb]$ whois facebook.com

   Domain <span class="hljs-string">Name:</span> FACEBOOK.COM
   Registry Domain <span class="hljs-string">ID:</span> <span class="hljs-number">2320948</span>_DOMAIN_COM-VRSN
   Registrar WHOIS <span class="hljs-string">Server:</span> whois.registrarsafe.com
   Registrar <span class="hljs-string">URL:</span> <span class="hljs-string">http:</span><span class="hljs-comment">//www.registrarsafe.com</span>
   Updated <span class="hljs-string">Date:</span> <span class="hljs-number">2024</span><span class="hljs-number">-04</span><span class="hljs-number">-24</span><span class="hljs-string">T19:</span><span class="hljs-number">06</span>:<span class="hljs-number">12</span>Z
   Creation <span class="hljs-string">Date:</span> <span class="hljs-number">1997</span><span class="hljs-number">-03</span><span class="hljs-number">-29</span><span class="hljs-string">T05:</span><span class="hljs-number">00</span>:<span class="hljs-number">00</span>Z
   Registry Expiry <span class="hljs-string">Date:</span> <span class="hljs-number">2033</span><span class="hljs-number">-03</span><span class="hljs-number">-30</span><span class="hljs-string">T04:</span><span class="hljs-number">00</span>:<span class="hljs-number">00</span>Z
<span class="hljs-symbol">   Registrar:</span> RegistrarSafe, LLC
   Registrar IANA <span class="hljs-string">ID:</span> <span class="hljs-number">3237</span>
   Registrar Abuse Contact <span class="hljs-string">Email:</span> abusecomplaints<span class="hljs-meta">@registrarsafe</span>.com
   Registrar Abuse Contact <span class="hljs-string">Phone:</span> +<span class="hljs-number">1</span><span class="hljs-number">-650</span><span class="hljs-number">-308</span><span class="hljs-number">-7004</span>
   Domain <span class="hljs-string">Status:</span> clientDeleteProhibited <span class="hljs-string">https:</span><span class="hljs-comment">//icann.org/epp#clientDeleteProhibited</span>
   Domain <span class="hljs-string">Status:</span> clientTransferProhibited <span class="hljs-string">https:</span><span class="hljs-comment">//icann.org/epp#clientTransferProhibited</span>
   Domain <span class="hljs-string">Status:</span> clientUpdateProhibited <span class="hljs-string">https:</span><span class="hljs-comment">//icann.org/epp#clientUpdateProhibited</span>
   Domain <span class="hljs-string">Status:</span> serverDeleteProhibited <span class="hljs-string">https:</span><span class="hljs-comment">//icann.org/epp#serverDeleteProhibited</span>
   Domain <span class="hljs-string">Status:</span> serverTransferProhibited <span class="hljs-string">https:</span><span class="hljs-comment">//icann.org/epp#serverTransferProhibited</span>
   Domain <span class="hljs-string">Status:</span> serverUpdateProhibited <span class="hljs-string">https:</span><span class="hljs-comment">//icann.org/epp#serverUpdateProhibited</span>
   Name <span class="hljs-string">Server:</span> A.NS.FACEBOOK.COM
   Name <span class="hljs-string">Server:</span> B.NS.FACEBOOK.COM
   Name <span class="hljs-string">Server:</span> C.NS.FACEBOOK.COM
   Name <span class="hljs-string">Server:</span> D.NS.FACEBOOK.COM
<span class="hljs-symbol">   DNSSEC:</span> unsigned
   URL of the ICANN Whois Inaccuracy Complaint <span class="hljs-string">Form:</span> <span class="hljs-string">https:</span><span class="hljs-comment">//www.icann.org/wicf/</span>
&gt;&gt;&gt; Last update of whois <span class="hljs-string">database:</span> <span class="hljs-number">2024</span><span class="hljs-number">-06</span><span class="hljs-number">-01</span><span class="hljs-string">T11:</span><span class="hljs-number">24</span>:<span class="hljs-number">10</span>Z &lt;&lt;&lt;

[...]
Registry Registrant <span class="hljs-string">ID:</span>
Registrant <span class="hljs-string">Name:</span> Domain Admin
Registrant <span class="hljs-string">Organization:</span> Meta Platforms, Inc.
[...]
</code></pre>
<p>The WHOIS output for <code>facebook.com</code> reveals several key details:</p>
<ol>
<li><p><code>Domain Registration</code>:</p>
<ul>
<li><code>Registrar</code>: RegistrarSafe, LLC</li>
<li><code>Creation Date</code>: 1997-03-29</li>
<li><code>Expiry Date</code>: 2033-03-30</li>
</ul>
<p>These details indicate that the domain is registered with RegistrarSafe, LLC, and has been active for a considerable period, suggesting its legitimacy and established online presence. The distant expiry date further reinforces its longevity.</p>
</li>
<li><p><code>Domain Owner</code>:</p>
<ul>
<li><code>Registrant/Admin/Tech Organization</code>: Meta Platforms, Inc.</li>
<li><code>Registrant/Admin/Tech Contact</code>: Domain Admin</li>
</ul>
<p>This information identifies Meta Platforms, Inc. as the organization behind <code>facebook.com</code>, and &quot;Domain Admin&quot; as the point of contact for domain-related matters. This is consistent with the expectation that Facebook, a prominent social media platform, is owned by Meta Platforms, Inc.</p>
</li>
<li><p><code>Domain Status</code>:</p>
<ul>
<li><code>clientDeleteProhibited</code>, <code>clientTransferProhibited</code>, <code>clientUpdateProhibited</code>, <code>serverDeleteProhibited</code>, <code>serverTransferProhibited</code>, and <code>serverUpdateProhibited</code></li>
</ul>
<p>These statuses indicate that the domain is protected against unauthorized changes, transfers, or deletions on both the client and server sides. This highlights a strong emphasis on security and control over the domain.</p>
</li>
<li><p><code>Name Servers</code>:</p>
<ul>
<li><code>A.NS.FACEBOOK.COM</code>, <code>B.NS.FACEBOOK.COM</code>, <code>C.NS.FACEBOOK.COM</code>, <code>D.NS.FACEBOOK.COM</code></li>
</ul>
<p>These name servers are all within the <code>facebook.com</code> domain, suggesting that Meta Platforms, Inc. manages its DNS infrastructure. It is common practice for large organizations to maintain control and reliability over their DNS resolution.</p>
</li>
</ol>
<p>Overall, the WHOIS output for <code>facebook.com</code> aligns with expectations for a well-established and secure domain owned by a large organization like Meta Platforms, Inc.</p>
<p>While the WHOIS record provides contact information for domain-related issues, it might not be directly helpful in identifying individual employees or specific vulnerabilities. This highlights the need to combine WHOIS data with other reconnaissance techniques to understand the target&#39;s digital footprint comprehensively.</p>
<hr>
<h2 id="dns">DNS</h2>
<hr>
<p>The <code>Domain Name System</code> (<code>DNS</code>) acts as the internet&#39;s GPS, guiding your online journey from memorable landmarks (domain names) to precise numerical coordinates (IP addresses). Much like how GPS translates a destination name into latitude and longitude for navigation, DNS translates human-readable domain names (like <code>www.example.com</code>) into the numerical IP addresses (like <code>192.0.2.1</code>) that computers use to communicate.</p>
<p>Imagine navigating a city by memorizing the exact latitude and longitude of every location you want to visit. It would be incredibly cumbersome and inefficient. DNS eliminates this complexity by allowing us to use easy-to-remember domain names instead. When you type a domain name into your browser, DNS acts as your navigator, swiftly finding the corresponding IP address and directing your request to the correct destination on the internet.</p>
<p>Without DNS, navigating the online world would be akin to driving without a map or GPS â€“ a frustrating and error-prone endeavour.</p>
<h3 id="how-dns-works">How DNS Works</h3>
<p>Imagine you want to visit a website like <code>www.example.com</code>. You type this friendly domain name into your browser, but your computer doesn&#39;t understand words â€“ it speaks the language of numbers, specifically IP addresses. So, how does your computer find the website&#39;s IP address? Enter DNS, the internet&#39;s trusty translator.</p>
<p><img src="https://mermaid.ink/svg/pako:eNptkk1uwjAQha8y8rpcIItWkAAtUNQmlSrksDDxlEQQT-QfJIS4ex2nNG1arzx-n5-ex3NhBUlkEdtr0ZSwSnMFfhm36w425DTEVDfOou60do15XGJxMBCLosQtjEb3MLk8vcCMnJIP156ceA3WFIiYZ6ikgWSdwatDfQZLkKKh4wn1dnBngyZceuQxKYWFNS39jjtTWfyCvdsgb2t9c-wN4-CU_A7dy8mPjFOeYuG0qU4IK6KDa4bgLdjck9ZpZcC_20e7dWmYbRroGU-JLKxFjZCh7h88C_KCv62Sf9RFUJd87GxJurLCtsH-cssuUlfMu8blit2xGnUtKul_-NKKObMl1pizyG-l0Iec5erqOeEsZWdVsMhqh3dMk9uXLPoQR-Mr10hhMamE73L9fYqysqSfuwEKc3T9BOe0sj4" alt="Flowchart showing two main sections: User Database and Data Collection. Includes steps like &#39;Check User&#39;, &#39;Send to Source Database&#39;, &#39;Store Data&#39;, and &#39;Send to UI System&#39;."></p>
<ol>
<li><code>Your Computer Asks for Directions (DNS Query)</code>: When you enter the domain name, your computer first checks its memory (cache) to see if it remembers the IP address from a previous visit. If not, it reaches out to a DNS resolver, usually provided by your internet service provider (ISP).</li>
<li><code>The DNS Resolver Checks its Map (Recursive Lookup)</code>: The resolver also has a cache, and if it doesn&#39;t find the IP address there, it starts a journey through the DNS hierarchy. It begins by asking a root name server, which is like the librarian of the internet.</li>
<li><code>Root Name Server Points the Way</code>: The root server doesn&#39;t know the exact address but knows who does â€“ the Top-Level Domain (TLD) name server responsible for the domain&#39;s ending (e.g., .com, .org). It points the resolver in the right direction.</li>
<li><code>TLD Name Server Narrows It Down</code>: The TLD name server is like a regional map. It knows which authoritative name server is responsible for the specific domain you&#39;re looking for (e.g., <code>example.com</code>) and sends the resolver there.</li>
<li><code>Authoritative Name Server Delivers the Address</code>: The authoritative name server is the final stop. It&#39;s like the street address of the website you want. It holds the correct IP address and sends it back to the resolver.</li>
<li><code>The DNS Resolver Returns the Information</code>: The resolver receives the IP address and gives it to your computer. It also remembers it for a while (caches it), in case you want to revisit the website soon.</li>
<li><code>Your Computer Connects</code>: Now that your computer knows the IP address, it can connect directly to the web server hosting the website, and you can start browsing.</li>
</ol>
<h4 id="the-hosts-file">The Hosts File</h4>
<p>The <code>hosts</code> file is a simple text file used to map hostnames to IP addresses, providing a manual method of domain name resolution that bypasses the DNS process. While DNS automates the translation of domain names to IP addresses, the <code>hosts</code> file allows for direct, local overrides. This can be particularly useful for development, troubleshooting, or blocking websites.</p>
<p>The <code>hosts</code> file is located in <code>C:\Windows\System32\drivers\etc\hosts</code> on Windows and in <code>/etc/hosts</code> on Linux and MacOS. Each line in the file follows the format:</p>
<p>Code: txt</p>
<pre><code class="lang-txt">&lt;<span class="hljs-built_in">IP</span> <span class="hljs-keyword">Address&gt; </span>   &lt;Hostname&gt; [&lt;<span class="hljs-meta">Alias</span>&gt; ...]
</code></pre>
<p>For example:</p>
<p>Code: txt</p>
<pre><code class="lang-txt">127<span class="hljs-selector-class">.0</span><span class="hljs-selector-class">.0</span><span class="hljs-selector-class">.1</span>       <span class="hljs-selector-tag">localhost</span>
192<span class="hljs-selector-class">.168</span><span class="hljs-selector-class">.1</span><span class="hljs-selector-class">.10</span>    <span class="hljs-selector-tag">devserver</span><span class="hljs-selector-class">.local</span>
</code></pre>
<p>To edit the <code>hosts</code> file, open it with a text editor using administrative/root privileges. Add new entries as needed, and then save the file. The changes take effect immediately without requiring a system restart.</p>
<p>Common uses include redirecting a domain to a local server for development:</p>
<p>Code: txt</p>
<pre><code class="lang-txt">127<span class="hljs-selector-class">.0</span><span class="hljs-selector-class">.0</span><span class="hljs-selector-class">.1</span>       <span class="hljs-selector-tag">myapp</span><span class="hljs-selector-class">.local</span>
</code></pre>
<p>testing connectivity by specifying an IP address:</p>
<p>Code: txt</p>
<pre><code class="lang-txt">192<span class="hljs-selector-class">.168</span><span class="hljs-selector-class">.1</span><span class="hljs-selector-class">.20</span>    <span class="hljs-selector-tag">testserver</span><span class="hljs-selector-class">.local</span>
</code></pre>
<p>or blocking unwanted websites by redirecting their domains to a non-existent IP address:</p>
<p>Code: txt</p>
<pre><code class="lang-txt">0<span class="hljs-selector-class">.0</span><span class="hljs-selector-class">.0</span><span class="hljs-selector-class">.0</span>       <span class="hljs-selector-tag">unwanted-site</span><span class="hljs-selector-class">.com</span>
</code></pre>
<h4 id="it-s-like-a-relay-race">It&#39;s Like a Relay Race</h4>
<p>Think of the DNS process as a relay race. Your computer starts with the domain name and passes it along to the resolver. The resolver then passes the request to the root server, the TLD server, and finally, the authoritative server, each one getting closer to the destination. Once the IP address is found, it&#39;s relayed back down the chain to your computer, allowing you to access the website.</p>
<h4 id="key-dns-concepts">Key DNS Concepts</h4>
<p>In the <code>Domain Name System</code> (<code>DNS</code>), a <code>zone</code> is a distinct part of the domain namespace that a specific entity or administrator manages. Think of it as a virtual container for a set of domain names. For example, <code>example.com</code> and all its subdomains (like <code>mail.example.com</code> or <code>blog.example.com</code>) would typically belong to the same DNS zone.</p>
<p>The zone file, a text file residing on a DNS server, defines the resource records (discussed below) within this zone, providing crucial information for translating domain names into IP addresses.</p>
<p>To illustrate, here&#39;s a simplified example of what a zone file, for <code>example.com</code> might look like:</p>
<p>Code: zone</p>
<pre><code class="lang-dns-zone"><span class="hljs-meta">$TTL</span> <span class="hljs-number">3600</span> <span class="hljs-comment">; Default Time-To-Live (1 hour)</span>
@       <span class="hljs-keyword">IN</span> <span class="hljs-keyword">SOA</span>   ns1.example.com. admin.example.com. (
                <span class="hljs-number">2024060401</span> <span class="hljs-comment">; Serial number (YYYYMMDDNN)</span>
                <span class="hljs-number">3600</span>       <span class="hljs-comment">; Refresh interval</span>
                <span class="hljs-number">900</span>        <span class="hljs-comment">; Retry interval</span>
                <span class="hljs-number">604800</span>     <span class="hljs-comment">; Expire time</span>
                <span class="hljs-number">86400</span> )    <span class="hljs-comment">; Minimum TTL</span>

@       <span class="hljs-keyword">IN</span> <span class="hljs-keyword">NS</span>    ns1.example.com.
@       <span class="hljs-keyword">IN</span> <span class="hljs-keyword">NS</span>    ns2.example.com.
@       <span class="hljs-keyword">IN</span> <span class="hljs-keyword">MX</span> <span class="hljs-number">10</span> mail.example.com.
www     <span class="hljs-keyword">IN</span> <span class="hljs-keyword">A</span>     <span class="hljs-number">192.0.2.1</span>
mail    <span class="hljs-keyword">IN</span> <span class="hljs-keyword">A</span>     <span class="hljs-number">198.51.100.1</span>
ftp     <span class="hljs-keyword">IN</span> <span class="hljs-keyword">CNAME</span> www.example.com.
</code></pre>
<p>This file defines the authoritative name servers (<code>NS</code> records), mail server (<code>MX</code> record), and IP addresses (<code>A</code> records) for various hosts within the <code>example.com</code> domain.</p>
<p>DNS servers store various resource records, each serving a specific purpose in the domain name resolution process. Let&#39;s explore some of the most common DNS concepts:</p>
<table>
<thead>
<tr>
<th>DNS Concept</th>
<th>Description</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Domain Name</code></td>
<td>A human-readable label for a website or other internet resource.</td>
<td><code>www.example.com</code></td>
</tr>
<tr>
<td><code>IP Address</code></td>
<td>A unique numerical identifier assigned to each device connected to the internet.</td>
<td><code>192.0.2.1</code></td>
</tr>
<tr>
<td><code>DNS Resolver</code></td>
<td>A server that translates domain names into IP addresses.</td>
<td>Your ISP&#39;s DNS server or public resolvers like Google DNS (<code>8.8.8.8</code>)</td>
</tr>
<tr>
<td><code>Root Name Server</code></td>
<td>The top-level servers in the DNS hierarchy.</td>
<td>There are 13 root servers worldwide, named A-M: <code>a.root-servers.net</code></td>
</tr>
<tr>
<td><code>TLD Name Server</code></td>
<td>Servers responsible for specific top-level domains (e.g., .com, .org).</td>
<td><a href="https://en.wikipedia.org/wiki/Verisign">Verisign</a> for <code>.com</code>, <a href="https://en.wikipedia.org/wiki/Public_Interest_Registry">PIR</a> for <code>.org</code></td>
</tr>
<tr>
<td><code>Authoritative Name Server</code></td>
<td>The server that holds the actual IP address for a domain.</td>
<td>Often managed by hosting providers or domain registrars.</td>
</tr>
<tr>
<td><code>DNS Record Types</code></td>
<td>Different types of information stored in DNS.</td>
<td>A, AAAA, CNAME, MX, NS, TXT, etc.</td>
</tr>
</tbody>
</table>
<p>Now that we&#39;ve explored the fundamental concepts of DNS, let&#39;s dive deeper into the building blocks of DNS information â€“ the various record types. These records store different types of data associated with domain names, each serving a specific purpose:</p>
<table>
<thead>
<tr>
<th>Record Type</th>
<th>Full Name</th>
<th>Description</th>
<th>Zone File Example</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>A</code></td>
<td>Address Record</td>
<td>Maps a hostname to its IPv4 address.</td>
<td><code>www.example.com.</code> IN A <code>192.0.2.1</code></td>
</tr>
<tr>
<td><code>AAAA</code></td>
<td>IPv6 Address Record</td>
<td>Maps a hostname to its IPv6 address.</td>
<td><code>www.example.com.</code> IN AAAA <code>2001:db8:85a3::8a2e:370:7334</code></td>
</tr>
<tr>
<td><code>CNAME</code></td>
<td>Canonical Name Record</td>
<td>Creates an alias for a hostname, pointing it to another hostname.</td>
<td><code>blog.example.com.</code> IN CNAME <code>webserver.example.net.</code></td>
</tr>
<tr>
<td><code>MX</code></td>
<td>Mail Exchange Record</td>
<td>Specifies the mail server(s) responsible for handling email for the domain.</td>
<td><code>example.com.</code> IN MX 10 <code>mail.example.com.</code></td>
</tr>
<tr>
<td><code>NS</code></td>
<td>Name Server Record</td>
<td>Delegates a DNS zone to a specific authoritative name server.</td>
<td><code>example.com.</code> IN NS <code>ns1.example.com.</code></td>
</tr>
<tr>
<td><code>TXT</code></td>
<td>Text Record</td>
<td>Stores arbitrary text information, often used for domain verification or security policies.</td>
<td><code>example.com.</code> IN TXT <code>&quot;v=spf1 mx -all&quot;</code> (SPF record)</td>
</tr>
<tr>
<td><code>SOA</code></td>
<td>Start of Authority Record</td>
<td>Specifies administrative information about a DNS zone, including the primary name server, responsible person&#39;s email, and other parameters.</td>
<td><code>example.com.</code> IN SOA <code>ns1.example.com. admin.example.com. 2024060301 10800 3600 604800 86400</code></td>
</tr>
<tr>
<td><code>SRV</code></td>
<td>Service Record</td>
<td>Defines the hostname and port number for specific services.</td>
<td><code>_sip._udp.example.com.</code> IN SRV 10 5 5060 <code>sipserver.example.com.</code></td>
</tr>
<tr>
<td><code>PTR</code></td>
<td>Pointer Record</td>
<td>Used for reverse DNS lookups, mapping an IP address to a hostname.</td>
<td><code>1.2.0.192.in-addr.arpa.</code> IN PTR <code>www.example.com.</code></td>
</tr>
</tbody>
</table>
<p>The &quot;<code>IN</code>&quot; in the examples stands for &quot;Internet.&quot; It&#39;s a class field in DNS records that specifies the protocol family. In most cases, you&#39;ll see &quot;<code>IN</code>&quot; used, as it denotes the Internet protocol suite (IP) used for most domain names. Other class values exist (e.g., <code>CH</code> for Chaosnet, <code>HS</code> for Hesiod) but are rarely used in modern DNS configurations.</p>
<p>In essence, &quot;<code>IN</code>&quot; is simply a convention that indicates that the record applies to the standard internet protocols we use today. While it might seem like an extra detail, understanding its meaning provides a deeper understanding of DNS record structure.</p>
<h3 id="why-dns-matters-for-web-recon">Why DNS Matters for Web Recon</h3>
<p>DNS is not merely a technical protocol for translating domain names; it&#39;s a critical component of a target&#39;s infrastructure that can be leveraged to uncover vulnerabilities and gain access during a penetration test:</p>
<ul>
<li><code>Uncovering Assets</code>: DNS records can reveal a wealth of information, including subdomains, mail servers, and name server records. For instance, a <code>CNAME</code> record pointing to an outdated server (<code>dev.example.com</code> CNAME <code>oldserver.example.net</code>) could lead to a vulnerable system.</li>
<li><code>Mapping the Network Infrastructure</code>: You can create a comprehensive map of the target&#39;s network infrastructure by analysing DNS data. For example, identifying the name servers (<code>NS</code> records) for a domain can reveal the hosting provider used, while an <code>A</code> record for <code>loadbalancer.example.com</code> can pinpoint a load balancer. This helps you understand how different systems are connected, identify traffic flow, and pinpoint potential choke points or weaknesses that could be exploited during a penetration test.</li>
<li><code>Monitoring for Changes</code>: Continuously monitoring DNS records can reveal changes in the target&#39;s infrastructure over time. For example, the sudden appearance of a new subdomain (<code>vpn.example.com</code>) might indicate a new entry point into the network, while a <code>TXT</code> record containing a value like <code>_1password=...</code> strongly suggests the organization is using 1Password, which could be leveraged for social engineering attacks or targeted phishing campaigns.</li>
</ul>
<hr>
<h2 id="digging-dns">Digging DNS</h2>
<hr>
<p>Having established a solid understanding of DNS fundamentals and its various record types, let&#39;s now transition to the practical. This section will explore the tools and techniques for leveraging DNS for web reconnaissance.</p>
<h3 id="dns-tools">DNS Tools</h3>
<p>DNS reconnaissance involves utilizing specialized tools designed to query DNS servers and extract valuable information. Here are some of the most popular and versatile tools in the arsenal of web recon professionals:</p>
<table>
<thead>
<tr>
<th>Tool</th>
<th>Key Features</th>
<th>Use Cases</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>dig</code></td>
<td>Versatile DNS lookup tool that supports various query types (A, MX, NS, TXT, etc.) and detailed output.</td>
<td>Manual DNS queries, zone transfers (if allowed), troubleshooting DNS issues, and in-depth analysis of DNS records.</td>
</tr>
<tr>
<td><code>nslookup</code></td>
<td>Simpler DNS lookup tool, primarily for A, AAAA, and MX records.</td>
<td>Basic DNS queries, quick checks of domain resolution and mail server records.</td>
</tr>
<tr>
<td><code>host</code></td>
<td>Streamlined DNS lookup tool with concise output.</td>
<td>Quick checks of A, AAAA, and MX records.</td>
</tr>
<tr>
<td><code>dnsenum</code></td>
<td>Automated DNS enumeration tool, dictionary attacks, brute-forcing, zone transfers (if allowed).</td>
<td>Discovering subdomains and gathering DNS information efficiently.</td>
</tr>
<tr>
<td><code>fierce</code></td>
<td>DNS reconnaissance and subdomain enumeration tool with recursive search and wildcard detection.</td>
<td>User-friendly interface for DNS reconnaissance, identifying subdomains and potential targets.</td>
</tr>
<tr>
<td><code>dnsrecon</code></td>
<td>Combines multiple DNS reconnaissance techniques and supports various output formats.</td>
<td>Comprehensive DNS enumeration, identifying subdomains, and gathering DNS records for further analysis.</td>
</tr>
<tr>
<td><code>theHarvester</code></td>
<td>OSINT tool that gathers information from various sources, including DNS records (email addresses).</td>
<td>Collecting email addresses, employee information, and other data associated with a domain from multiple sources.</td>
</tr>
<tr>
<td><code>Online DNS Lookup Services</code></td>
<td>User-friendly interfaces for performing DNS lookups.</td>
<td>Quick and easy DNS lookups, convenient when command-line tools are not available, checking for domain availability or basic information</td>
</tr>
</tbody>
</table>
<h3 id="the-domain-information-groper">The Domain Information Groper</h3>
<p>The <code>dig</code> command (<code>Domain Information Groper</code>) is a versatile and powerful utility for querying DNS servers and retrieving various types of DNS records. Its flexibility and detailed and customizable output make it a go-to choice.</p>
<h4 id="common-dig-commands">Common dig Commands</h4>
<table>
<thead>
<tr>
<th>Command</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>dig domain.com</code></td>
<td>Performs a default A record lookup for the domain.</td>
</tr>
<tr>
<td><code>dig domain.com A</code></td>
<td>Retrieves the IPv4 address (A record) associated with the domain.</td>
</tr>
<tr>
<td><code>dig domain.com AAAA</code></td>
<td>Retrieves the IPv6 address (AAAA record) associated with the domain.</td>
</tr>
<tr>
<td><code>dig domain.com MX</code></td>
<td>Finds the mail servers (MX records) responsible for the domain.</td>
</tr>
<tr>
<td><code>dig domain.com NS</code></td>
<td>Identifies the authoritative name servers for the domain.</td>
</tr>
<tr>
<td><code>dig domain.com TXT</code></td>
<td>Retrieves any TXT records associated with the domain.</td>
</tr>
<tr>
<td><code>dig domain.com CNAME</code></td>
<td>Retrieves the canonical name (CNAME) record for the domain.</td>
</tr>
<tr>
<td><code>dig domain.com SOA</code></td>
<td>Retrieves the start of authority (SOA) record for the domain.</td>
</tr>
<tr>
<td><code>dig @1.1.1.1 domain.com</code></td>
<td>Specifies a specific name server to query; in this case 1.1.1.1</td>
</tr>
<tr>
<td><code>dig +trace domain.com</code></td>
<td>Shows the full path of DNS resolution.</td>
</tr>
<tr>
<td><code>dig -x 192.168.1.1</code></td>
<td>Performs a reverse lookup on the IP address 192.168.1.1 to find the associated host name. You may need to specify a name server.</td>
</tr>
<tr>
<td><code>dig +short domain.com</code></td>
<td>Provides a short, concise answer to the query.</td>
</tr>
<tr>
<td><code>dig +noall +answer domain.com</code></td>
<td>Displays only the answer section of the query output.</td>
</tr>
<tr>
<td><code>dig domain.com ANY</code></td>
<td>Retrieves all available DNS records for the domain (Note: Many DNS servers ignore <code>ANY</code> queries to reduce load and prevent abuse, as per <a href="https://datatracker.ietf.org/doc/html/rfc8482">RFC 8482</a>).</td>
</tr>
</tbody>
</table>
<p>Caution: Some servers can detect and block excessive DNS queries. Use caution and respect rate limits. Always obtain permission before performing extensive DNS reconnaissance on a target.</p>
<h3 id="groping-dns">Groping DNS</h3>
<p>&#x20; Digging DNS</p>
<pre><code class="lang-shell-session"><span class="hljs-selector-tag">root</span>@<span class="hljs-keyword">htb</span>[/<span class="hljs-keyword">htb</span>]$ dig google.com

; &lt;&lt;&gt;&gt; <span class="hljs-selector-tag">DiG</span> 9<span class="hljs-selector-class">.18</span><span class="hljs-selector-class">.24-0ubuntu0</span><span class="hljs-selector-class">.22</span><span class="hljs-selector-class">.04</span><span class="hljs-selector-class">.1-Ubuntu</span> &lt;&lt;&gt;&gt; <span class="hljs-selector-tag">google</span><span class="hljs-selector-class">.com</span>
;; <span class="hljs-selector-tag">global</span> <span class="hljs-selector-tag">options</span>: +<span class="hljs-selector-tag">cmd</span>
;; <span class="hljs-selector-tag">Got</span> <span class="hljs-selector-tag">answer</span>:
;; <span class="hljs-selector-tag">-</span>&gt;&gt;<span class="hljs-selector-tag">HEADER</span>&lt;&lt;<span class="hljs-selector-tag">-</span> <span class="hljs-selector-tag">opcode</span>: <span class="hljs-selector-tag">QUERY</span>, <span class="hljs-selector-tag">status</span>: <span class="hljs-selector-tag">NOERROR</span>, <span class="hljs-selector-tag">id</span>: 16449
;; <span class="hljs-selector-tag">flags</span>: <span class="hljs-selector-tag">qr</span> <span class="hljs-selector-tag">rd</span> <span class="hljs-selector-tag">ad</span>; <span class="hljs-selector-tag">QUERY</span>: 1, <span class="hljs-selector-tag">ANSWER</span>: 1, <span class="hljs-selector-tag">AUTHORITY</span>: 0, <span class="hljs-selector-tag">ADDITIONAL</span>: 0
;; <span class="hljs-selector-tag">WARNING</span>: <span class="hljs-selector-tag">recursion</span> <span class="hljs-selector-tag">requested</span> <span class="hljs-selector-tag">but</span> <span class="hljs-selector-tag">not</span> <span class="hljs-selector-tag">available</span>

;; <span class="hljs-selector-tag">QUESTION</span> <span class="hljs-selector-tag">SECTION</span>:
;<span class="hljs-selector-tag">google</span><span class="hljs-selector-class">.com</span>.                    <span class="hljs-selector-tag">IN</span>      <span class="hljs-selector-tag">A</span>

;; <span class="hljs-selector-tag">ANSWER</span> <span class="hljs-selector-tag">SECTION</span>:
<span class="hljs-selector-tag">google</span><span class="hljs-selector-class">.com</span>.             0       <span class="hljs-selector-tag">IN</span>      <span class="hljs-selector-tag">A</span>       142<span class="hljs-selector-class">.251</span><span class="hljs-selector-class">.47</span><span class="hljs-selector-class">.142</span>

;; <span class="hljs-selector-tag">Query</span> <span class="hljs-selector-tag">time</span>: 0 <span class="hljs-selector-tag">msec</span>
;; <span class="hljs-selector-tag">SERVER</span>: 172<span class="hljs-selector-class">.23</span><span class="hljs-selector-class">.176</span><span class="hljs-selector-class">.1</span><span class="hljs-selector-id">#53</span>(172<span class="hljs-selector-class">.23</span><span class="hljs-selector-class">.176</span><span class="hljs-selector-class">.1</span>) (<span class="hljs-selector-tag">UDP</span>)
;; <span class="hljs-selector-tag">WHEN</span>: <span class="hljs-selector-tag">Thu</span> <span class="hljs-selector-tag">Jun</span> 13 10<span class="hljs-selector-pseudo">:45</span><span class="hljs-selector-pseudo">:58</span> <span class="hljs-selector-tag">SAST</span> 2024
;; <span class="hljs-selector-tag">MSG</span> <span class="hljs-selector-tag">SIZE</span>  <span class="hljs-selector-tag">rcvd</span>: 54
</code></pre>
<p>This output is the result of a DNS query using the <code>dig</code> command for the domain <code>google.com</code>. The command was executed on a system running <code>DiG</code> version <code>9.18.24-0ubuntu0.22.04.1-Ubuntu</code>. The output can be broken down into four key sections:</p>
<ol>
<li>Header<ul>
<li><code>;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 16449</code>: This line indicates the type of query (<code>QUERY</code>), the successful status (<code>NOERROR</code>), and a unique identifier (<code>16449</code>) for this specific query.<ul>
<li><code>;; flags: qr rd ad; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0</code>: This describes the flags in the DNS header:<ul>
<li><code>qr</code>: Query Response flag - indicates this is a response.</li>
<li><code>rd</code>: Recursion Desired flag - means recursion was requested.</li>
<li><code>ad</code>: Authentic Data flag - means the resolver considers the data authentic.</li>
<li>The remaining numbers indicate the number of entries in each section of the DNS response: 1 question, 1 answer, 0 authority records, and 0 additional records.</li>
</ul>
</li>
</ul>
</li>
<li><code>;; WARNING: recursion requested but not available</code>: This indicates that recursion was requested, but the server does not support it.</li>
</ul>
</li>
<li>Question Section<ul>
<li><code>;google.com. IN A</code>: This line specifies the question: &quot;What is the IPv4 address (A record) for <code>google.com</code>?&quot;</li>
</ul>
</li>
<li>Answer Section<ul>
<li><code>google.com. 0 IN A 142.251.47.142</code>: This is the answer to the query. It indicates that the IP address associated with <code>google.com</code> is <code>142.251.47.142</code>. The &#39;<code>0</code>&#39; represents the <code>TTL</code> (time-to-live), indicating how long the result can be cached before being refreshed.</li>
</ul>
</li>
<li>Footer<ul>
<li><code>;; Query time: 0 msec</code>: This shows the time it took for the query to be processed and the response to be received (0 milliseconds).</li>
<li><code>;; SERVER: 172.23.176.1#53(172.23.176.1) (UDP)</code>: This identifies the DNS server that provided the answer and the protocol used (UDP).</li>
<li><code>;; WHEN: Thu Jun 13 10:45:58 SAST 2024</code>: This is the timestamp of when the query was made.</li>
<li><code>;; MSG SIZE rcvd: 54</code>: This indicates the size of the DNS message received (54 bytes).</li>
</ul>
</li>
</ol>
<p>An <code>opt pseudosection</code> can sometimes exist in a <code>dig</code> query. This is due to Extension Mechanisms for DNS (<code>EDNS</code>), which allows for additional features such as larger message sizes and DNS Security Extensions (<code>DNSSEC</code>) support.</p>
<p>If you just want the answer to the question, without any of the other information, you can query <code>dig</code> using <code>+short</code>:</p>
<p>&#x20; Digging DNS</p>
<pre><code class="lang-shell-session">root@htb[/htb]$ dig +short hackthebox.com

<span class="hljs-number">104.18</span><span class="hljs-number">.20</span><span class="hljs-number">.126</span>
<span class="hljs-number">104.18</span><span class="hljs-number">.21</span><span class="hljs-number">.126</span>
</code></pre>
<hr>
<h2 id="subdomains">Subdomains</h2>
<hr>
<p>When exploring DNS records, we&#39;ve primarily focused on the main domain (e.g., <code>example.com</code>) and its associated information. However, beneath the surface of this primary domain lies a potential network of subdomains. These subdomains are extensions of the main domain, often created to organise and separate different sections or functionalities of a website. For instance, a company might use <code>blog.example.com</code> for its blog, <code>shop.example.com</code> for its online store, or <code>mail.example.com</code> for its email services.</p>
<h3 id="why-is-this-important-for-web-reconnaissance-">Why is this important for web reconnaissance?</h3>
<p>Subdomains often host valuable information and resources that aren&#39;t directly linked from the main website. This can include:</p>
<ul>
<li><code>Development and Staging Environments</code>: Companies often use subdomains to test new features or updates before deploying them to the main site. Due to relaxed security measures, these environments sometimes contain vulnerabilities or expose sensitive information.</li>
<li><code>Hidden Login Portals</code>: Subdomains might host administrative panels or other login pages that are not meant to be publicly accessible. Attackers seeking unauthorised access can find these as attractive targets.</li>
<li><code>Legacy Applications</code>: Older, forgotten web applications might reside on subdomains, potentially containing outdated software with known vulnerabilities.</li>
<li><code>Sensitive Information</code>: Subdomains can inadvertently expose confidential documents, internal data, or configuration files that could be valuable to attackers.</li>
</ul>
<h3 id="subdomain-enumeration">Subdomain Enumeration</h3>
<p><code>Subdomain enumeration</code> is the process of systematically identifying and listing these subdomains. From a DNS perspective, subdomains are typically represented by <code>A</code> (or <code>AAAA</code> for IPv6) records, which map the subdomain name to its corresponding IP address. Additionally, <code>CNAME</code> records might be used to create aliases for subdomains, pointing them to other domains or subdomains. There are two main approaches to subdomain enumeration:</p>
<h4 id="1-active-subdomain-enumeration">1. Active Subdomain Enumeration</h4>
<p>This involves directly interacting with the target domain&#39;s DNS servers to uncover subdomains. One method is attempting a <code>DNS zone transfer</code>, where a misconfigured server might inadvertently leak a complete list of subdomains. However, due to tightened security measures, this is rarely successful.</p>
<p>A more common active technique is <code>brute-force enumeration</code>, which involves systematically testing a list of potential subdomain names against the target domain. Tools like <code>dnsenum</code>, <code>ffuf</code>, and <code>gobuster</code> can automate this process, using wordlists of common subdomain names or custom-generated lists based on specific patterns.</p>
<h4 id="2-passive-subdomain-enumeration">2. Passive Subdomain Enumeration</h4>
<p>This relies on external sources of information to discover subdomains without directly querying the target&#39;s DNS servers. One valuable resource is <code>Certificate Transparency (CT) logs</code>, public repositories of SSL/TLS certificates. These certificates often include a list of associated subdomains in their Subject Alternative Name (SAN) field, providing a treasure trove of potential targets.</p>
<p>Another passive approach involves utilising <code>search engines</code> like Google or DuckDuckGo. By employing specialised search operators (e.g., <code>site:</code>), you can filter results to show only subdomains related to the target domain.</p>
<p>Additionally, various online databases and tools aggregate DNS data from multiple sources, allowing you to search for subdomains without directly interacting with the target.</p>
<p>Each of these methods has its strengths and weaknesses. Active enumeration offers more control and potential for comprehensive discovery but can be more detectable. Passive enumeration is stealthier but might not uncover all existing subdomains. Combining both approaches provides a more thorough and effective subdomain enumeration strategy.</p>
<hr>
<h2 id="subdomain-bruteforcing">Subdomain Bruteforcing</h2>
<hr>
<p><code>Subdomain Brute-Force Enumeration</code> is a powerful active subdomain discovery technique that leverages pre-defined lists of potential subdomain names. This approach systematically tests these names against the target domain to identify valid subdomains. By using carefully crafted wordlists, you can significantly increase the efficiency and effectiveness of your subdomain discovery efforts.</p>
<p>The process breaks down into four steps:</p>
<ol>
<li><code>Wordlist Selection</code>: The process begins with selecting a wordlist containing potential subdomain names. These wordlists can be:<ul>
<li><code>General-Purpose</code>: Containing a broad range of common subdomain names (e.g., <code>dev</code>, <code>staging</code>, <code>blog</code>, <code>mail</code>, <code>admin</code>, <code>test</code>). This approach is useful when you don&#39;t know the target&#39;s naming conventions.</li>
<li><code>Targeted</code>: Focused on specific industries, technologies, or naming patterns relevant to the target. This approach is more efficient and reduces the chances of false positives.</li>
<li><code>Custom</code>: You can create your own wordlist based on specific keywords, patterns, or intelligence gathered from other sources.</li>
</ul>
</li>
<li><code>Iteration and Querying</code>: A script or tool iterates through the wordlist, appending each word or phrase to the main domain (e.g., <code>example.com</code>) to create potential subdomain names (e.g., <code>dev.example.com</code>, <code>staging.example.com</code>).</li>
<li><code>DNS Lookup</code>: A DNS query is performed for each potential subdomain to check if it resolves to an IP address. This is typically done using the A or AAAA record type.</li>
<li><code>Filtering and Validation</code>: If a subdomain resolves successfully, it&#39;s added to a list of valid subdomains. Further validation steps might be taken to confirm the subdomain&#39;s existence and functionality (e.g., by attempting to access it through a web browser).</li>
</ol>
<p>There are several tools available that excel at brute-force enumeration:</p>
<table>
<thead>
<tr>
<th>Tool</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/fwaeytens/dnsenum">dnsenum</a></td>
<td>Comprehensive DNS enumeration tool that supports dictionary and brute-force attacks for discovering subdomains.</td>
</tr>
<tr>
<td><a href="https://github.com/mschwager/fierce">fierce</a></td>
<td>User-friendly tool for recursive subdomain discovery, featuring wildcard detection and an easy-to-use interface.</td>
</tr>
<tr>
<td><a href="https://github.com/darkoperator/dnsrecon">dnsrecon</a></td>
<td>Versatile tool that combines multiple DNS reconnaissance techniques and offers customisable output formats.</td>
</tr>
<tr>
<td><a href="https://github.com/owasp-amass/amass">amass</a></td>
<td>Actively maintained tool focused on subdomain discovery, known for its integration with other tools and extensive data sources.</td>
</tr>
<tr>
<td><a href="https://github.com/tomnomnom/assetfinder">assetfinder</a></td>
<td>Simple yet effective tool for finding subdomains using various techniques, ideal for quick and lightweight scans.</td>
</tr>
<tr>
<td><a href="https://github.com/d3mondev/puredns">puredns</a></td>
<td>Powerful and flexible DNS brute-forcing tool, capable of resolving and filtering results effectively.</td>
</tr>
</tbody>
</table>
<h4 id="dnsenum">DNSEnum</h4>
<p><code>dnsenum</code> is a versatile and widely-used command-line tool written in Perl. It is a comprehensive toolkit for DNS reconnaissance, providing various functionalities to gather information about a target domain&#39;s DNS infrastructure and potential subdomains. The tool offers several key functions:</p>
<ul>
<li><code>DNS Record Enumeration</code>: <code>dnsenum</code> can retrieve various DNS records, including A, AAAA, NS, MX, and TXT records, providing a comprehensive overview of the target&#39;s DNS configuration.</li>
<li><code>Zone Transfer Attempts</code>: The tool automatically attempts zone transfers from discovered name servers. While most servers are configured to prevent unauthorised zone transfers, a successful attempt can reveal a treasure trove of DNS information.</li>
<li><code>Subdomain Brute-Forcing</code>: <code>dnsenum</code> supports brute-force enumeration of subdomains using a wordlist. This involves systematically testing potential subdomain names against the target domain to identify valid ones.</li>
<li><code>Google Scraping</code>: The tool can scrape Google search results to find additional subdomains that might not be listed in DNS records directly.</li>
<li><code>Reverse Lookup</code>: <code>dnsenum</code> can perform reverse DNS lookups to identify domains associated with a given IP address, potentially revealing other websites hosted on the same server.</li>
<li><code>WHOIS Lookups</code>: The tool can also perform WHOIS queries to gather information about domain ownership and registration details.</li>
</ul>
<p>Let&#39;s see <code>dnsenum</code> in action by demonstrating how to enumerate subdomains for our target, <code>inlanefreight.com</code>. In this demonstration, we&#39;ll use the <code>subdomains-top1million-5000.txt</code> wordlist from <a href="https://github.com/danielmiessler/SecLists">SecLists</a>, which contains the top 5000 most common subdomains.</p>
<p>Code: bash</p>
<pre><code class="lang-bash">dnsenum --<span class="hljs-class"><span class="hljs-keyword">enum</span> <span class="hljs-title">inlanefreight</span>.<span class="hljs-title">com</span> -<span class="hljs-title">f</span> /<span class="hljs-title">usr</span>/<span class="hljs-title">share</span>/<span class="hljs-title">seclists</span>/<span class="hljs-title">Discovery</span>/<span class="hljs-title">DNS</span>/<span class="hljs-title">subdomains</span>-<span class="hljs-title">top1million</span>-110000.<span class="hljs-title">txt</span> -<span class="hljs-title">r</span></span>
</code></pre>
<p>In this command:</p>
<ul>
<li><code>dnsenum --enum inlanefreight.com</code>: We specify the target domain we want to enumerate, along with a shortcut for some tuning options <code>--enum</code>.</li>
<li><code>-f /usr/share/seclists/Discovery/DNS/subdomains-top1million-5000.txt</code>: We indicate the path to the SecLists wordlist we&#39;ll use for brute-forcing. Adjust the path if your SecLists installation is different.</li>
<li><code>-r</code>: This option enables recursive subdomain brute-forcing, meaning that if <code>dnsenum</code> finds a subdomain, it will then try to enumerate subdomains of that subdomain.</li>
</ul>
<p>&#x20; Subdomain Bruteforcing</p>
<pre><code class="lang-shell-session">root@htb[/htb]$ dnsenum --enum inlanefreight.com -f  /usr/share/seclists/Discovery/DNS/subdomains-top1million-20000.txt 

dnsenum VERSION:1.2.6

-----   inlanefreight.com   -----


Host's addresses:
<span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-emphasis">___</span>

inlanefreight.com.                       300      IN    A        134.209.24.248

[...]

Brute forcing with /usr/share/seclists/Discovery/DNS/subdomains-top1million-20000.txt:
<span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span>__

www.inlanefreight.com.                   300      IN    A        134.209.24.248
support.inlanefreight.com.               300      IN    A        134.209.24.248
[...]


done.
</code></pre>
<hr>
<h2 id="dns-zone-transfers">DNS Zone Transfers</h2>
<hr>
<p>While brute-forcing can be a fruitful approach, there&#39;s a less invasive and potentially more efficient method for uncovering subdomains â€“ DNS zone transfers. This mechanism, designed for replicating DNS records between name servers, can inadvertently become a goldmine of information for prying eyes if misconfigured.</p>
<h3 id="what-is-a-zone-transfer">What is a Zone Transfer</h3>
<p>A DNS zone transfer is essentially a wholesale copy of all DNS records within a zone (a domain and its subdomains) from one name server to another. This process is essential for maintaining consistency and redundancy across DNS servers. However, if not adequately secured, unauthorised parties can download the entire zone file, revealing a complete list of subdomains, their associated IP addresses, and other sensitive DNS data.</p>
<p><img src="https://mermaid.ink/svg/pako:eNqNkc9qwzAMxl9F-JSx7gV8KISWXcY2aHYYwxdjK39obGWKvBFK333ukg5aGNQnW9b3Q_q-g3LkUWk14mfC6HDb2YZtMBHyGdFR9JanCvkL-WG9vh-4C38FDeX74w52J-0oUHxQRHhjG8ca-W5mXAgy4YqpoXotM8EReygqsSxANZRJWuJOpoXSEw0gC3ku3QTfvlQLfBZh9DeOdbELbCgMPQr-58u1LZsnKEq3j_Tdo28wYJS8iVqpgBxs57PjhxPLKGnzr1E6XzNxb5SJx9xnk1A1Rae0cMKVYkpNq3Rt-zG_0uCtnLM6t6DvhPh5zvM31uMPG8qm-A" alt="Diagram showing data transfer between secondary and primary servers. Includes steps: XML Request, XML Record, loop for retries, XML Report, and AOK (Acknowledgment)."></p>
<ol>
<li><code>Zone Transfer Request (AXFR)</code>: The secondary DNS server initiates the process by sending a zone transfer request to the primary server. This request typically uses the AXFR (Full Zone Transfer) type.</li>
<li><code>SOA Record Transfer</code>: Upon receiving the request (and potentially authenticating the secondary server), the primary server responds by sending its Start of Authority (SOA) record. The SOA record contains vital information about the zone, including its serial number, which helps the secondary server determine if its zone data is current.</li>
<li><code>DNS Records Transmission</code>: The primary server then transfers all the DNS records in the zone to the secondary server, one by one. This includes records like A, AAAA, MX, CNAME, NS, and others that define the domain&#39;s subdomains, mail servers, name servers, and other configurations.</li>
<li><code>Zone Transfer Complete</code>: Once all records have been transmitted, the primary server signals the end of the zone transfer. This notification informs the secondary server that it has received a complete copy of the zone data.</li>
<li><code>Acknowledgement (ACK)</code>: The secondary server sends an acknowledgement message to the primary server, confirming the successful receipt and processing of the zone data. This completes the zone transfer process.</li>
</ol>
<h3 id="the-zone-transfer-vulnerability">The Zone Transfer Vulnerability</h3>
<p>While zone transfers are essential for legitimate DNS management, a misconfigured DNS server can transform this process into a significant security vulnerability. The core issue lies in the access controls governing who can initiate a zone transfer.</p>
<p>In the early days of the internet, allowing any client to request a zone transfer from a DNS server was common practice. This open approach simplified administration but opened a gaping security hole. It meant that anyone, including malicious actors, could ask a DNS server for a complete copy of its zone file, which contains a wealth of sensitive information.</p>
<p>The information gleaned from an unauthorised zone transfer can be invaluable to an attacker. It reveals a comprehensive map of the target&#39;s DNS infrastructure, including:</p>
<ul>
<li><code>Subdomains</code>: A complete list of subdomains, many of which might not be linked from the main website or easily discoverable through other means. These hidden subdomains could host development servers, staging environments, administrative panels, or other sensitive resources.</li>
<li><code>IP Addresses</code>: The IP addresses associated with each subdomain, providing potential targets for further reconnaissance or attacks.</li>
<li><code>Name Server Records</code>: Details about the authoritative name servers for the domain, revealing the hosting provider and potential misconfigurations.</li>
</ul>
<h4 id="remediation">Remediation</h4>
<p>Fortunately, awareness of this vulnerability has grown, and most DNS server administrators have mitigated the risk. Modern DNS servers are typically configured to allow zone transfers only to trusted secondary servers, ensuring that sensitive zone data remains confidential.</p>
<p>However, misconfigurations can still occur due to human error or outdated practices. This is why attempting a zone transfer (with proper authorisation) remains a valuable reconnaissance technique. Even if unsuccessful, the attempt can reveal information about the DNS server&#39;s configuration and security posture.</p>
<p><strong>Exploiting Zone Transfers</strong></p>
<p>You can use the <code>dig</code> command to request a zone transfer:</p>
<p>&#x20; DNS Zone Transfers</p>
<pre><code class="lang-shell-session">root@htb[/htb]$ dig axfr @nsztm1<span class="hljs-selector-class">.digi</span><span class="hljs-selector-class">.ninja</span> zonetransfer.me
</code></pre>
<p>This command instructs <code>dig</code> to request a full zone transfer (<code>axfr</code>) from the DNS server responsible for <code>zonetransfer.me</code>. If the server is misconfigured and allows the transfer, you&#39;ll receive a complete list of DNS records for the domain, including all subdomains.</p>
<p>&#x20; DNS Zone Transfers</p>
<pre><code class="lang-shell-session">root@htb[/htb]$ dig axfr @nsztm1.digi.ninja zonetransfer.<span class="hljs-keyword">me</span>

; &lt;&lt;&gt;&gt; DiG <span class="hljs-number">9.18</span><span class="hljs-number">.12</span><span class="hljs-number">-1</span>~bpo11+<span class="hljs-number">1</span>-Debian &lt;&lt;&gt;&gt; axfr @nsztm1.digi.ninja zonetransfer.<span class="hljs-keyword">me</span>
; (<span class="hljs-number">1</span> server found)
;; <span class="hljs-keyword">global</span> options: +cmd
zonetransfer.<span class="hljs-keyword">me</span>.    <span class="hljs-number">7200</span>    <span class="hljs-keyword">IN</span>    SOA    nsztm1.digi.ninja. robin.digi.ninja. <span class="hljs-number">2019100801</span> <span class="hljs-number">172800</span> <span class="hljs-number">900</span> <span class="hljs-number">1209600</span> <span class="hljs-number">3600</span>
zonetransfer.<span class="hljs-keyword">me</span>.    <span class="hljs-number">300</span>    <span class="hljs-keyword">IN</span>    HINFO    <span class="hljs-string">"Casio fx-700G"</span> <span class="hljs-string">"Windows XP"</span>
zonetransfer.<span class="hljs-keyword">me</span>.    <span class="hljs-number">301</span>    <span class="hljs-keyword">IN</span>    TXT    <span class="hljs-string">"google-site-verification=tyP28J7JAUHA9fw2sHXMgcCC0I6XBmmoVi04VlMewxA"</span>
zonetransfer.<span class="hljs-keyword">me</span>.    <span class="hljs-number">7200</span>    <span class="hljs-keyword">IN</span>    MX    <span class="hljs-number">0</span> ASPMX.L.GOOGLE.COM.
...
zonetransfer.<span class="hljs-keyword">me</span>.    <span class="hljs-number">7200</span>    <span class="hljs-keyword">IN</span>    A    <span class="hljs-number">5.196</span><span class="hljs-number">.105</span><span class="hljs-number">.14</span>
zonetransfer.<span class="hljs-keyword">me</span>.    <span class="hljs-number">7200</span>    <span class="hljs-keyword">IN</span>    NS    nsztm1.digi.ninja.
zonetransfer.<span class="hljs-keyword">me</span>.    <span class="hljs-number">7200</span>    <span class="hljs-keyword">IN</span>    NS    nsztm2.digi.ninja.
_acme-challenge.zonetransfer.<span class="hljs-keyword">me</span>. <span class="hljs-number">301</span> <span class="hljs-keyword">IN</span>    TXT    <span class="hljs-string">"6Oa05hbUJ9xSsvYy7pApQvwCUSSGgxvrbdizjePEsZI"</span>
_sip._tcp.zonetransfer.<span class="hljs-keyword">me</span>. <span class="hljs-number">14000</span> <span class="hljs-keyword">IN</span>    SRV    <span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">5060</span> www.zonetransfer.<span class="hljs-keyword">me</span>.
<span class="hljs-number">14.105</span><span class="hljs-number">.196</span><span class="hljs-number">.5</span>.<span class="hljs-keyword">IN</span>-ADDR.ARPA.zonetransfer.<span class="hljs-keyword">me</span>. <span class="hljs-number">7200</span>    <span class="hljs-keyword">IN</span> PTR www.zonetransfer.<span class="hljs-keyword">me</span>.
asfdbauthdns.zonetransfer.<span class="hljs-keyword">me</span>. <span class="hljs-number">7900</span> <span class="hljs-keyword">IN</span>    AFSDB    <span class="hljs-number">1</span> asfdbbox.zonetransfer.<span class="hljs-keyword">me</span>.
asfdbbox.zonetransfer.<span class="hljs-keyword">me</span>. <span class="hljs-number">7200</span>    <span class="hljs-keyword">IN</span>    A    <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>
asfdbvolume.zonetransfer.<span class="hljs-keyword">me</span>. <span class="hljs-number">7800</span> <span class="hljs-keyword">IN</span>    AFSDB    <span class="hljs-number">1</span> asfdbbox.zonetransfer.<span class="hljs-keyword">me</span>.
canberra-office.zonetransfer.<span class="hljs-keyword">me</span>. <span class="hljs-number">7200</span> <span class="hljs-keyword">IN</span> A    <span class="hljs-number">202.14</span><span class="hljs-number">.81</span><span class="hljs-number">.230</span>
...
;; Query time: <span class="hljs-number">10</span> msec
;; SERVER: <span class="hljs-number">81.4</span><span class="hljs-number">.108</span><span class="hljs-number">.41</span><span class="hljs-meta">#53(nsztm1.digi.ninja) (TCP)</span>
;; <span class="hljs-keyword">WHEN</span>: Mon May <span class="hljs-number">27</span> <span class="hljs-number">18</span>:<span class="hljs-number">31</span>:<span class="hljs-number">35</span> BST <span class="hljs-number">2024</span>
;; XFR size: <span class="hljs-number">50</span> records (messages <span class="hljs-number">1</span>, bytes <span class="hljs-number">2085</span>)
</code></pre>
<p><code>zonetransfer.me</code> is a service specifically setup to demonstrate the risks of zone transfers so that the <code>dig</code> command will return the full zone record.</p>
<hr>
<h2 id="virtual-hosts">Virtual Hosts</h2>
<hr>
<p>Once the DNS directs traffic to the correct server, the web server configuration becomes crucial in determining how the incoming requests are handled. Web servers like Apache, Nginx, or IIS are designed to host multiple websites or applications on a single server. They achieve this through virtual hosting, which allows them to differentiate between domains, subdomains, or even separate websites with distinct content.</p>
<h3 id="how-virtual-hosts-work-understanding-vhosts-and-subdomains">How Virtual Hosts Work: Understanding VHosts and Subdomains</h3>
<p>At the core of <code>virtual hosting</code> is the ability of web servers to distinguish between multiple websites or applications sharing the same IP address. This is achieved by leveraging the <code>HTTP Host</code> header, a piece of information included in every <code>HTTP</code> request sent by a web browser.</p>
<p>The key difference between <code>VHosts</code> and <code>subdomains</code> is their relationship to the <code>Domain Name System (DNS)</code> and the web server&#39;s configuration.</p>
<ul>
<li><code>Subdomains</code>: These are extensions of a main domain name (e.g., <code>blog.example.com</code> is a subdomain of <code>example.com</code>). <code>Subdomains</code> typically have their own <code>DNS records</code>, pointing to either the same IP address as the main domain or a different one. They can be used to organise different sections or services of a website.</li>
<li><code>Virtual Hosts</code> (<code>VHosts</code>): Virtual hosts are configurations within a web server that allow multiple websites or applications to be hosted on a single server. They can be associated with top-level domains (e.g., <code>example.com</code>) or subdomains (e.g., <code>dev.example.com</code>). Each virtual host can have its own separate configuration, enabling precise control over how requests are handled.</li>
</ul>
<p>If a virtual host does not have a DNS record, you can still access it by modifying the <code>hosts</code> file on your local machine. The <code>hosts</code> file allows you to map a domain name to an IP address manually, bypassing DNS resolution.</p>
<p>Websites often have subdomains that are not public and won&#39;t appear in DNS records. These <code>subdomains</code> are only accessible internally or through specific configurations. <code>VHost fuzzing</code> is a technique to discover public and non-public <code>subdomains</code> and <code>VHosts</code> by testing various hostnames against a known IP address.</p>
<p>Virtual hosts can also be configured to use different domains, not just subdomains. For example:</p>
<p>Code: apacheconf</p>
<pre><code class="lang-apacheconf"><span class="hljs-comment"># Example of name-based virtual host configuration in Apache</span>
<span class="hljs-section">&lt;VirtualHost *:80&gt;</span>
    <span class="hljs-attribute"><span class="hljs-nomarkup">ServerName</span></span> www.example1.com
    <span class="hljs-attribute"><span class="hljs-nomarkup">DocumentRoot</span></span> /var/www/example1
<span class="hljs-section">&lt;/VirtualHost&gt;</span>

<span class="hljs-section">&lt;VirtualHost *:80&gt;</span>
    <span class="hljs-attribute"><span class="hljs-nomarkup">ServerName</span></span> www.example2.org
    <span class="hljs-attribute"><span class="hljs-nomarkup">DocumentRoot</span></span> /var/www/example2
<span class="hljs-section">&lt;/VirtualHost&gt;</span>

<span class="hljs-section">&lt;VirtualHost *:80&gt;</span>
    <span class="hljs-attribute"><span class="hljs-nomarkup">ServerName</span></span> www.another-example.net
    <span class="hljs-attribute"><span class="hljs-nomarkup">DocumentRoot</span></span> /var/www/another-example
<span class="hljs-section">&lt;/VirtualHost&gt;</span>
</code></pre>
<p>Here, <code>example1.com</code>, <code>example2.org</code>, and <code>another-example.net</code> are distinct domains hosted on the same server. The web server uses the <code>Host</code> header to serve the appropriate content based on the requested domain name.</p>
<h4 id="server-vhost-lookup">Server VHost Lookup</h4>
<p>The following illustrates the process of how a web server determines the correct content to serve based on the <code>Host</code> header:</p>
<p><img src="https://mermaid.ink/svg/pako:eNqNUsFuwjAM_ZUop00CPqAHDhubuCBNBW2XXrzUtNFap3McOoT496WUVUA3aTkltp_f84sP2rgcdaI9fgYkgwsLBUOdkYqnARZrbAMk6oFd65HHiTd8XyPvfku9WpYA1dJ5eXS0tcW4ZOFMqJEkdU4y6vNnqul8PvRO1HKzeVFpp9KLumvbdmapAsItoy1KmRlX3_fwAXTd4OkLakuoOjVqiZAj_7_PaJJEPVvK1QrElJYK1UcDg1h3HmOEmV4LSlEC0-CA6i24Zb406IRhizuM7BV6BVFCit4FNuh77GX9DeGfmEu-s_mD4b5x5PH2Y4aqhfVNBftufomsGemJrpFrsHncqkOHy7SUWGOmk3jNgT8yndEx1kEQt96T0YlwwIlmF4pSJ1uofHyFJgf52cchirkVx6t-aU-7e_wG--_4bQ" alt="Sequence diagram showing interactions between Browser, WebServer, VirtualHostConfig, and DocumentRoot. Includes HTTP request, server response, and file access steps."></p>
<ol>
<li><code>Browser Requests a Website</code>: When you enter a domain name (e.g., <code>www.inlanefreight.com</code>) into your browser, it initiates an HTTP request to the web server associated with that domain&#39;s IP address.</li>
<li><code>Host Header Reveals the Domain</code>: The browser includes the domain name in the request&#39;s <code>Host</code> header, which acts as a label to inform the web server which website is being requested.</li>
<li><code>Web Server Determines the Virtual Host</code>: The web server receives the request, examines the <code>Host</code> header, and consults its virtual host configuration to find a matching entry for the requested domain name.</li>
<li><code>Serving the Right Content</code>: Upon identifying the correct virtual host configuration, the web server retrieves the corresponding files and resources associated with that website from its document root and sends them back to the browser as the HTTP response.</li>
</ol>
<p>In essence, the <code>Host</code> header functions as a switch, enabling the web server to dynamically determine which website to serve based on the domain name requested by the browser.</p>
<h4 id="types-of-virtual-hosting">Types of Virtual Hosting</h4>
<p>There are three primary types of virtual hosting, each with its advantages and drawbacks:</p>
<ol>
<li><code>Name-Based Virtual Hosting</code>: This method relies solely on the <code>HTTP Host header</code> to distinguish between websites. It is the most common and flexible method, as it doesn&#39;t require multiple IP addresses. Itâ€™s cost-effective, easy to set up, and supports most modern web servers. However, it requires the web server to support name-based <code>virtual hosting</code> and can have limitations with certain protocols like <code>SSL/TLS</code>.</li>
<li><code>IP-Based Virtual Hosting</code>: This type of hosting assigns a unique IP address to each website hosted on the server. The server determines which website to serve based on the IP address to which the request was sent. It doesn&#39;t rely on the <code>Host header</code>, can be used with any protocol, and offers better isolation between websites. Still, it requires multiple IP addresses, which can be expensive and less scalable.</li>
<li><code>Port-Based Virtual Hosting</code>: Different websites are associated with different ports on the same IP address. For example, one website might be accessible on port 80, while another is on port 8080. <code>Port-based virtual hosting</code> can be used when IP addresses are limited, but itâ€™s not as common or user-friendly as <code>name-based virtual hosting</code> and might require users to specify the port number in the URL.</li>
</ol>
<h3 id="virtual-host-discovery-tools">Virtual Host Discovery Tools</h3>
<p>While manual analysis of <code>HTTP headers</code> and reverse <code>DNS lookups</code> can be effective, specialised <code>virtual host discovery tools</code> automate and streamline the process, making it more efficient and comprehensive. These tools employ various techniques to probe the target server and uncover potential <code>virtual hosts</code>.</p>
<p>Several tools are available to aid in the discovery of virtual hosts:</p>
<table>
<thead>
<tr>
<th>Tool</th>
<th>Description</th>
<th>Features</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/OJ/gobuster">gobuster</a></td>
<td>A multi-purpose tool often used for directory/file brute-forcing, but also effective for virtual host discovery.</td>
<td>Fast, supports multiple HTTP methods, can use custom wordlists.</td>
</tr>
<tr>
<td><a href="https://github.com/epi052/feroxbuster">Feroxbuster</a></td>
<td>Similar to Gobuster, but with a Rust-based implementation, known for its speed and flexibility.</td>
<td>Supports recursion, wildcard discovery, and various filters.</td>
</tr>
<tr>
<td><a href="https://github.com/ffuf/ffuf">ffuf</a></td>
<td>Another fast web fuzzer that can be used for virtual host discovery by fuzzing the <code>Host</code> header.</td>
<td>Customizable wordlist input and filtering options.</td>
</tr>
</tbody>
</table>
<h4 id="gobuster">gobuster</h4>
<p>Gobuster is a versatile tool commonly used for directory and file brute-forcing, but it also excels at virtual host discovery. It systematically sends HTTP requests with different <code>Host</code> headers to a target IP address and then analyses the responses to identify valid virtual hosts.</p>
<p>There are a couple of things you need to prepare to brute force <code>Host</code> headers:</p>
<ol>
<li><code>Target Identification</code>: First, identify the target web server&#39;s IP address. This can be done through DNS lookups or other reconnaissance techniques.</li>
<li><code>Wordlist Preparation</code>: Prepare a wordlist containing potential virtual host names. You can use a pre-compiled wordlist, such as SecLists, or create a custom one based on your target&#39;s industry, naming conventions, or other relevant information.</li>
</ol>
<p>The <code>gobuster</code> command to bruteforce vhosts generally looks like this:</p>
<p>&#x20; Virtual Hosts</p>
<pre><code class="lang-shell-session">root<span class="hljs-meta">@htb[/htb]$</span> gobuster vhost -u http://<span class="hljs-variable">&lt;target_IP_address&gt;</span> -w <span class="hljs-variable">&lt;wordlist_file&gt;</span> --append-domain
</code></pre>
<ul>
<li>The <code>-u</code> flag specifies the target URL (replace <code>&lt;target_IP_address&gt;</code> with the actual IP).</li>
<li>The <code>-w</code> flag specifies the wordlist file (replace <code>&lt;wordlist_file&gt;</code> with the path to your wordlist).</li>
<li>The <code>--append-domain</code> flag appends the base domain to each word in the wordlist.</li>
</ul>
<p>In newer versions of Gobuster, the --append-domain flag is required to append the base domain to each word in the wordlist when performing virtual host discovery. This flag ensures that Gobuster correctly constructs the full virtual hostnames, which is essential for the accurate enumeration of potential subdomains. In older versions of Gobuster, this functionality was handled differently, and the --append-domain flag was not necessary. Users of older versions might not find this flag available or needed, as the tool appended the base domain by default or employed a different mechanism for virtual host generation.</p>
<p><code>Gobuster</code> will output potential virtual hosts as it discovers them. Analyse the results carefully, noting any unusual or interesting findings. Further investigation might be needed to confirm the existence and functionality of the discovered virtual hosts.</p>
<p>There are a couple of other arguments that are worth knowing:</p>
<ul>
<li>Consider using the <code>-t</code> flag to increase the number of threads for faster scanning.</li>
<li>The <code>-k</code> flag can ignore SSL/TLS certificate errors.</li>
<li>You can use the <code>-o</code> flag to save the output to a file for later analysis.</li>
</ul>
<p>&#x20; Virtual Hosts</p>
<pre><code class="lang-shell-session">root@htb[/htb]$ gobuster vhost -u http://inlanefreight.htb:81 -w /usr/share/seclists/Discovery/DNS/subdomains-top1million-110000.txt --append-domain
===============================================================
Gobuster v3.6
by OJ Reeves (@TheColonial) &amp; Christian Mehlmauer (@firefart)
===============================================================
[+] Url:             http://inlanefreight.htb:81
[+] Method:          GET
[+] Threads:         10
[+] Wordlist:        /usr/share/seclists/Discovery/DNS/subdomains-top1million-110000.txt
[+] User Agent:      gobuster/3.6
[+] Timeout:         10s
[+] Append Domain:   true
===============================================================
Starting gobuster in VHOST enumeration mode
===============================================================
Found: forum.inlanefreight.htb:81 Status: 200 [Size: 100]
<span class="hljs-meta">[...]</span>
<span class="hljs-section">Progress: 114441 / 114442 (100.00%)
===============================================================</span>
<span class="hljs-section">Finished
===============================================================</span>
</code></pre>
<hr>
<h2 id="certificate-transparency-logs">Certificate Transparency Logs</h2>
<hr>
<p>In the sprawling mass of the internet, trust is a fragile commodity. One of the cornerstones of this trust is the <code>Secure Sockets Layer/Transport Layer Security</code> (<code>SSL/TLS</code>) protocol, which encrypts communication between your browser and a website. At the heart of SSL/TLS lies the <code>digital certificate</code>, a small file that verifies a website&#39;s identity and allows for secure, encrypted communication.</p>
<p>However, the process of issuing and managing these certificates isn&#39;t foolproof. Attackers can exploit rogue or mis-issued certificates to impersonate legitimate websites, intercept sensitive data, or spread malware. This is where Certificate Transparency (CT) logs come into play.</p>
<h3 id="what-are-certificate-transparency-logs-">What are Certificate Transparency Logs?</h3>
<p><code>Certificate Transparency</code> (<code>CT</code>) logs are public, append-only ledgers that record the issuance of SSL/TLS certificates. Whenever a Certificate Authority (CA) issues a new certificate, it must submit it to multiple CT logs. Independent organisations maintain these logs and are open for anyone to inspect.</p>
<p>Think of CT logs as a <code>global registry of certificates</code>. They provide a transparent and verifiable record of every SSL/TLS certificate issued for a website. This transparency serves several crucial purposes:</p>
<ul>
<li><code>Early Detection of Rogue Certificates</code>: By monitoring CT logs, security researchers and website owners can quickly identify suspicious or misissued certificates. A rogue certificate is an unauthorized or fraudulent digital certificate issued by a trusted certificate authority. Detecting these early allows for swift action to revoke the certificates before they can be used for malicious purposes.</li>
<li><code>Accountability for Certificate Authorities</code>: CT logs hold CAs accountable for their issuance practices. If a CA issues a certificate that violates the rules or standards, it will be publicly visible in the logs, leading to potential sanctions or loss of trust.</li>
<li><code>Strengthening the Web PKI (Public Key Infrastructure)</code>: The Web PKI is the trust system underpinning secure online communication. CT logs help to enhance the security and integrity of the Web PKI by providing a mechanism for public oversight and verification of certificates.</li>
</ul>
<details>

<summary>Click to expand a technical breakdown of how CT Logs Work</summary>

###

1.
2.
3.
4.
5.

####

<img src="https://academy.hackthebox.com/storage/modules/144/diagram-001.png" alt="">

<em>
</em>
*

1.
2.
3.

</details>

<p>\</p>
<h3 id="ct-logs-and-web-recon">CT Logs and Web Recon</h3>
<p>Certificate Transparency logs offer a unique advantage in subdomain enumeration compared to other methods. Unlike brute-forcing or wordlist-based approaches, which rely on guessing or predicting subdomain names, CT logs provide a definitive record of certificates issued for a domain and its subdomains. This means you&#39;re not limited by the scope of your wordlist or the effectiveness of your brute-forcing algorithm. Instead, you gain access to a historical and comprehensive view of a domain&#39;s subdomains, including those that might not be actively used or easily guessable.</p>
<p>Furthermore, CT logs can unveil subdomains associated with old or expired certificates. These subdomains might host outdated software or configurations, making them potentially vulnerable to exploitation.</p>
<p>In essence, CT logs provide a reliable and efficient way to discover subdomains without the need for exhaustive brute-forcing or relying on the completeness of wordlists. They offer a unique window into a domain&#39;s history and can reveal subdomains that might otherwise remain hidden, significantly enhancing your reconnaissance capabilities.</p>
<h3 id="searching-ct-logs">Searching CT Logs</h3>
<p>There are two popular options for searching CT logs:</p>
<table>
<thead>
<tr>
<th>Tool</th>
<th>Key Features</th>
<th>Use Cases</th>
<th>Pros</th>
<th>Cons</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://crt.sh/">crt.sh</a></td>
<td>User-friendly web interface, simple search by domain, displays certificate details, SAN entries.</td>
<td>Quick and easy searches, identifying subdomains, checking certificate issuance history.</td>
<td>Free, easy to use, no registration required.</td>
<td>Limited filtering and analysis options.</td>
</tr>
<tr>
<td><a href="https://search.censys.io/">Censys</a></td>
<td>Powerful search engine for internet-connected devices, advanced filtering by domain, IP, certificate attributes.</td>
<td>In-depth analysis of certificates, identifying misconfigurations, finding related certificates and hosts.</td>
<td>Extensive data and filtering options, API access.</td>
<td>Requires registration (free tier available).</td>
</tr>
</tbody>
</table>
<h4 id="crt-sh-lookup">crt.sh lookup</h4>
<p>While <code>crt.sh</code> offers a convenient web interface, you can also leverage its API for automated searches directly from your terminal. Let&#39;s see how to find all &#39;dev&#39; subdomains on <code>facebook.com</code> using <code>curl</code> and <code>jq</code>:</p>
<p>&#x20; Certificate Transparency Logs</p>
<pre><code class="lang-shell-session">root@htb[/htb]$ curl -s <span class="hljs-string">"https://crt.sh/?q=facebook.com&amp;output=json"</span> | jq -r '.[]
 | <span class="hljs-keyword">select</span>(.name_value | contains(<span class="hljs-string">"dev"</span>)) | .name_value' | sort -u

*.<span class="hljs-built_in">dev</span>.facebook.com
*.newdev.facebook.com
*.secure.<span class="hljs-built_in">dev</span>.facebook.com
<span class="hljs-built_in">dev</span>.facebook.com
devvm1958.ftw3.facebook.com
facebook-amex-<span class="hljs-built_in">dev</span>.facebook.com
facebook-amex-sign-enc-<span class="hljs-built_in">dev</span>.facebook.com
newdev.facebook.com
secure.<span class="hljs-built_in">dev</span>.facebook.com
</code></pre>
<ul>
<li><code>curl -s &quot;https://crt.sh/?q=facebook.com&amp;output=json&quot;</code>: This command fetches the JSON output from crt.sh for certificates matching the domain <code>facebook.com</code>.</li>
<li><code>jq -r &#39;.[] | select(.name_value | contains(&quot;dev&quot;)) | .name_value&#39;</code>: This part filters the JSON results, selecting only entries where the <code>name_value</code> field (which contains the domain or subdomain) includes the string &quot;<code>dev</code>&quot;. The <code>-r</code> flag tells <code>jq</code> to output raw strings.</li>
<li><code>sort -u</code>: This sorts the results alphabetically and removes duplicates.</li>
</ul>
<hr>
<h2 id="fingerprinting">Fingerprinting</h2>
<hr>
<p>Fingerprinting focuses on extracting technical details about the technologies powering a website or web application. Similar to how a fingerprint uniquely identifies a person, the digital signatures of web servers, operating systems, and software components can reveal critical information about a target&#39;s infrastructure and potential security weaknesses. This knowledge empowers attackers to tailor attacks and exploit vulnerabilities specific to the identified technologies.</p>
<p>Fingerprinting serves as a cornerstone of web reconnaissance for several reasons:</p>
<ul>
<li><code>Targeted Attacks</code>: By knowing the specific technologies in use, attackers can focus their efforts on exploits and vulnerabilities that are known to affect those systems. This significantly increases the chances of a successful compromise.</li>
<li><code>Identifying Misconfigurations</code>: Fingerprinting can expose misconfigured or outdated software, default settings, or other weaknesses that might not be apparent through other reconnaissance methods.</li>
<li><code>Prioritising Targets</code>: When faced with multiple potential targets, fingerprinting helps prioritise efforts by identifying systems more likely to be vulnerable or hold valuable information.</li>
<li><code>Building a Comprehensive Profile</code>: Combining fingerprint data with other reconnaissance findings creates a holistic view of the target&#39;s infrastructure, aiding in understanding its overall security posture and potential attack vectors.</li>
</ul>
<h3 id="fingerprinting-techniques">Fingerprinting Techniques</h3>
<p>There are several techniques used for web server and technology fingerprinting:</p>
<ul>
<li><code>Banner Grabbing</code>: Banner grabbing involves analysing the banners presented by web servers and other services. These banners often reveal the server software, version numbers, and other details.</li>
<li><code>Analysing HTTP Headers</code>: HTTP headers transmitted with every web page request and response contain a wealth of information. The <code>Server</code> header typically discloses the web server software, while the <code>X-Powered-By</code> header might reveal additional technologies like scripting languages or frameworks.</li>
<li><code>Probing for Specific Responses</code>: Sending specially crafted requests to the target can elicit unique responses that reveal specific technologies or versions. For example, certain error messages or behaviours are characteristic of particular web servers or software components.</li>
<li><code>Analysing Page Content</code>: A web page&#39;s content, including its structure, scripts, and other elements, can often provide clues about the underlying technologies. There may be a copyright header that indicates specific software being used, for example.</li>
</ul>
<p>A variety of tools exist that automate the fingerprinting process, combining various techniques to identify web servers, operating systems, content management systems, and other technologies:</p>
<table>
<thead>
<tr>
<th>Tool</th>
<th>Description</th>
<th>Features</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Wappalyzer</code></td>
<td>Browser extension and online service for website technology profiling.</td>
<td>Identifies a wide range of web technologies, including CMSs, frameworks, analytics tools, and more.</td>
</tr>
<tr>
<td><code>BuiltWith</code></td>
<td>Web technology profiler that provides detailed reports on a website&#39;s technology stack.</td>
<td>Offers both free and paid plans with varying levels of detail.</td>
</tr>
<tr>
<td><code>WhatWeb</code></td>
<td>Command-line tool for website fingerprinting.</td>
<td>Uses a vast database of signatures to identify various web technologies.</td>
</tr>
<tr>
<td><code>Nmap</code></td>
<td>Versatile network scanner that can be used for various reconnaissance tasks, including service and OS fingerprinting.</td>
<td>Can be used with scripts (NSE) to perform more specialised fingerprinting.</td>
</tr>
<tr>
<td><code>Netcraft</code></td>
<td>Offers a range of web security services, including website fingerprinting and security reporting.</td>
<td>Provides detailed reports on a website&#39;s technology, hosting provider, and security posture.</td>
</tr>
<tr>
<td><code>wafw00f</code></td>
<td>Command-line tool specifically designed for identifying Web Application Firewalls (WAFs).</td>
<td>Helps determine if a WAF is present and, if so, its type and configuration.</td>
</tr>
</tbody>
</table>
<h3 id="fingerprinting-inlanefreight-com">Fingerprinting inlanefreight.com</h3>
<p>Let&#39;s apply our fingerprinting knowledge to uncover the digital DNA of our purpose-built host, <code>inlanefreight.com</code>. We&#39;ll leverage both manual and automated techniques to gather information about its web server, technologies, and potential vulnerabilities.</p>
<h4 id="banner-grabbing">Banner Grabbing</h4>
<p>Our first step is to gather information directly from the web server itself. We can do this using the <code>curl</code> command with the <code>-I</code> flag (or <code>--head</code>) to fetch only the HTTP headers, not the entire page content.</p>
<p>&#x20; Fingerprinting</p>
<pre><code class="lang-shell-session"><span class="hljs-selector-tag">root</span>@<span class="hljs-keyword">htb</span>[/<span class="hljs-keyword">htb</span>]$ curl -I inlanefreight.com
</code></pre>
<p>The output will include the server banner, revealing the web server software and version number:</p>
<p>&#x20; Fingerprinting</p>
<pre><code class="lang-shell-session">root<span class="hljs-meta">@htb</span>[/htb]$ curl -I inlanefreight.com

HTTP/<span class="hljs-number">1.1</span> <span class="hljs-number">301</span> Moved Permanently
<span class="hljs-string">Date:</span> Fri, <span class="hljs-number">31</span> May <span class="hljs-number">2024</span> <span class="hljs-number">12</span>:<span class="hljs-number">07</span>:<span class="hljs-number">44</span> GMT
<span class="hljs-string">Server:</span> Apache/<span class="hljs-number">2.4</span><span class="hljs-number">.41</span> (Ubuntu)
<span class="hljs-string">Location:</span> <span class="hljs-string">https:</span><span class="hljs-comment">//inlanefreight.com/</span>
Content-<span class="hljs-string">Type:</span> text/html; charset=iso<span class="hljs-number">-8859</span><span class="hljs-number">-1</span>
</code></pre>
<p>In this case, we see that <code>inlanefreight.com</code> is running on <code>Apache/2.4.41</code>, specifically the <code>Ubuntu</code> version. This information is our first clue, hinting at the underlying technology stack. It&#39;s also trying to redirect to <code>https://inlanefreight.com/</code> so grab those banners too</p>
<p>&#x20; Fingerprinting</p>
<pre><code class="lang-shell-session">root<span class="hljs-meta">@htb</span>[<span class="hljs-regexp">/htb]$ curl -I https:/</span>/inlanefreight.com

HTTP/<span class="hljs-number">1.1</span> <span class="hljs-number">301</span> Moved Permanently
<span class="hljs-string">Date:</span> Fri, <span class="hljs-number">31</span> May <span class="hljs-number">2024</span> <span class="hljs-number">12</span>:<span class="hljs-number">12</span>:<span class="hljs-number">12</span> GMT
<span class="hljs-string">Server:</span> Apache/<span class="hljs-number">2.4</span><span class="hljs-number">.41</span> (Ubuntu)
X-Redirect-<span class="hljs-string">By:</span> WordPress
<span class="hljs-string">Location:</span> <span class="hljs-string">https:</span><span class="hljs-comment">//www.inlanefreight.com/</span>
Content-<span class="hljs-string">Type:</span> text/html; charset=UTF<span class="hljs-number">-8</span>
</code></pre>
<p>We now get a really interesting header, the server is trying to redirect us again, but this time we see that it&#39;s <code>WordPress</code> that is doing the redirection to <code>https://www.inlanefreight.com/</code></p>
<p>&#x20; Fingerprinting</p>
<pre><code class="lang-shell-session">root<span class="hljs-meta">@htb</span>[<span class="hljs-regexp">/htb]$ curl -I https:/</span>/www.inlanefreight.com

HTTP/<span class="hljs-number">1.1</span> <span class="hljs-number">200</span> OK
<span class="hljs-string">Date:</span> Fri, <span class="hljs-number">31</span> May <span class="hljs-number">2024</span> <span class="hljs-number">12</span>:<span class="hljs-number">12</span>:<span class="hljs-number">26</span> GMT
<span class="hljs-string">Server:</span> Apache/<span class="hljs-number">2.4</span><span class="hljs-number">.41</span> (Ubuntu)
<span class="hljs-string">Link:</span> &lt;<span class="hljs-string">https:</span><span class="hljs-comment">//www.inlanefreight.com/index.php/wp-json/&gt;; rel="https://api.w.org/"</span>
<span class="hljs-string">Link:</span> &lt;<span class="hljs-string">https:</span><span class="hljs-comment">//www.inlanefreight.com/index.php/wp-json/wp/v2/pages/7&gt;; rel="alternate"; type="application/json"</span>
<span class="hljs-string">Link:</span> &lt;<span class="hljs-string">https:</span><span class="hljs-comment">//www.inlanefreight.com/&gt;; rel=shortlink</span>
Content-<span class="hljs-string">Type:</span> text/html; charset=UTF<span class="hljs-number">-8</span>
</code></pre>
<p>A few more interesting headers, including an interesting path that contains <code>wp-json</code>. The <code>wp-</code> prefix is common to WordPress.</p>
<h4 id="wafw00f">Wafw00f</h4>
<p><code>Web Application Firewalls</code> (<code>WAFs</code>) are security solutions designed to protect web applications from various attacks. Before proceeding with further fingerprinting, it&#39;s crucial to determine if <code>inlanefreight.com</code> employs a WAF, as it could interfere with our probes or potentially block our requests.</p>
<p>To detect the presence of a WAF, we&#39;ll use the <code>wafw00f</code> tool. To install <code>wafw00f</code>, you can use pip3:</p>
<p>&#x20; Fingerprinting</p>
<pre><code class="lang-shell-session">root<span class="hljs-variable">@htb</span>[<span class="hljs-regexp">/htb]$ pip3 install git+https:/</span><span class="hljs-regexp">/github.com/</span>EnableSecurity/wafw00f
</code></pre>
<p>Once it&#39;s installed, pass the domain you want to check as an argument to the tool:</p>
<p>&#x20; Fingerprinting</p>
<pre><code class="lang-shell-session">root<span class="hljs-meta">@htb</span>[/htb]$ wafw00f inlanefreight.com

                ______
               /      \
              (  W00f! )
               \  ____/
               ,,    __            <span class="hljs-number">404</span> Hack Not Found
           |`-.__   <span class="hljs-regexp">/ /</span>                      __     __
           <span class="hljs-regexp">/"  _/</span>  <span class="hljs-regexp">/_/</span>                       \ \   <span class="hljs-regexp">/ /</span>
          *===*    <span class="hljs-regexp">/                          \ \_/</span> /  <span class="hljs-number">405</span> Not Allowed
         <span class="hljs-regexp">/     )__/</span><span class="hljs-regexp">/                           \   /</span>
    <span class="hljs-regexp">/|  /</span>     /---`                        <span class="hljs-number">403</span> Forbidden
    \\<span class="hljs-regexp">/`   \ |                                 /</span> _ \
    `\    <span class="hljs-regexp">/_\\_              502 Bad Gateway  /</span> / \ \  <span class="hljs-number">500</span> Internal Error
      `_____``-`                             <span class="hljs-regexp">/_/</span>   \_\

                        ~ <span class="hljs-string">WAFW00F :</span> v2<span class="hljs-number">.2</span><span class="hljs-number">.0</span> ~
        The Web Application Firewall Fingerprinting Toolkit

[*] Checking <span class="hljs-string">https:</span><span class="hljs-comment">//inlanefreight.com</span>
[+] The site <span class="hljs-string">https:</span><span class="hljs-comment">//inlanefreight.com is behind Wordfence (Defiant) WAF.</span>
[~] Number of <span class="hljs-string">requests:</span> <span class="hljs-number">2</span>
</code></pre>
<p>The <code>wafw00f</code> scan on <code>inlanefreight.com</code> reveals that the website is protected by the <code>Wordfence Web Application Firewall</code> (<code>WAF</code>), developed by Defiant.</p>
<p>This means the site has an additional security layer that could block or filter our reconnaissance attempts. In a real-world scenario, it would be crucial to keep this in mind as you proceed with further investigation, as you might need to adapt techniques to bypass or evade the WAF&#39;s detection mechanisms.</p>
<h4 id="nikto">Nikto</h4>
<p><code>Nikto</code> is a powerful open-source web server scanner. In addition to its primary function as a vulnerability assessment tool, <code>Nikto&#39;s</code> fingerprinting capabilities provide insights into a website&#39;s technology stack.</p>
<p><code>Nikto</code> is pre-installed on pwnbox, but if you need to install it, you can run the following commands:</p>
<p>&#x20; Fingerprinting</p>
<pre><code class="lang-shell-session">root<span class="hljs-meta">@htb</span>[/htb]$ sudo apt update &amp;&amp; sudo apt install -y perl
root<span class="hljs-meta">@htb</span>[<span class="hljs-regexp">/htb]$ git clone https:/</span><span class="hljs-regexp">/github.com/</span>sullo/nikto
root<span class="hljs-meta">@htb</span>[<span class="hljs-regexp">/htb]$ cd nikto/</span>program
root<span class="hljs-meta">@htb</span>[<span class="hljs-regexp">/htb]$ chmod +x ./</span>nikto.pl
</code></pre>
<p>To scan <code>inlanefreight.com</code> using <code>Nikto</code>, only running the fingerprinting modules, execute the following command:</p>
<p>&#x20; Fingerprinting</p>
<pre><code class="lang-shell-session"><span class="hljs-selector-tag">root</span>@<span class="hljs-keyword">htb</span>[/<span class="hljs-keyword">htb</span>]$ nikto -h inlanefreight.com -Tuning b
</code></pre>
<p>The <code>-h</code> flag specifies the target host. The <code>-Tuning b</code> flag tells <code>Nikto</code> to only run the Software Identification modules.</p>
<p><code>Nikto</code> will then initiate a series of tests, attempting to identify outdated software, insecure files or configurations, and other potential security risks.</p>
<p>&#x20; Fingerprinting</p>
<pre><code class="lang-shell-session">root@htb[/htb]$ nikto -h inlanefreight.com -Tuning b

- Nikto v2.5.0
<span class="hljs-comment">---------------------------------------------------------------------------</span>
+ Multiple IPs found: 134.209.24.248, 2a03:b0c0:1:e0::32c:b001
+ Target IP:          134.209.24.248
+ Target Hostname:    www.inlanefreight.com
+ Target Port:        443
<span class="hljs-comment">---------------------------------------------------------------------------</span>
+ SSL Info:        Subject:  /CN=inlanefreight.com
                   Altnames: inlanefreight.com, www.inlanefreight.com
                   Ciphers:  TLS_AES_256_GCM_SHA384
                   Issuer:   /C=US/O=Let's Encrypt/CN=R3
+ <span class="hljs-keyword">Start</span> <span class="hljs-keyword">Time</span>:         <span class="hljs-number">2024</span><span class="hljs-number">-05</span><span class="hljs-number">-31</span> <span class="hljs-number">13</span>:<span class="hljs-number">35</span>:<span class="hljs-number">54</span> (GMT0)
<span class="hljs-comment">---------------------------------------------------------------------------</span>
+ <span class="hljs-keyword">Server</span>: Apache/<span class="hljs-number">2.4</span><span class="hljs-number">.41</span> (Ubuntu)
+ /: <span class="hljs-keyword">Link</span> header <span class="hljs-keyword">found</span> <span class="hljs-keyword">with</span> <span class="hljs-keyword">value</span>: <span class="hljs-built_in">ARRAY</span>(<span class="hljs-number">0x558e78790248</span>). See: https://developer.mozilla.org/en-US/docs/Web/<span class="hljs-keyword">HTTP</span>/Headers/<span class="hljs-keyword">Link</span>
+ /: The site uses TLS <span class="hljs-keyword">and</span> the <span class="hljs-keyword">Strict</span>-Transport-<span class="hljs-keyword">Security</span> <span class="hljs-keyword">HTTP</span> header <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> defined. See: https://developer.mozilla.org/en-US/docs/Web/<span class="hljs-keyword">HTTP</span>/Headers/<span class="hljs-keyword">Strict</span>-Transport-<span class="hljs-keyword">Security</span>
+ /: The X-<span class="hljs-keyword">Content</span>-<span class="hljs-keyword">Type</span>-Options header <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> set. This could <span class="hljs-keyword">allow</span> the <span class="hljs-keyword">user</span> <span class="hljs-keyword">agent</span> <span class="hljs-keyword">to</span> render the <span class="hljs-keyword">content</span> <span class="hljs-keyword">of</span> the site <span class="hljs-keyword">in</span> a different fashion <span class="hljs-keyword">to</span> the MIME type. See: https://www.netsparker.com/web-vulnerability-scanner/vulnerabilities/<span class="hljs-keyword">missing</span>-<span class="hljs-keyword">content</span>-<span class="hljs-keyword">type</span>-header/
+ /index.php?: Uncommon header <span class="hljs-string">'x-redirect-by'</span> <span class="hljs-keyword">found</span>, <span class="hljs-keyword">with</span> <span class="hljs-keyword">contents</span>: WordPress.
+ <span class="hljs-keyword">No</span> CGI Directories <span class="hljs-keyword">found</span> (<span class="hljs-keyword">use</span> <span class="hljs-string">'-C all'</span> <span class="hljs-keyword">to</span> <span class="hljs-keyword">force</span> <span class="hljs-keyword">check</span> all possible dirs)
+ /: The <span class="hljs-keyword">Content</span>-<span class="hljs-keyword">Encoding</span> header <span class="hljs-keyword">is</span> <span class="hljs-keyword">set</span> <span class="hljs-keyword">to</span> <span class="hljs-string">"deflate"</span> which may mean that the <span class="hljs-keyword">server</span> <span class="hljs-keyword">is</span> vulnerable <span class="hljs-keyword">to</span> the BREACH attack. See: <span class="hljs-keyword">http</span>://breachattack.com/
+ Apache/<span class="hljs-number">2.4</span><span class="hljs-number">.41</span> appears <span class="hljs-keyword">to</span> be outdated (<span class="hljs-keyword">current</span> <span class="hljs-keyword">is</span> <span class="hljs-keyword">at</span> <span class="hljs-keyword">least</span> <span class="hljs-number">2.4</span><span class="hljs-number">.59</span>). Apache <span class="hljs-number">2.2</span><span class="hljs-number">.34</span> <span class="hljs-keyword">is</span> the EOL <span class="hljs-keyword">for</span> the <span class="hljs-number">2.</span>x branch.
+ /: Web <span class="hljs-keyword">Server</span> <span class="hljs-keyword">returns</span> a valid response <span class="hljs-keyword">with</span> junk <span class="hljs-keyword">HTTP</span> methods which may cause <span class="hljs-literal">false</span> positives.
+ /license.txt: License <span class="hljs-keyword">file</span> <span class="hljs-keyword">found</span> may identify site software.
+ /: A Wordpress installation was found.
+ /wp-login.php?<span class="hljs-keyword">action</span>=<span class="hljs-keyword">register</span>: Cookie wordpress_test_cookie created <span class="hljs-keyword">without</span> the httponly flag. See: https://developer.mozilla.org/en-US/docs/Web/<span class="hljs-keyword">HTTP</span>/Cookies
+ /wp-login.php:X-Frame-Options header <span class="hljs-keyword">is</span> deprecated <span class="hljs-keyword">and</span> has been replaced <span class="hljs-keyword">with</span> the <span class="hljs-keyword">Content</span>-<span class="hljs-keyword">Security</span>-<span class="hljs-keyword">Policy</span> <span class="hljs-keyword">HTTP</span> header <span class="hljs-keyword">with</span> the frame-ancestors directive instead. See: https://developer.mozilla.org/en-US/docs/Web/<span class="hljs-keyword">HTTP</span>/Headers/X-Frame-Options
+ /wp-login.php: Wordpress login found.
+ <span class="hljs-number">1316</span> requests: <span class="hljs-number">0</span> <span class="hljs-keyword">error</span>(s) <span class="hljs-keyword">and</span> <span class="hljs-number">12</span> item(s) reported <span class="hljs-keyword">on</span> remote host
+ <span class="hljs-keyword">End</span> <span class="hljs-keyword">Time</span>:           <span class="hljs-number">2024</span><span class="hljs-number">-05</span><span class="hljs-number">-31</span> <span class="hljs-number">13</span>:<span class="hljs-number">47</span>:<span class="hljs-number">27</span> (GMT0) (<span class="hljs-number">693</span> seconds)
<span class="hljs-comment">---------------------------------------------------------------------------</span>
+ <span class="hljs-number">1</span> host(s) tested
</code></pre>
<p>The reconnaissance scan on <code>inlanefreight.com</code> reveals several key findings:</p>
<ul>
<li><code>IPs</code>: The website resolves to both IPv4 (<code>134.209.24.248</code>) and IPv6 (<code>2a03:b0c0:1:e0::32c:b001</code>) addresses.</li>
<li><code>Server Technology</code>: The website runs on <code>Apache/2.4.41 (Ubuntu)</code></li>
<li><code>WordPress Presence</code>: The scan identified a WordPress installation, including the login page (<code>/wp-login.php</code>). This suggests the site might be a potential target for common WordPress-related exploits.</li>
<li><code>Information Disclosure</code>: The presence of a <code>license.txt</code> file could reveal additional details about the website&#39;s software components.</li>
<li><code>Headers</code>: Several non-standard or insecure headers were found, including a missing <code>Strict-Transport-Security</code> header and a potentially insecure <code>x-redirect-by</code> header.</li>
</ul>
<hr>
<h2 id="crawling">Crawling</h2>
<p><code>Crawling</code>, often called <code>spidering</code>, is the <code>automated process of systematically browsing the World Wide Web</code>. Similar to how a spider navigates its web, a web crawler follows links from one page to another, collecting information. These crawlers are essentially bots that use pre-defined algorithms to discover and index web pages, making them accessible through search engines or for other purposes like data analysis and web reconnaissance.</p>
<h3 id="how-web-crawlers-work">How Web Crawlers Work</h3>
<p>The basic operation of a web crawler is straightforward yet powerful. It starts with a seed URL, which is the initial web page to crawl. The crawler fetches this page, parses its content, and extracts all its links. It then adds these links to a queue and crawls them, repeating the process iteratively. Depending on its scope and configuration, the crawler can explore an entire website or even a vast portion of the web.</p>
<ol>
<li><p><code>Homepage</code>: You start with the homepage containing <code>link1</code>, <code>link2</code>, and <code>link3</code>.</p>
<p>Code: txt</p>
<pre><code class="lang-txt">Homepage
â”œâ”€â”€ li<span class="hljs-symbol">nk1</span>
â”œâ”€â”€ li<span class="hljs-symbol">nk2</span>
â””â”€â”€ li<span class="hljs-symbol">nk3</span>
</code></pre>
</li>
<li><p><code>Visiting link1</code>: Visiting <code>link1</code> shows the homepage, <code>link2</code>, and also <code>link4</code> and <code>link5</code>.</p>
<p>Code: txt</p>
<pre><code class="lang-txt">li<span class="hljs-symbol">nk1</span> Page
â”œâ”€â”€ Homepage
â”œâ”€â”€ li<span class="hljs-symbol">nk2</span>
â”œâ”€â”€ li<span class="hljs-symbol">nk4</span>
â””â”€â”€ li<span class="hljs-symbol">nk5</span>
</code></pre>
</li>
<li><code>Continuing the Crawl</code>: The crawler continues to follow these links systematically, gathering all accessible pages and their links.</li>
</ol>
<p>This example illustrates how a web crawler discovers and collects information by systematically following links, distinguishing it from fuzzing which involves guessing potential links.</p>
<p>There are two primary types of crawling strategies.</p>
<h4 id="breadth-first-crawling">Breadth-First Crawling</h4>
<p><img src="https://mermaid.ink/svg/pako:eNo90D0PgjAQBuC_0twsg98Jgwkf6oKJgThZhkpPIEohpR0M4b970shNd09uuHsHKFqJ4EOpRVexJOWqtw83ZIiS3dKEK0YV3K-iRLbMuUIluQqY5x1Y6HSV_yFysCYIJ4gdbGY4OtgSRBOcHOxmODvYE8ACGtSNqCXdOPwu4WAqbJCDT60U-sWBq5H2hDVt9lEF-EZbXIBubVmB_xTvnibbSWEwrgX91syKsjatvrgIpiTGL-8RVcQ" alt="Flowchart showing a Seed URL leading to Page 1, which branches to Page 2 and Page 3. Page 2 connects to Page 4 and Page 5, while Page 3 connects to Page 6 and Page 7."></p>
<p><code>Breadth-first crawling</code> prioritizes exploring a website&#39;s width before going deep. It starts by crawling all the links on the seed page, then moves on to the links on those pages, and so on. This is useful for getting a broad overview of a website&#39;s structure and content.</p>
<h4 id="depth-first-crawling">Depth-First Crawling</h4>
<p><img src="https://mermaid.ink/svg/pako:eNo9zz0PgjAQBuC_0twsg18LgwlfGyYG4uQ5VHoC0RZS2sEQ_rsnTezU98mlvXeGZlAEMbRWjp0oKzSTf4RQEylxrUo0gk9yu8iWxPaOhoxCk4goOok06I41XSELsGfIVsgDHBjyFYoAR4YivCEEGtiAJqtlr3iZ-fclgutIE0LMVyXtCwHNwnPSu6H-mAZiZz1twA6-7SB-yvfEyY9KOsp7ySX0X0n1brDn0HWtvHwB2SFOww" alt="Flowchart showing a Seed URL leading to Page 1, then to Page 2. Page 2 connects to Page 3, which branches to Page 4 and Page 5."></p>
<p>In contrast, <code>depth-first crawling</code> prioritizes depth over breadth. It follows a single path of links as far as possible before backtracking and exploring other paths. This can be useful for finding specific content or reaching deep into a website&#39;s structure.</p>
<p>The choice of strategy depends on the specific goals of the crawling process.</p>
<h3 id="extracting-valuable-information">Extracting Valuable Information</h3>
<p>Crawlers can extract a diverse array of data, each serving a specific purpose in the reconnaissance process:</p>
<ul>
<li><code>Links (Internal and External)</code>: These are the fundamental building blocks of the web, connecting pages within a website (<code>internal links</code>) and to other websites (<code>external links</code>). Crawlers meticulously collect these links, allowing you to map out a website&#39;s structure, discover hidden pages, and identify relationships with external resources.</li>
<li><code>Comments</code>: Comments sections on blogs, forums, or other interactive pages can be a goldmine of information. Users often inadvertently reveal sensitive details, internal processes, or hints of vulnerabilities in their comments.</li>
<li><code>Metadata</code>: Metadata refers to <code>data about data</code>. In the context of web pages, it includes information like page titles, descriptions, keywords, author names, and dates. This metadata can provide valuable context about a page&#39;s content, purpose, and relevance to your reconnaissance goals.</li>
<li><code>Sensitive Files</code>: Web crawlers can be configured to actively search for sensitive files that might be inadvertently exposed on a website. This includes <code>backup files</code> (e.g., <code>.bak</code>, <code>.old</code>), <code>configuration files</code> (e.g., <code>web.config</code>, <code>settings.php</code>), <code>log files</code> (e.g., <code>error_log</code>, <code>access_log</code>), and other files containing passwords, <code>API keys</code>, or other confidential information. Carefully examining the extracted files, especially backup and configuration files, can reveal a trove of sensitive information, such as <code>database credentials</code>, <code>encryption keys</code>, or even source code snippets.</li>
</ul>
<h4 id="the-importance-of-context">The Importance of Context</h4>
<p>Understanding the context surrounding the extracted data is paramount.</p>
<p>A single piece of information, like a comment mentioning a specific software version, might not seem significant on its own. However, when combined with other findingsâ€”such as an outdated version listed in metadata or a potentially vulnerable configuration file discovered through crawlingâ€”it can transform into a critical indicator of a potential vulnerability.</p>
<p>The true value of extracted data lies in connecting the dots and constructing a comprehensive picture of the target&#39;s digital landscape.</p>
<p>For instance, a list of extracted links might initially appear mundane. But upon closer examination, you notice a pattern: several URLs point to a directory named <code>/files/</code>. This triggers your curiosity, and you decide to manually visit the directory. To your surprise, you find that directory browsing is enabled, exposing a host of files, including backup archives, internal documents, and potentially sensitive data. This discovery wouldn&#39;t have been possible by merely looking at individual links in isolation; the contextual analysis led you to this critical finding.</p>
<p>Similarly, seemingly innocuous comments can gain significance when correlated with other discoveries. A comment mentioning a &quot;file server&quot; might not raise any red flags initially. However, when combined with the aforementioned discovery of the <code>/files/</code> directory, it reinforces the possibility that the file server is publicly accessible, potentially exposing sensitive information or confidential data.</p>
<p>Therefore, it&#39;s essential to approach data analysis holistically, considering the relationships between different data points and their potential implications for your reconnaissance goals.</p>
<hr>
<h2 id="robots-txt">robots.txt</h2>
<hr>
<p>Imagine you&#39;re a guest at a grand house party. While you&#39;re free to mingle and explore, there might be certain rooms marked &quot;Private&quot; that you&#39;re expected to avoid. This is akin to how <code>robots.txt</code> functions in the world of web crawling. It acts as a virtual &quot;<code>etiquette guide</code>&quot; for bots, outlining which areas of a website they are allowed to access and which are off-limits.</p>
<h3 id="what-is-robots-txt-">What is robots.txt?</h3>
<p>Technically, <code>robots.txt</code> is a simple text file placed in the root directory of a website (e.g., <code>www.example.com/robots.txt</code>). It adheres to the Robots Exclusion Standard, guidelines for how web crawlers should behave when visiting a website. This file contains instructions in the form of &quot;directives&quot; that tell bots which parts of the website they can and cannot crawl.</p>
<h4 id="how-robots-txt-works">How robots.txt Works</h4>
<p>The directives in robots.txt typically target specific user-agents, which are identifiers for different types of bots. For example, a directive might look like this:</p>
<p>Code: txt</p>
<pre><code class="lang-txt">User-<span class="hljs-string">agent:</span> *
<span class="hljs-string">Disallow:</span> <span class="hljs-regexp">/private/</span>
</code></pre>
<p>This directive tells all user-agents (<code>*</code> is a wildcard) that they are not allowed to access any URLs that start with <code>/private/</code>. Other directives can allow access to specific directories or files, set crawl delays to avoid overloading a server or provide links to sitemaps for efficient crawling.</p>
<h4 id="understanding-robots-txt-structure">Understanding robots.txt Structure</h4>
<p>The robots.txt file is a plain text document that lives in the root directory of a website. It follows a straightforward structure, with each set of instructions, or &quot;record,&quot; separated by a blank line. Each record consists of two main components:</p>
<ol>
<li><code>User-agent</code>: This line specifies which crawler or bot the following rules apply to. A wildcard (<code>*</code>) indicates that the rules apply to all bots. Specific user agents can also be targeted, such as &quot;Googlebot&quot; (Google&#39;s crawler) or &quot;Bingbot&quot; (Microsoft&#39;s crawler).</li>
<li><code>Directives</code>: These lines provide specific instructions to the identified user-agent.</li>
</ol>
<p>Common directives include:</p>
<table>
<thead>
<tr>
<th>Directive</th>
<th>Description</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Disallow</code></td>
<td>Specifies paths or patterns that the bot should not crawl.</td>
<td><code>Disallow: /admin/</code> (disallow access to the admin directory)</td>
</tr>
<tr>
<td><code>Allow</code></td>
<td>Explicitly permits the bot to crawl specific paths or patterns, even if they fall under a broader <code>Disallow</code> rule.</td>
<td><code>Allow: /public/</code> (allow access to the public directory)</td>
</tr>
<tr>
<td><code>Crawl-delay</code></td>
<td>Sets a delay (in seconds) between successive requests from the bot to avoid overloading the server.</td>
<td><code>Crawl-delay: 10</code> (10-second delay between requests)</td>
</tr>
<tr>
<td><code>Sitemap</code></td>
<td>Provides the URL to an XML sitemap for more efficient crawling.</td>
<td><code>Sitemap: https://www.example.com/sitemap.xml</code></td>
</tr>
</tbody>
</table>
<h4 id="why-respect-robots-txt-">Why Respect robots.txt?</h4>
<p>While robots.txt is not strictly enforceable (a rogue bot could still ignore it), most legitimate web crawlers and search engine bots will respect its directives. This is important for several reasons:</p>
<ul>
<li><code>Avoiding Overburdening Servers</code>: By limiting crawler access to certain areas, website owners can prevent excessive traffic that could slow down or even crash their servers.</li>
<li><code>Protecting Sensitive Information</code>: Robots.txt can shield private or confidential information from being indexed by search engines.</li>
<li><code>Legal and Ethical Compliance</code>: In some cases, ignoring robots.txt directives could be considered a violation of a website&#39;s terms of service or even a legal issue, especially if it involves accessing copyrighted or private data.</li>
</ul>
<h3 id="robots-txt-in-web-reconnaissance">robots.txt in Web Reconnaissance</h3>
<p>For web reconnaissance, robots.txt serves as a valuable source of intelligence. While respecting the directives outlined in this file, security professionals can glean crucial insights into the structure and potential vulnerabilities of a target website:</p>
<ul>
<li><code>Uncovering Hidden Directories</code>: Disallowed paths in robots.txt often point to directories or files the website owner intentionally wants to keep out of reach from search engine crawlers. These hidden areas might house sensitive information, backup files, administrative panels, or other resources that could interest an attacker.</li>
<li><code>Mapping Website Structure</code>: By analyzing the allowed and disallowed paths, security professionals can create a rudimentary map of the website&#39;s structure. This can reveal sections that are not linked from the main navigation, potentially leading to undiscovered pages or functionalities.</li>
<li><code>Detecting Crawler Traps</code>: Some websites intentionally include &quot;honeypot&quot; directories in robots.txt to lure malicious bots. Identifying such traps can provide insights into the target&#39;s security awareness and defensive measures.</li>
</ul>
<h4 id="analyzing-robots-txt">Analyzing robots.txt</h4>
<p>Here&#39;s an example of a robots.txt file:</p>
<p>Code: txt</p>
<pre><code class="lang-txt">User-<span class="hljs-string">agent:</span> *
<span class="hljs-string">Disallow:</span> <span class="hljs-regexp">/admin/</span>
<span class="hljs-string">Disallow:</span> <span class="hljs-regexp">/private/</span>
<span class="hljs-string">Allow:</span> <span class="hljs-regexp">/public/</span>

User-<span class="hljs-string">agent:</span> Googlebot
Crawl-<span class="hljs-string">delay:</span> <span class="hljs-number">10</span>
<span class="hljs-symbol">
Sitemap:</span> <span class="hljs-string">https:</span><span class="hljs-comment">//www.example.com/sitemap.xml</span>
</code></pre>
<p>This file contains the following directives:</p>
<ul>
<li>All user agents are disallowed from accessing the <code>/admin/</code> and <code>/private/</code> directories.</li>
<li>All user agents are allowed to access the <code>/public/</code> directory.</li>
<li>The <code>Googlebot</code> (Google&#39;s web crawler) is specifically instructed to wait 10 seconds between requests.</li>
<li>The sitemap, located at <code>https://www.example.com/sitemap.xml</code>, is provided for easier crawling and indexing.</li>
</ul>
<p>By analyzing this robots.txt, we can infer that the website likely has an admin panel located at <code>/admin/</code> and some private content in the <code>/private/</code> directory.</p>
<hr>
<h2 id="well-known-uris">Well-Known URIs</h2>
<hr>
<p>The <code>.well-known</code> standard, defined in <a href="https://datatracker.ietf.org/doc/html/rfc8615">RFC 8615</a>, serves as a standardized directory within a website&#39;s root domain. This designated location, typically accessible via the <code>/.well-known/</code> path on a web server, centralizes a website&#39;s critical metadata, including configuration files and information related to its services, protocols, and security mechanisms.</p>
<p>By establishing a consistent location for such data, <code>.well-known</code> simplifies the discovery and access process for various stakeholders, including web browsers, applications, and security tools. This streamlined approach enables clients to automatically locate and retrieve specific configuration files by constructing the appropriate URL. For instance, to access a website&#39;s security policy, a client would request <code>https://example.com/.well-known/security.txt</code>.</p>
<p>The <code>Internet Assigned Numbers Authority</code> (<code>IANA</code>) maintains a <a href="https://www.iana.org/assignments/well-known-uris/well-known-uris.xhtml">registry</a> of <code>.well-known</code> URIs, each serving a specific purpose defined by various specifications and standards. Below is a table highlighting a few notable examples:</p>
<table>
<thead>
<tr>
<th>URI Suffix</th>
<th>Description</th>
<th>Status</th>
<th>Reference</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>security.txt</code></td>
<td>Contains contact information for security researchers to report vulnerabilities.</td>
<td>Permanent</td>
<td>RFC 9116</td>
</tr>
<tr>
<td><code>/.well-known/change-password</code></td>
<td>Provides a standard URL for directing users to a password change page.</td>
<td>Provisional</td>
<td><a href="https://w3c.github.io/webappsec-change-password-url/#the-change-password-well-known-uri">https://w3c.github.io/webappsec-change-password-url/#the-change-password-well-known-uri</a></td>
</tr>
<tr>
<td><code>openid-configuration</code></td>
<td>Defines configuration details for OpenID Connect, an identity layer on top of the OAuth 2.0 protocol.</td>
<td>Permanent</td>
<td><a href="http://openid.net/specs/openid-connect-discovery-1\_0.html">http://openid.net/specs/openid-connect-discovery-1\_0.html</a></td>
</tr>
<tr>
<td><code>assetlinks.json</code></td>
<td>Used for verifying ownership of digital assets (e.g., apps) associated with a domain.</td>
<td>Permanent</td>
<td><a href="https://github.com/google/digitalassetlinks/blob/master/well-known/specification.md">https://github.com/google/digitalassetlinks/blob/master/well-known/specification.md</a></td>
</tr>
<tr>
<td><code>mta-sts.txt</code></td>
<td>Specifies the policy for SMTP MTA Strict Transport Security (MTA-STS) to enhance email security.</td>
<td>Permanent</td>
<td>RFC 8461</td>
</tr>
</tbody>
</table>
<p>This is just a small sample of the many <code>.well-known</code> URIs registered with IANA. Each entry in the registry offers specific guidelines and requirements for implementation, ensuring a standardized approach to leveraging the <code>.well-known</code> mechanism for various applications.</p>
<h3 id="web-recon-and-well-known">Web Recon and .well-known</h3>
<p>In web recon, the <code>.well-known</code> URIs can be invaluable for discovering endpoints and configuration details that can be further tested during a penetration test. One particularly useful URI is <code>openid-configuration</code>.</p>
<p>The <code>openid-configuration</code> URI is part of the OpenID Connect Discovery protocol, an identity layer built on top of the OAuth 2.0 protocol. When a client application wants to use OpenID Connect for authentication, it can retrieve the OpenID Connect Provider&#39;s configuration by accessing the <code>https://example.com/.well-known/openid-configuration</code> endpoint. This endpoint returns a JSON document containing metadata about the provider&#39;s endpoints, supported authentication methods, token issuance, and more:</p>
<p>Code: json</p>
<pre><code class="lang-json">{
  <span class="hljs-attr">"issuer"</span>: <span class="hljs-string">"https://example.com"</span>,
  <span class="hljs-attr">"authorization_endpoint"</span>: <span class="hljs-string">"https://example.com/oauth2/authorize"</span>,
  <span class="hljs-attr">"token_endpoint"</span>: <span class="hljs-string">"https://example.com/oauth2/token"</span>,
  <span class="hljs-attr">"userinfo_endpoint"</span>: <span class="hljs-string">"https://example.com/oauth2/userinfo"</span>,
  <span class="hljs-attr">"jwks_uri"</span>: <span class="hljs-string">"https://example.com/oauth2/jwks"</span>,
  <span class="hljs-attr">"response_types_supported"</span>: [<span class="hljs-string">"code"</span>, <span class="hljs-string">"token"</span>, <span class="hljs-string">"id_token"</span>],
  <span class="hljs-attr">"subject_types_supported"</span>: [<span class="hljs-string">"public"</span>],
  <span class="hljs-attr">"id_token_signing_alg_values_supported"</span>: [<span class="hljs-string">"RS256"</span>],
  <span class="hljs-attr">"scopes_supported"</span>: [<span class="hljs-string">"openid"</span>, <span class="hljs-string">"profile"</span>, <span class="hljs-string">"email"</span>]
}
</code></pre>
<p>The information obtained from the <code>openid-configuration</code> endpoint provides multiple exploration opportunities:</p>
<ol>
<li><code>Endpoint Discovery</code>:<ul>
<li><code>Authorization Endpoint</code>: Identifying the URL for user authorization requests.</li>
<li><code>Token Endpoint</code>: Finding the URL where tokens are issued.</li>
<li><code>Userinfo Endpoint</code>: Locating the endpoint that provides user information.</li>
</ul>
</li>
<li><code>JWKS URI</code>: The <code>jwks_uri</code> reveals the <code>JSON Web Key Set</code> (<code>JWKS</code>), detailing the cryptographic keys used by the server.</li>
<li><code>Supported Scopes and Response Types</code>: Understanding which scopes and response types are supported helps in mapping out the functionality and limitations of the OpenID Connect implementation.</li>
<li><code>Algorithm Details</code>: Information about supported signing algorithms can be crucial for understanding the security measures in place.</li>
</ol>
<p>Exploring the <a href="https://www.iana.org/assignments/well-known-uris/well-known-uris.xhtml">IANA Registry</a> and experimenting with the various <code>.well-known</code> URIs is an invaluable approach to uncovering additional web reconnaissance opportunities. As demonstrated with the <code>openid-configuration</code> endpoint above, these standardized URIs provide structured access to critical metadata and configuration details, enabling security professionals to comprehensively map out a website&#39;s security landscape.</p>
<hr>
<h2 id="creepy-crawlies">Creepy Crawlies</h2>
<hr>
<p>Web crawling is vast and intricate, but you don&#39;t have to embark on this journey alone. A plethora of web crawling tools are available to assist you, each with its own strengths and specialties. These tools automate the crawling process, making it faster and more efficient, allowing you to focus on analyzing the extracted data.</p>
<h3 id="popular-web-crawlers">Popular Web Crawlers</h3>
<ol>
<li><code>Burp Suite Spider</code>: Burp Suite, a widely used web application testing platform, includes a powerful active crawler called Spider. Spider excels at mapping out web applications, identifying hidden content, and uncovering potential vulnerabilities.</li>
<li><code>OWASP ZAP (Zed Attack Proxy)</code>: ZAP is a free, open-source web application security scanner. It can be used in automated and manual modes and includes a spider component to crawl web applications and identify potential vulnerabilities.</li>
<li><code>Scrapy (Python Framework)</code>: Scrapy is a versatile and scalable Python framework for building custom web crawlers. It provides rich features for extracting structured data from websites, handling complex crawling scenarios, and automating data processing. Its flexibility makes it ideal for tailored reconnaissance tasks.</li>
<li><code>Apache Nutch (Scalable Crawler)</code>: Nutch is a highly extensible and scalable open-source web crawler written in Java. It&#39;s designed to handle massive crawls across the entire web or focus on specific domains. While it requires more technical expertise to set up and configure, its power and flexibility make it a valuable asset for large-scale reconnaissance projects.</li>
</ol>
<p>Adhering to ethical and responsible crawling practices is crucial no matter which tool you choose. Always obtain permission before crawling a website, especially if you plan to perform extensive or intrusive scans. Be mindful of the website&#39;s server resources and avoid overloading them with excessive requests.</p>
<h3 id="scrapy">Scrapy</h3>
<p>We will leverage Scrapy and a custom spider tailored for reconnaissance on <code>inlanefreight.com</code>. If you are interested in more information on crawling/spidering techniques, refer to the &quot;<a href="https://academy.hackthebox.com/module/details/110">Using Web Proxies</a>&quot; module, as it forms part of CBBH as well.</p>
<h4 id="installing-scrapy">Installing Scrapy</h4>
<p>Before we begin, ensure you have Scrapy installed on your system. If you don&#39;t, you can easily install it using pip, the Python package installer:</p>
<p>&#x20; Creepy Crawlies</p>
<pre><code class="lang-shell-session"><span class="hljs-selector-tag">root</span>@<span class="hljs-keyword">htb</span>[/<span class="hljs-keyword">htb</span>]$ pip3 install scrapy
</code></pre>
<p>This command will download and install Scrapy along with its dependencies, preparing your environment for building our spider.</p>
<h4 id="reconspider">ReconSpider</h4>
<p>First, run this command in your terminal to download the custom scrapy spider, <code>ReconSpider</code>, and extract it to the current working directory.</p>
<p>&#x20; Creepy Crawlies</p>
<pre><code class="lang-shell-session">root<span class="hljs-variable">@htb</span>[<span class="hljs-regexp">/htb]$ wget -O ReconSpider.zip https:/</span><span class="hljs-regexp">/academy.hackthebox.com/storage</span><span class="hljs-regexp">/modules/</span><span class="hljs-number">144</span>/ReconSpider.v1.<span class="hljs-number">2</span>.zip
root<span class="hljs-variable">@htb</span>[<span class="hljs-regexp">/htb]$ unzip ReconSpider.zip</span>
</code></pre>
<p>With the files extracted, you can run <code>ReconSpider.py</code> using the following command:</p>
<p>&#x20; Creepy Crawlies</p>
<pre><code class="lang-shell-session">root@htb[/htb]$ python3 ReconSpider<span class="hljs-selector-class">.py</span> http:<span class="hljs-comment">//inlanefreight.com</span>
</code></pre>
<p>Replace <code>inlanefreight.com</code> with the domain you want to spider. The spider will crawl the target and collect valuable information.</p>
<h4 id="results-json">results.json</h4>
<p>After running <code>ReconSpider.py</code>, the data will be saved in a JSON file, <code>results.json</code>. This file can be explored using any text editor. Below is the structure of the JSON file produced:</p>
<p>Code: json</p>
<pre><code class="lang-json">{
    <span class="hljs-attr">"emails"</span>: [
        <span class="hljs-string">"lily.floid@inlanefreight.com"</span>,
        <span class="hljs-string">"cvs@inlanefreight.com"</span>,
        ...
    ],
    <span class="hljs-attr">"links"</span>: [
        <span class="hljs-string">"https://www.themeansar.com"</span>,
        <span class="hljs-string">"https://www.inlanefreight.com/index.php/offices/"</span>,
        ...
    ],
    <span class="hljs-attr">"external_files"</span>: [
        <span class="hljs-string">"https://www.inlanefreight.com/wp-content/uploads/2020/09/goals.pdf"</span>,
        ...
    ],
    <span class="hljs-attr">"js_files"</span>: [
        <span class="hljs-string">"https://www.inlanefreight.com/wp-includes/js/jquery/jquery-migrate.min.js?ver=3.3.2"</span>,
        ...
    ],
    <span class="hljs-attr">"form_fields"</span>: [],
    <span class="hljs-attr">"images"</span>: [
        <span class="hljs-string">"https://www.inlanefreight.com/wp-content/uploads/2021/03/AboutUs_01-1024x810.png"</span>,
        ...
    ],
    <span class="hljs-attr">"videos"</span>: [],
    <span class="hljs-attr">"audio"</span>: [],
    <span class="hljs-attr">"comments"</span>: [
        <span class="hljs-string">"&lt;!-- #masthead --&gt;"</span>,
        ...
    ]
}
</code></pre>
<p>Each key in the JSON file represents a different type of data extracted from the target website:</p>
<table><thead><tr><th width="185">JSON Key</th><th>Description</th></tr></thead><tbody><tr><td><code>emails</code></td><td>Lists email addresses found on the domain.</td></tr><tr><td><code>links</code></td><td>Lists URLs of links found within the domain.</td></tr><tr><td><code>external_files</code></td><td>Lists URLs of external files such as PDFs.</td></tr><tr><td><code>js_files</code></td><td>Lists URLs of JavaScript files used by the website.</td></tr><tr><td><code>form_fields</code></td><td>Lists form fields found on the domain (empty in this example).</td></tr><tr><td><code>images</code></td><td>Lists URLs of images found on the domain.</td></tr><tr><td><code>videos</code></td><td>Lists URLs of videos found on the domain (empty in this example).</td></tr><tr><td><code>audio</code></td><td>Lists URLs of audio files found on the domain (empty in this example).</td></tr><tr><td><code>comments</code></td><td>Lists HTML comments found in the source code.</td></tr></tbody></table>

<p>By exploring this JSON structure, you can gain valuable insights into the web application&#39;s architecture, content, and potential points of interest for further investigation.</p>
<hr>
<h2 id="search-engine-discovery">Search Engine Discovery</h2>
<hr>
<p>Search engines serve as our guides in the vast landscape of the internet, helping us navigate through the seemingly endless expanse of information. However, beyond their primary function of answering everyday queries, search engines also hold a treasure trove of data that can be invaluable for web reconnaissance and information gathering. This practice, known as search engine discovery or OSINT (Open Source Intelligence) gathering, involves using search engines as powerful tools to uncover information about target websites, organisations, and individuals.</p>
<p>At its core, search engine discovery leverages the immense power of search algorithms to extract data that may not be readily visible on websites. Security professionals and researchers can delve deep into the indexed web by employing specialised search operators, techniques, and tools, uncovering everything from employee information and sensitive documents to hidden login pages and exposed credentials.</p>
<h3 id="why-search-engine-discovery-matters">Why Search Engine Discovery Matters</h3>
<p>Search engine discovery is a crucial component of web reconnaissance for several reasons:</p>
<ul>
<li><code>Open Source</code>: The information gathered is publicly accessible, making it a legal and ethical way to gain insights into a target.</li>
<li><code>Breadth of Information</code>: Search engines index a vast portion of the web, offering a wide range of potential information sources.</li>
<li><code>Ease of Use</code>: Search engines are user-friendly and require no specialised technical skills.</li>
<li><code>Cost-Effective</code>: It&#39;s a free and readily available resource for information gathering.</li>
</ul>
<p>The information you can pull together from Search Engines can be applied in several different ways as well:</p>
<ul>
<li><code>Security Assessment</code>: Identifying vulnerabilities, exposed data, and potential attack vectors.</li>
<li><code>Competitive Intelligence</code>: Gathering information about competitors&#39; products, services, and strategies.</li>
<li><code>Investigative Journalism</code>: Uncovering hidden connections, financial transactions, and unethical practices.</li>
<li><code>Threat Intelligence</code>: Identifying emerging threats, tracking malicious actors, and predicting potential attacks.</li>
</ul>
<p>However, it&#39;s important to note that search engine discovery has limitations. Search engines do not index all information, and some data may be deliberately hidden or protected.</p>
<h3 id="search-operators">Search Operators</h3>
<p>Search operators are like search engines&#39; secret codes. These special commands and modifiers unlock a new level of precision and control, allowing you to pinpoint specific types of information amidst the vastness of the indexed web.</p>
<p>While the exact syntax may vary slightly between search engines, the underlying principles remain consistent. Let&#39;s delve into some essential and advanced search operators:</p>
<table>
<thead>
<tr>
<th>Operator</th>
<th>Operator Description</th>
<th>Example</th>
<th>Example Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>site:</code></td>
<td>Limits results to a specific website or domain.</td>
<td><code>site:example.com</code></td>
<td>Find all publicly accessible pages on example.com.</td>
</tr>
<tr>
<td><code>inurl:</code></td>
<td>Finds pages with a specific term in the URL.</td>
<td><code>inurl:login</code></td>
<td>Search for login pages on any website.</td>
</tr>
<tr>
<td><code>filetype:</code></td>
<td>Searches for files of a particular type.</td>
<td><code>filetype:pdf</code></td>
<td>Find downloadable PDF documents.</td>
</tr>
<tr>
<td><code>intitle:</code></td>
<td>Finds pages with a specific term in the title.</td>
<td><code>intitle:&quot;confidential report&quot;</code></td>
<td>Look for documents titled &quot;confidential report&quot; or similar variations.</td>
</tr>
<tr>
<td><code>intext:</code> or <code>inbody:</code></td>
<td>Searches for a term within the body text of pages.</td>
<td><code>intext:&quot;password reset&quot;</code></td>
<td>Identify webpages containing the term â€œpassword resetâ€.</td>
</tr>
<tr>
<td><code>cache:</code></td>
<td>Displays the cached version of a webpage (if available).</td>
<td><code>cache:example.com</code></td>
<td>View the cached version of example.com to see its previous content.</td>
</tr>
<tr>
<td><code>link:</code></td>
<td>Finds pages that link to a specific webpage.</td>
<td><code>link:example.com</code></td>
<td>Identify websites linking to example.com.</td>
</tr>
<tr>
<td><code>related:</code></td>
<td>Finds websites related to a specific webpage.</td>
<td><code>related:example.com</code></td>
<td>Discover websites similar to example.com.</td>
</tr>
<tr>
<td><code>info:</code></td>
<td>Provides a summary of information about a webpage.</td>
<td><code>info:example.com</code></td>
<td>Get basic details about example.com, such as its title and description.</td>
</tr>
<tr>
<td><code>define:</code></td>
<td>Provides definitions of a word or phrase.</td>
<td><code>define:phishing</code></td>
<td>Get a definition of &quot;phishing&quot; from various sources.</td>
</tr>
<tr>
<td><code>numrange:</code></td>
<td>Searches for numbers within a specific range.</td>
<td><code>site:example.com numrange:1000-2000</code></td>
<td>Find pages on example.com containing numbers between 1000 and 2000.</td>
</tr>
<tr>
<td><code>allintext:</code></td>
<td>Finds pages containing all specified words in the body text.</td>
<td><code>allintext:admin password reset</code></td>
<td>Search for pages containing both &quot;admin&quot; and &quot;password reset&quot; in the body text.</td>
</tr>
<tr>
<td><code>allinurl:</code></td>
<td>Finds pages containing all specified words in the URL.</td>
<td><code>allinurl:admin panel</code></td>
<td>Look for pages with &quot;admin&quot; and &quot;panel&quot; in the URL.</td>
</tr>
<tr>
<td><code>allintitle:</code></td>
<td>Finds pages containing all specified words in the title.</td>
<td><code>allintitle:confidential report 2023</code></td>
<td>Search for pages with &quot;confidential,&quot; &quot;report,&quot; and &quot;2023&quot; in the title.</td>
</tr>
<tr>
<td><code>AND</code></td>
<td>Narrows results by requiring all terms to be present.</td>
<td><code>site:example.com AND (inurl:admin OR inurl:login)</code></td>
<td>Find admin or login pages specifically on example.com.</td>
</tr>
<tr>
<td><code>OR</code></td>
<td>Broadens results by including pages with any of the terms.</td>
<td><code>&quot;linux&quot; OR &quot;ubuntu&quot; OR &quot;debian&quot;</code></td>
<td>Search for webpages mentioning Linux, Ubuntu, or Debian.</td>
</tr>
<tr>
<td><code>NOT</code></td>
<td>Excludes results containing the specified term.</td>
<td><code>site:bank.com NOT inurl:login</code></td>
<td>Find pages on bank.com excluding login pages.</td>
</tr>
<tr>
<td><code>*</code> (wildcard)</td>
<td>Represents any character or word.</td>
<td><code>site:socialnetwork.com filetype:pdf user* manual</code></td>
<td>Search for user manuals (user guide, user handbook) in PDF format on socialnetwork.com.</td>
</tr>
<tr>
<td><code>..</code> (range search)</td>
<td>Finds results within a specified numerical range.</td>
<td><code>site:ecommerce.com &quot;price&quot; 100..500</code></td>
<td>Look for products priced between 100 and 500 on an e-commerce website.</td>
</tr>
<tr>
<td><code>&quot; &quot;</code> (quotation marks)</td>
<td>Searches for exact phrases.</td>
<td><code>&quot;information security policy&quot;</code></td>
<td>Find documents mentioning the exact phrase &quot;information security policy&quot;.</td>
</tr>
<tr>
<td><code>-</code> (minus sign)</td>
<td>Excludes terms from the search results.</td>
<td><code>site:news.com -inurl:sports</code></td>
<td>Search for news articles on news.com excluding sports-related content.</td>
</tr>
</tbody>
</table>
<h4 id="google-dorking">Google Dorking</h4>
<p>Google Dorking, also known as Google Hacking, is a technique that leverages the power of search operators to uncover sensitive information, security vulnerabilities, or hidden content on websites, using Google Search.</p>
<p>Here are some common examples of Google Dorks, for more examples, refer to the <a href="https://www.exploit-db.com/google-hacking-database">Google Hacking Database</a>:</p>
<ul>
<li>Finding Login Pages:<ul>
<li><code>site:example.com inurl:login</code></li>
<li><code>site:example.com (inurl:login OR inurl:admin)</code></li>
</ul>
</li>
<li>Identifying Exposed Files:<ul>
<li><code>site:example.com filetype:pdf</code></li>
<li><code>site:example.com (filetype:xls OR filetype:docx)</code></li>
</ul>
</li>
<li>Uncovering Configuration Files:<ul>
<li><code>site:example.com inurl:config.php</code></li>
<li><code>site:example.com (ext:conf OR ext:cnf)</code> (searches for extensions commonly used for configuration files)</li>
</ul>
</li>
<li>Locating Database Backups:<ul>
<li><code>site:example.com inurl:backup</code></li>
<li><code>site:example.com filetype:sql</code>&#x20;</li>
</ul>
</li>
</ul>
<hr>
<h2 id="web-archives">Web Archives</h2>
<hr>
<p>In the fast-paced digital world, websites come and go, leaving only fleeting traces of their existence behind. However, thanks to the <a href="https://web.archive.org/">Internet Archive&#39;s Wayback Machine</a>, we have a unique opportunity to revisit the past and explore the digital footprints of websites as they once were.</p>
<h4 id="what-is-the-wayback-machine-">What is the Wayback Machine?</h4>
<p><img src="https://academy.hackthebox.com/storage/modules/144/wayback.png" alt="Internet Archive Wayback Machine homepage with search bar for web pages, tools like browser extensions, and options for subscription service, collection search, and saving pages."></p>
<p><code>The Wayback Machine</code> is a digital archive of the World Wide Web and other information on the Internet. Founded by the Internet Archive, a non-profit organization, it has been archiving websites since 1996.</p>
<p>It allows users to &quot;go back in time&quot; and view snapshots of websites as they appeared at various points in their history. These snapshots, known as captures or archives, provide a glimpse into the past versions of a website, including its design, content, and functionality.</p>
<h4 id="how-does-the-wayback-machine-work-">How Does the Wayback Machine Work?</h4>
<p>The Wayback Machine operates by using web crawlers to capture snapshots of websites at regular intervals automatically. These crawlers navigate through the web, following links and indexing pages, much like how search engine crawlers work. However, instead of simply indexing the information for search purposes, the Wayback Machine stores the entire content of the pages, including HTML, CSS, JavaScript, images, and other resources.</p>
<p>The Wayback Machine&#39;s operation can be visualized as a three-step process:</p>
<p><img src="https://mermaid.ink/svg/pako:eNpNjkEOgjAQRa_SzBou0IUJ4lI3uqQsJu1IG2lLhlZjCHe3YGLc_f9m8vMW0NEQSBgYJyvOVxWarmV8jS4Mvajrgzh2DWvrnhtQ4b_t57ZrtKZ53gBU4Ik9OlMWFxWEUJAseVIgSzTIDwUqrOUPc4q3d9AgE2eqgGMeLMg7jnNpeTKY6OSwaPkfJeNS5MtXePdeP1LGQQs" alt="Flowchart with three steps: Crawling, Archiving, Accessing."></p>
<ol>
<li><code>Crawling</code>: The Wayback Machine employs automated web crawlers, often called &quot;bots,&quot; to browse the internet systematically. These bots follow links from one webpage to another, like how you would click hyperlinks to explore a website. However, instead of just reading the content, these bots download copies of the webpages they encounter.</li>
<li><code>Archiving</code>: The downloaded webpages, along with their associated resources like images, stylesheets, and scripts, are stored in the Wayback Machine&#39;s vast archive. Each captured webpage is linked to a specific date and time, creating a historical snapshot of the website at that moment. This archiving process happens at regular intervals, sometimes daily, weekly, or monthly, depending on the website&#39;s popularity and frequency of updates.</li>
<li><code>Accessing</code>: Users can access these archived snapshots through the Wayback Machine&#39;s interface. By entering a website&#39;s URL and selecting a date, you can view how the website looked at that specific point. The Wayback Machine allows you to browse individual pages and provides tools to search for specific terms within the archived content or download entire archived websites for offline analysis.</li>
</ol>
<p>The frequency with which the Wayback Machine archives a website varies. Some websites might be archived multiple times a day, while others might only have a few snapshots spread out over several years. Factors that influence this frequency include the website&#39;s popularity, its rate of change, and the resources available to the Internet Archive.</p>
<p>It&#39;s important to note that the Wayback Machine does not capture every single webpage online. It prioritizes websites deemed to be of cultural, historical, or research value. Additionally, website owners can request that their content be excluded from the Wayback Machine, although this is not always guaranteed.</p>
<h3 id="why-the-wayback-machine-matters-for-web-reconnaissance">Why the Wayback Machine Matters for Web Reconnaissance</h3>
<p>The Wayback Machine is a treasure trove for web reconnaissance, offering information that can be instrumental in various scenarios. Its significance lies in its ability to unveil a website&#39;s past, providing valuable insights that may not be readily apparent in its current state:</p>
<ol>
<li><code>Uncovering Hidden Assets and Vulnerabilities</code>: The Wayback Machine allows you to discover old web pages, directories, files, or subdomains that might not be accessible on the current website, potentially exposing sensitive information or security flaws.</li>
<li><code>Tracking Changes and Identifying Patterns</code>: By comparing historical snapshots, you can observe how the website has evolved, revealing changes in structure, content, technologies, and potential vulnerabilities.</li>
<li><code>Gathering Intelligence</code>: Archived content can be a valuable source of OSINT, providing insights into the target&#39;s past activities, marketing strategies, employees, and technology choices.</li>
<li><code>Stealthy Reconnaissance</code>: Accessing archived snapshots is a passive activity that doesn&#39;t directly interact with the target&#39;s infrastructure, making it a less detectable way to gather information.</li>
</ol>
<h3 id="going-wayback-on-htb">Going Wayback on HTB</h3>
<p>We can view the first archived version of HackTheBox by entering the page we are looking for into the Wayback Machine and selecting the earliest available capture date, being <code>2017-06-10 @ 04h23:01</code></p>
<p><img src="https://academy.hackthebox.com/storage/modules/144/wayback-htb.png" alt="Wayback Machine capture of Hack The Box homepage, version 0.8.7 beta, featuring a geometric cube logo and an &#39;About&#39; section describing the platform for testing penetration skills."></p>
<hr>
<h2 id="automating-recon">Automating Recon</h2>
<hr>
<p>While manual reconnaissance can be effective, it can also be time-consuming and prone to human error. Automating web reconnaissance tasks can significantly enhance efficiency and accuracy, allowing you to gather information at scale and identify potential vulnerabilities more rapidly.</p>
<h3 id="why-automate-reconnaissance-">Why Automate Reconnaissance?</h3>
<p>Automation offers several key advantages for web reconnaissance:</p>
<ul>
<li><code>Efficiency</code>: Automated tools can perform repetitive tasks much faster than humans, freeing up valuable time for analysis and decision-making.</li>
<li><code>Scalability</code>: Automation allows you to scale your reconnaissance efforts across a large number of targets or domains, uncovering a broader scope of information.</li>
<li><code>Consistency</code>: Automated tools follow predefined rules and procedures, ensuring consistent and reproducible results and minimising the risk of human error.</li>
<li><code>Comprehensive Coverage</code>: Automation can be programmed to perform a wide range of reconnaissance tasks, including DNS enumeration, subdomain discovery, web crawling, port scanning, and more, ensuring thorough coverage of potential attack vectors.</li>
<li><code>Integration</code>: Many automation frameworks allow for easy integration with other tools and platforms, creating a seamless workflow from reconnaissance to vulnerability assessment and exploitation.</li>
</ul>
<h3 id="reconnaissance-frameworks">Reconnaissance Frameworks</h3>
<p>These frameworks aim to provide a complete suite of tools for web reconnaissance:</p>
<ul>
<li><a href="https://github.com/thewhiteh4t/FinalRecon">FinalRecon</a>: A Python-based reconnaissance tool offering a range of modules for different tasks like SSL certificate checking, Whois information gathering, header analysis, and crawling. Its modular structure enables easy customisation for specific needs.</li>
<li><a href="https://github.com/lanmaster53/recon-ng">Recon-ng</a>: A powerful framework written in Python that offers a modular structure with various modules for different reconnaissance tasks. It can perform DNS enumeration, subdomain discovery, port scanning, web crawling, and even exploit known vulnerabilities.</li>
<li><a href="https://github.com/laramies/theHarvester">theHarvester</a>: Specifically designed for gathering email addresses, subdomains, hosts, employee names, open ports, and banners from different public sources like search engines, PGP key servers, and the SHODAN database. It is a command-line tool written in Python.</li>
<li><a href="https://github.com/smicallef/spiderfoot">SpiderFoot</a>: An open-source intelligence automation tool that integrates with various data sources to collect information about a target, including IP addresses, domain names, email addresses, and social media profiles. It can perform DNS lookups, web crawling, port scanning, and more.</li>
<li><a href="https://osintframework.com/">OSINT Framework</a>: A collection of various tools and resources for open-source intelligence gathering. It covers a wide range of information sources, including social media, search engines, public records, and more.</li>
</ul>
<h4 id="finalrecon">FinalRecon</h4>
<p><code>FinalRecon</code> offers a wealth of recon information:</p>
<ul>
<li><code>Header Information</code>: Reveals server details, technologies used, and potential security misconfigurations.</li>
<li><code>Whois Lookup</code>: Uncovers domain registration details, including registrant information and contact details.</li>
<li><code>SSL Certificate Information</code>: Examines the SSL/TLS certificate for validity, issuer, and other relevant details.</li>
<li><code>Crawler</code>:<ul>
<li>HTML, CSS, JavaScript: Extracts links, resources, and potential vulnerabilities from these files.</li>
<li>Internal/External Links: Maps out the website&#39;s structure and identifies connections to other domains.</li>
<li>Images, robots.txt, sitemap.xml: Gathers information about allowed/disallowed crawling paths and website structure.</li>
<li>Links in JavaScript, Wayback Machine: Uncovers hidden links and historical website data.</li>
</ul>
</li>
<li><code>DNS Enumeration</code>: Queries over 40 DNS record types, including DMARC records for email security assessment.</li>
<li><code>Subdomain Enumeration</code>: Leverages multiple data sources (crt.sh, AnubisDB, ThreatMiner, CertSpotter, Facebook API, VirusTotal API, Shodan API, BeVigil API) to discover subdomains.</li>
<li><code>Directory Enumeration</code>: Supports custom wordlists and file extensions to uncover hidden directories and files.</li>
<li><code>Wayback Machine</code>: Retrieves URLs from the last five years to analyse website changes and potential vulnerabilities.</li>
</ul>
<p>Installation is quick and easy:</p>
<p>&#x20; Automating Recon</p>
<pre><code class="lang-shell-session">root@htb[/htb]$ git clone https://github.com/thewhiteh4t/FinalRecon.git
root@htb[/htb]$ cd FinalRecon
root@htb[/htb]$ pip3 <span class="hljs-keyword">install</span> -r requirements.txt
root@htb[/htb]$ chmod +x ./finalrecon.py
root@htb[/htb]$ ./finalrecon.py <span class="hljs-comment">--help</span>

<span class="hljs-keyword">usage</span>: finalrecon.py [-h] [<span class="hljs-comment">--url URL] [--headers] [--sslinfo] [--whois]</span>
                     [<span class="hljs-comment">--crawl] [--dns] [--sub] [--dir] [--wayback] [--ps]</span>
                     [<span class="hljs-comment">--full] [-nb] [-dt DT] [-pt PT] [-T T] [-w W] [-r] [-s]</span>
                     [-sp SP] [-d D] [-e E] [-o O] [-cd CD] [-k K]

FinalRecon - All <span class="hljs-keyword">in</span> One Web Recon | v1<span class="hljs-number">.1</span><span class="hljs-number">.6</span>

optional arguments:
  -h, <span class="hljs-comment">--help  show this help message and exit</span>
  <span class="hljs-comment">--url URL   Target URL</span>
  <span class="hljs-comment">--headers   Header Information</span>
  <span class="hljs-comment">--sslinfo   SSL Certificate Information</span>
  <span class="hljs-comment">--whois     Whois Lookup</span>
  <span class="hljs-comment">--crawl     Crawl Target</span>
  <span class="hljs-comment">--dns       DNS Enumeration</span>
  <span class="hljs-comment">--sub       Sub-Domain Enumeration</span>
  <span class="hljs-comment">--dir       Directory Search</span>
  <span class="hljs-comment">--wayback   Wayback URLs</span>
  <span class="hljs-comment">--ps        Fast Port Scan</span>
  <span class="hljs-comment">--full      Full Recon</span>

Extra Options:
  -nb         Hide Banner
  -dt DT      <span class="hljs-built_in">Number</span> <span class="hljs-keyword">of</span> threads <span class="hljs-keyword">for</span> <span class="hljs-keyword">directory</span> enum [ <span class="hljs-keyword">Default</span> : <span class="hljs-number">30</span> ]
  -pt PT      <span class="hljs-built_in">Number</span> <span class="hljs-keyword">of</span> threads <span class="hljs-keyword">for</span> port <span class="hljs-keyword">scan</span> [ <span class="hljs-keyword">Default</span> : <span class="hljs-number">50</span> ]
  -T T        Request <span class="hljs-keyword">Timeout</span> [ <span class="hljs-keyword">Default</span> : <span class="hljs-number">30.0</span> ]
  -w W        <span class="hljs-keyword">Path</span> <span class="hljs-keyword">to</span> Wordlist [ <span class="hljs-keyword">Default</span> : wordlists/dirb_common.txt ]
  -r          <span class="hljs-keyword">Allow</span> Redirect [ <span class="hljs-keyword">Default</span> : <span class="hljs-literal">False</span> ]
  -s          Toggle SSL Verification [ <span class="hljs-keyword">Default</span> : <span class="hljs-literal">True</span> ]
  -sp SP      Specify SSL Port [ <span class="hljs-keyword">Default</span> : <span class="hljs-number">443</span> ]
  -d D        Custom DNS Servers [ <span class="hljs-keyword">Default</span> : <span class="hljs-number">1.1</span><span class="hljs-number">.1</span><span class="hljs-number">.1</span> ]
  -e E        <span class="hljs-keyword">File</span> Extensions [ Example : txt, <span class="hljs-keyword">xml</span>, php ]
  -o O        <span class="hljs-keyword">Export</span> <span class="hljs-keyword">Format</span> [ <span class="hljs-keyword">Default</span> : txt ]
  -cd CD      <span class="hljs-keyword">Change</span> <span class="hljs-keyword">export</span> <span class="hljs-keyword">directory</span> [ <span class="hljs-keyword">Default</span> : ~/.local/<span class="hljs-keyword">share</span>/finalrecon ]
  -k K        <span class="hljs-keyword">Add</span> API <span class="hljs-keyword">key</span> [ Example : shodan@<span class="hljs-keyword">key</span> ]
</code></pre>
<p>To get started, you will first clone the <code>FinalRecon</code> repository from GitHub using <code>git clone https://github.com/thewhiteh4t/FinalRecon.git</code>. This will create a new directory named &quot;FinalRecon&quot; containing all the necessary files.</p>
<p>Next, navigate into the newly created directory with <code>cd FinalRecon</code>. Once inside, you will install the required Python dependencies using <code>pip3 install -r requirements.txt</code>. This ensures that <code>FinalRecon</code> has all the libraries and modules it needs to function correctly.</p>
<p>To ensure that the main script is executable, you will need to change the file permissions using <code>chmod +x ./finalrecon.py</code>. This allows you to run the script directly from your terminal.</p>
<p>Finally, you can verify that <code>FinalRecon</code> is installed correctly and get an overview of its available options by running <code>./finalrecon.py --help</code>. This will display a help message with details on how to use the tool, including the various modules and their respective options:</p>
<table>
<thead>
<tr>
<th>Option</th>
<th>Argument</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>-h</code>, <code>--help</code></td>
<td></td>
<td>Show the help message and exit.</td>
</tr>
<tr>
<td><code>--url</code></td>
<td>URL</td>
<td>Specify the target URL.</td>
</tr>
<tr>
<td><code>--headers</code></td>
<td></td>
<td>Retrieve header information for the target URL.</td>
</tr>
<tr>
<td><code>--sslinfo</code></td>
<td></td>
<td>Get SSL certificate information for the target URL.</td>
</tr>
<tr>
<td><code>--whois</code></td>
<td></td>
<td>Perform a Whois lookup for the target domain.</td>
</tr>
<tr>
<td><code>--crawl</code></td>
<td></td>
<td>Crawl the target website.</td>
</tr>
<tr>
<td><code>--dns</code></td>
<td></td>
<td>Perform DNS enumeration on the target domain.</td>
</tr>
<tr>
<td><code>--sub</code></td>
<td></td>
<td>Enumerate subdomains for the target domain.</td>
</tr>
<tr>
<td><code>--dir</code></td>
<td></td>
<td>Search for directories on the target website.</td>
</tr>
<tr>
<td><code>--wayback</code></td>
<td></td>
<td>Retrieve Wayback URLs for the target.</td>
</tr>
<tr>
<td><code>--ps</code></td>
<td></td>
<td>Perform a fast port scan on the target.</td>
</tr>
<tr>
<td><code>--full</code></td>
<td></td>
<td>Perform a full reconnaissance scan on the target.</td>
</tr>
</tbody>
</table>
<p>For instance, if we want <code>FinalRecon</code> to gather header information and perform a Whois lookup for <code>inlanefreight.com</code>, we would use the corresponding flags (<code>--headers</code> and <code>--whois</code>), so the command would be:</p>
<p>&#x20; Automating Recon</p>
<pre><code class="lang-shell-session">root@htb[/htb]<span class="hljs-formula">$ ./finalrecon.py --headers --whois --url http://inlanefreight.com

 ______  __   __   __   ______   __
/<span class="hljs-tag">\<span class="hljs-name"> </span></span> ___<span class="hljs-tag">\<span class="hljs-name">/</span></span><span class="hljs-tag">\<span class="hljs-name"> </span></span><span class="hljs-tag">\<span class="hljs-name"> </span></span>/<span class="hljs-tag">\<span class="hljs-name"> </span></span>"-.<span class="hljs-tag">\<span class="hljs-name"> </span></span><span class="hljs-tag">\<span class="hljs-name"> </span></span>/<span class="hljs-tag">\<span class="hljs-name"> </span></span> __ <span class="hljs-tag">\<span class="hljs-name"> </span></span>/<span class="hljs-tag">\<span class="hljs-name"> </span></span><span class="hljs-tag">\<span class="hljs-name">
</span></span><span class="hljs-tag">\<span class="hljs-name"> </span></span><span class="hljs-tag">\<span class="hljs-name"> </span></span> __<span class="hljs-tag">\<span class="hljs-name">\</span></span> <span class="hljs-tag">\<span class="hljs-name"> </span></span><span class="hljs-tag">\<span class="hljs-name">\</span></span> <span class="hljs-tag">\<span class="hljs-name"> </span></span><span class="hljs-tag">\<span class="hljs-name">-</span></span>.  <span class="hljs-tag">\<span class="hljs-name">\</span></span> <span class="hljs-tag">\<span class="hljs-name"> </span></span> __ <span class="hljs-tag">\<span class="hljs-name">\</span></span> <span class="hljs-tag">\<span class="hljs-name"> </span></span><span class="hljs-tag">\<span class="hljs-name">_</span></span>___
 <span class="hljs-tag">\<span class="hljs-name"> </span></span><span class="hljs-tag">\<span class="hljs-name">_</span></span><span class="hljs-tag">\<span class="hljs-name"> </span></span>  <span class="hljs-tag">\<span class="hljs-name"> </span></span><span class="hljs-tag">\<span class="hljs-name">_</span></span><span class="hljs-tag">\<span class="hljs-name">\</span></span> <span class="hljs-tag">\<span class="hljs-name">_</span></span><span class="hljs-tag">\<span class="hljs-name">\</span></span>"<span class="hljs-tag">\<span class="hljs-name">_</span></span><span class="hljs-tag">\<span class="hljs-name">\</span></span> <span class="hljs-tag">\<span class="hljs-name">_</span></span><span class="hljs-tag">\<span class="hljs-name"> </span></span><span class="hljs-tag">\<span class="hljs-name">_</span></span><span class="hljs-tag">\<span class="hljs-name">\</span></span> <span class="hljs-tag">\<span class="hljs-name">_</span></span>____<span class="hljs-tag">\<span class="hljs-name">
</span></span>  <span class="hljs-tag">\<span class="hljs-name">/</span></span>_/    <span class="hljs-tag">\<span class="hljs-name">/</span></span>_/ <span class="hljs-tag">\<span class="hljs-name">/</span></span>_/ <span class="hljs-tag">\<span class="hljs-name">/</span></span>_/ <span class="hljs-tag">\<span class="hljs-name">/</span></span>_/<span class="hljs-tag">\<span class="hljs-name">/</span></span>_/ <span class="hljs-tag">\<span class="hljs-name">/</span></span>_____/
 ______   ______   ______   ______   __   __
/<span class="hljs-tag">\<span class="hljs-name"> </span> =</span>= <span class="hljs-tag">\<span class="hljs-name"> </span></span>/<span class="hljs-tag">\<span class="hljs-name"> </span></span> ___<span class="hljs-tag">\<span class="hljs-name"> </span></span>/<span class="hljs-tag">\<span class="hljs-name"> </span></span> ___<span class="hljs-tag">\<span class="hljs-name"> </span></span>/<span class="hljs-tag">\<span class="hljs-name"> </span></span> __ <span class="hljs-tag">\<span class="hljs-name"> </span></span>/<span class="hljs-tag">\<span class="hljs-name"> </span></span>"-.<span class="hljs-tag">\<span class="hljs-name"> </span></span><span class="hljs-tag">\<span class="hljs-name">
</span></span><span class="hljs-tag">\<span class="hljs-name"> </span></span><span class="hljs-tag">\<span class="hljs-name"> </span></span> __&lt; <span class="hljs-tag">\<span class="hljs-name"> </span></span><span class="hljs-tag">\<span class="hljs-name"> </span></span> __<span class="hljs-tag">\<span class="hljs-name"> </span></span><span class="hljs-tag">\<span class="hljs-name"> </span></span><span class="hljs-tag">\<span class="hljs-name"> </span></span><span class="hljs-tag">\<span class="hljs-name">_</span></span>___<span class="hljs-tag">\<span class="hljs-name"> </span></span><span class="hljs-tag">\<span class="hljs-name"> </span></span><span class="hljs-tag">\<span class="hljs-name">/</span></span><span class="hljs-tag">\<span class="hljs-name"> </span></span><span class="hljs-tag">\<span class="hljs-name">\</span></span> <span class="hljs-tag">\<span class="hljs-name"> </span></span><span class="hljs-tag">\<span class="hljs-name">-</span></span>.  <span class="hljs-tag">\<span class="hljs-name">
</span></span> <span class="hljs-tag">\<span class="hljs-name"> </span></span><span class="hljs-tag">\<span class="hljs-name">_</span></span><span class="hljs-tag">\<span class="hljs-name"> </span></span><span class="hljs-tag">\<span class="hljs-name">_</span></span><span class="hljs-tag">\<span class="hljs-name">\</span></span> <span class="hljs-tag">\<span class="hljs-name">_</span></span>____<span class="hljs-tag">\<span class="hljs-name">\</span></span> <span class="hljs-tag">\<span class="hljs-name">_</span></span>____<span class="hljs-tag">\<span class="hljs-name">\</span></span> <span class="hljs-tag">\<span class="hljs-name">_</span></span>____<span class="hljs-tag">\<span class="hljs-name">\</span></span> <span class="hljs-tag">\<span class="hljs-name">_</span></span><span class="hljs-tag">\<span class="hljs-name">\</span></span>"<span class="hljs-tag">\<span class="hljs-name">_</span></span><span class="hljs-tag">\<span class="hljs-name">
</span></span>  <span class="hljs-tag">\<span class="hljs-name">/</span></span>_/ /_/ <span class="hljs-tag">\<span class="hljs-name">/</span></span>_____/ <span class="hljs-tag">\<span class="hljs-name">/</span></span>_____/ <span class="hljs-tag">\<span class="hljs-name">/</span></span>_____/ <span class="hljs-tag">\<span class="hljs-name">/</span></span>_/ <span class="hljs-tag">\<span class="hljs-name">/</span></span>_/

[&gt;] Created By   : thewhiteh4t
 |---&gt; Twitter   : https://twitter.com/thewhiteh4t
 |---&gt; Community : https://twc1rcle.com/
[&gt;] Version      : 1.1.6

[+] Target : http://inlanefreight.com

[+] IP Address : 134.209.24.248

[!] Headers :

Date : Tue, 11 Jun 2024 10:08:00 GMT
Server : Apache/2.4.41 (Ubuntu)
Link : &lt;https://www.inlanefreight.com/index.php/wp-json/&gt;; rel="https://api.w.org/", &lt;https://www.inlanefreight.com/index.php/wp-json/wp/v2/pages/7&gt;; rel="alternate"; type="application/json", &lt;https://www.inlanefreight.com/&gt;; rel=shortlink
Vary : Accept-Encoding
Content-Encoding : gzip
Content-Length : 5483
Keep-Alive : timeout=5, max=100
Connection : Keep-Alive
Content-Type : text/html; charset=UTF-8

[!] Whois Lookup : 

   Domain Name: INLANEFREIGHT.COM
   Registry Domain ID: 2420436757_DOMAIN_COM-VRSN
   Registrar WHOIS Server: whois.registrar.amazon.com
   Registrar URL: http://registrar.amazon.com
   Updated Date: 2023-07-03T01:11:15Z
   Creation Date: 2019-08-05T22:43:09Z
   Registry Expiry Date: 2024-08-05T22:43:09Z
   Registrar: Amazon Registrar, Inc.
   Registrar IANA ID: 468
   Registrar Abuse Contact Email: abuse@amazonaws.com
   Registrar Abuse Contact Phone: +1.2024422253
   Domain Status: clientDeleteProhibited https://icann.org/epp#clientDeleteProhibited
   Domain Status: clientTransferProhibited https://icann.org/epp#clientTransferProhibited
   Domain Status: clientUpdateProhibited https://icann.org/epp#clientUpdateProhibited
   Name Server: NS-1303.AWSDNS-34.ORG
   Name Server: NS-1580.AWSDNS-05.CO.UK
   Name Server: NS-161.AWSDNS-20.COM
   Name Server: NS-671.AWSDNS-19.NET
   DNSSEC: unsigned
   URL of the ICANN Whois Inaccuracy Complaint Form: https://www.icann.org/wicf/


[+] Completed in 0:00:00.257780

[+] Exported : /home/htb-ac-643601/.local/share/finalrecon/dumps/fr_inlanefreight.com_11-06-2024_11:07:59</span>
</code></pre>
<hr>
